[
{
	"uri": "http://example.org/projects/",
	"title": "Projects",
	"tags": [],
	"description": "",
	"content": "Projects This page list some of the small projects I worked in during my free time in ad-hoc mode.\nWawa A baby monitoring trail. Wawa, from the Aymara Baby, is a project inspired in how to help a mum to keep one eye in her newborn baby while she is executing some activity inside the house, in particular in the case when the newborn baby is sleeping and you want to monitor your baby. Qilquiri Qilquiri, from the Aymara Writer, is a helper that generates snipes of common Java-base code from a Vim session. For example, create a class signature by just presing two keys. Achacachi Achachachi is the name of a small town located on the Altiplano in the South American Andes. This project is a small set of scripts to re-deploy my laptop configuration. When my laptop broken or simply needs to be re-installed, \u0026#34;I need to back to Achacachi\u0026#34; to get all the configuration back as before. "
},
{
	"uri": "http://example.org/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": "Devops Infrastructure as a code  Building images: This section presents examples of building images using Packer from HashiCorp.  Cloud Computing  GCP notes Short notes from GCP training sessions.  Basic notes about cloud security  Cloud security notes A collection of notes from different trainings.  "
},
{
	"uri": "http://example.org/sre/",
	"title": "SRE",
	"tags": [],
	"description": "",
	"content": "Site Reliability Engineering This page is in its initial stage. I am trying to collect and read references which can be found at references.\n"
},
{
	"uri": "http://example.org/notes/",
	"title": "Notes",
	"tags": [],
	"description": "",
	"content": "General notes This page containts brief examples and explanation of different topics of interests from my students.\nMore details will be added gradually.\nA K  Kubernetes  G  Golang  S  Shell tricks  V  Vim  "
},
{
	"uri": "http://example.org/devopssec/3h/",
	"title": "3h",
	"tags": [],
	"description": "",
	"content": "Etichal Hacking notes General "
},
{
	"uri": "http://example.org/devops/aws/cloudpractitioner/",
	"title": "CloudPractitioner",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "http://example.org/teaching/resources/",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": "This site present a list of resources shared by or to my students during the teaching and training sessions. I am placing here as a way to remember them and share with people with similar interest.\nA M  Math for programmers by Simon Robinson  This course covers the maths behind how your computer stores and manipulates data. You\u0026#39;ll learn how to read binary and hexadecimal, how both integers and floating point numbers are stored and the limitations of using them. Advice on best practices... "
},
{
	"uri": "http://example.org/devops/gcp/",
	"title": "GCP",
	"tags": [],
	"description": "",
	"content": "Googgle Cloud Platform Notes. This section contains notes about different GCP topics.\nGoogle Cloud Platform Fundamentals: Core Infrastructure "
},
{
	"uri": "http://example.org/tools/",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": "Tools This page is just to remember the references of the tools I think are useful and the tools recommened by friends. It helps me to not searching it again from scratch and also safe time when a friend ask me for more information.\nCategories  Browsing and plugins Text editors Terminals  "
},
{
	"uri": "http://example.org/devops/aws/devopseng/",
	"title": "DevopsEng",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "http://example.org/devops/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "http://example.org/reproducibility/",
	"title": "Reproducibility",
	"tags": [],
	"description": "",
	"content": "Reproducibility The idea to promote reproducibility was inspired by my last research work, nMANET. At many points of my research, I was stopped by many issues, which were required help from experts. Helpers need to reproduce the issue locally, which not always is a trivial task. However, if there is a method to let the helper reproduce the issue in a fast and painless manner, the discusion of the problem became more predominant compared with the effort of reproduce the problem. I am planning to fill this page with more details during my free time, which is the reason of why I tag this site as under construction. "
},
{
	"uri": "http://example.org/teaching/",
	"title": "Teaching",
	"tags": [],
	"description": "",
	"content": "Teaching This site present my teaching and knowledge transfer activities inside and outside the university. Some resources are showed in here in case of interest.\n2020  A practical overview of Vagrant A technical session for Jenkins and Vagrant Using plugins in Jenkins  2019  Introduction to Continuous Integration and pipelines. Introduction to Jenkins  2017/2018 academic teaching activities St-Andrews University. Tutorials: CS2006 Advanced Programming Projects Demonstrations: CS2002 Computer architectures CS2006 Advanced Programming Projects Lecturer assistance: CS3102 Data Communications and Networks. CS2002 Computer architectures CS2006 Advanced Programming Projects  Past years academic teaching activities St-Andrews University. Semester One 2017/2018 Tutorials CS1002 Object Oriented Programming CS1005 Computer Science in Everyday Life CS1003 Programming with Data   Semester One 2017/2018 Demonstrations: CS1002 Object Oriented Programming Exercise classes: CS1002 Object Oriented Programming CS2003 The Internet and the Web: Concepts and Programming   Semester Two 2016/2017 Tutorials: CS1003 Programming with Data Demonstrations: CS1006 Programming Projects CS2002 Computer Systems CS2006 Advanced Programming Projects   Semester One 2016/2017 Tutorials: CS1005 Computer Science in Everyday Life Demonstrations: CS1002 Object Oriented Programming CS2003 The Internet and the Web: Concepts and Programming  "
},
{
	"uri": "http://example.org/research/",
	"title": "Research",
	"tags": [],
	"description": "",
	"content": "Research This section presents the research topics of interest.\nnMANET nManet is the short name for NDN applied in Manets. NDN stands for Named Data Neworking and Manets stands for Mobile Ad-hoc Networks. In this research, nMANET aims to offer an alterntive to ad-hoc mobile exchange of information in emergency scenarios.\n"
},
{
	"uri": "http://example.org/blog/",
	"title": "Blog",
	"tags": [],
	"description": "",
	"content": "Blog This blog is empty for now, but I am planning to write one blog about:\n Cooking rice. Share my life location. Books I read  Hope it come soon.\n"
},
{
	"uri": "http://example.org/funding/",
	"title": "Funding",
	"tags": [],
	"description": "",
	"content": "Demetria \u0026amp; Mario foundation This is a foundation created by Demetria Inocencia Aruni Rojas de Perez and Mario Hermogenes Perez Limachi with the intention to support education and culture for Aymara children in the community of Viacha in La Paz - Bolivia. The Demetria \u0026amp; Mario foundation is in its initial stage and it is planned to start its first intervention by the end of 2020.\nI decided to join this foundation after the talk provided by the founders in Viacha-Bolivia, which explained the importance to invest in education of Aymara Children as a way to preserve Aymara culture and let the future of Viacha coexists with new technoligies that can enhance the life standards of people within the community.\nMore details are planned to appear in the comming months and if you are interested to join, please feel free to contact me.\n"
},
{
	"uri": "http://example.org/devopssec/",
	"title": "DevOpsSec",
	"tags": [],
	"description": "",
	"content": "Devops Security notes  3H notes  "
},
{
	"uri": "http://example.org/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "Welcome My name is Percy, and this site has a minimal content about my interests, activities, and notes I recorded for myself or for people with similar interests.\nContact  percyperezdante [at] gmail (dot) com Github Linkedin, Vagrant Cloud Twitter: @percyperezd  Interests My main interests include to develop, experiment and test new technologies related to:\n nMANET Reproducibility Teaching Automation Continuous Integration and Continuous Delivery Site Reliability Engineering DevOps Software Development Enginering Cloud computing Distributed systems Security  NOTE: nMANET is an approach that intends to offer an alternative to solve the limitations of Mobile Ad-hoc Networks in emergency scenarios such as earthquakes and social riots.\nnMANET intends to show whether future Internet architectures such as Name Data based Networking protocol can be applied in Mobila Ad-hoc networks. To achieve this, I am designing, implementing and testing JNFD, which is the prototype of nMANET, and that is functional in Andriod and Linux base mobile devices. The source code of JNFD is open source and it has been tested in smartphones such as MotoE, MotoG, and Samsumg Note4, and also in laptops and through JNFD simulators such as Mini-JNFD.\nActivities   Sports. I like to play football and basketball, but also I often swim. I currenly swiming for the Diabetes UK charity, swim22. We formed a great team of colleges who are willing to swim approximatelly 230 Km all together. As a team we would love to have your contributions if possible of course.\n  Devops days Edinburgh 2020. I am member of the organizers for the 2020 Devops days in Edinburgh. Devops days happen here in Edinburhg once a year and is an international conference that include technical talks about software development, IT infrastructure operations, and a mix of them.\n  Sadly, this event is cancelled due to COVID-19.\nTooling  Jfrog Artifactory  Notes This section contains a set of basic tutorials I wrote for my students. They are simple notes I took from books and online resources. These notes are grouped in the following categories.\n  Minimal infrastructure\n  Software Development\n  Infrastrucuture as a code\n  Devops \u0026amp; SRE\n  "
},
{
	"uri": "http://example.org/notes/terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": "1. Terraform plugin cache By setting this variable, Terraform allows to cache all provider plugins in a local directory. Note that provider plugins could be large. Therefore, if you do not use this feature, terraform will download these provider plugins in all directories or spaces where you call terraform init. This means it will use more disk space, redundancy of plugins, and potentially slow compare with caching plugins locally.\nHow to set: export TF_PLUGIN_CACHE_DIR=\u0026quot;$HOME/.terraform.d/plugin-cache\u0026quot; Or create a .terraformrc file in your $HOME folder and add the following\nplugin_cache_dir = \u0026quot;$HOME/.terraform.d/plugin-cache\u0026quot; Example percy@BRIX:uswest2-dev[]$ terraform init percy@BRIX:uswest2-dev[]$ terraform plan percy@BRIX:uswest2-dev[]$ ls -ltra total 28 drwxrwxr-x 3 percy percy 4096 Jan 31 08:17 .. -rw-rw-r-- 1 percy percy 151 Feb 7 07:01 provider.tf -rw-r--r-- 1 percy percy 1107 Feb 7 10:50 .terraform.lock.hcl -rw-rw-r-- 1 percy percy 4032 Feb 7 11:20 terraform.tfstate -rw-rw-r-- 1 percy percy 227 Feb 7 11:21 general.tf drwxr-xr-x 3 percy percy 4096 Feb 7 11:22 .terraform drwxrwxr-x 3 percy percy 4096 Feb 7 11:29 . percy@BRIX:uswest2-dev[]$ tree .terraform .terraform └── providers └── registry.terraform.io └── hashicorp └── aws └── 3.74.0 └── linux_amd64 -\u0026gt; /home/percy/.terraform.d/plugin-cache/registry.terraform.io/hashicorp/aws/3.74.0/linux_amd64 6 directories, 0 files percy@BRIX:uswest2-dev[]$ tree ~/.terraform.d/ checkpoint_cache checkpoint_signature plugin-cache/ percy@BRIX:uswest2-dev[]$ tree ~/.terraform.d/plugin-cache/ /home/percy/.terraform.d/plugin-cache/ └── registry.terraform.io └── hashicorp └── aws └── 3.74.0 └── linux_amd64 └── terraform-provider-aws_v3.74.0_x5 Note Using the previous example. You could remove the provider.tf ( content below ), the .terraform.lock.hcl and the .terraform. Then, terraform init/plan still works because it is useing the local cache.\nterraform { required_providers { aws = { source = \u0026quot;hashicorp/aws\u0026quot; version = \u0026quot;~\u0026gt; 3.27\u0026quot; } } required_version = \u0026quot;\u0026gt;= 0.14.9\u0026quot; } 2. terraform.tfstate file This file is created, by default in the same folder when terraform is invoqued, when \u0026ldquo;terraform apply\u0026rdquo; is executed. It contains the last uptodate state of the resources in the provider. For example, if you delete this file, terraform concludes that there resource does not exists and will try to create it.\nAnother way to create the terraform.tfstate file is to import from the provider. For example in the case of AWS, the following will import the configuration of the instance i-0001 into a state file called aws_instance.app_server, then you can re-use it in your .tf files of your project.\n$ cat general.tf provider \u0026quot;aws\u0026quot; { profile = \u0026quot;default\u0026quot; region = \u0026quot;us-west-2\u0026quot; } resource \u0026quot;aws_instance\u0026quot; \u0026quot;app_server\u0026quot; { ami = \u0026quot;ami-xxxx\u0026quot; instance_type = \u0026quot;t2.micro\u0026quot; tags = { Name = \u0026quot;2-ExampleAppServerInstance\u0026quot; } } $ terraform import aws_instance.app_server i-0001 $ terraform state list aws_instance.app_server "
},
{
	"uri": "http://example.org/notes/cmd/",
	"title": "Cmd",
	"tags": [],
	"description": "",
	"content": "VIM 0. Help - :help ins-completion 1. Completion - Ctrl-n For anything specified by 'complete' - Ctrl-p For anything specified by 'complete' - Ctrl-x Ctrl-f To search files paths. Need start with path, eg. / or ./ - From help ins-completion - 1. Whole lines\t|i_CTRL-X_CTRL-L| - 2. keywords in the current file\t|i_CTRL-X_CTRL-N| - 3. keywords in 'dictionary'\t|i_CTRL-X_CTRL-K| - 4. keywords in 'thesaurus', thesaurus-style\t|i_CTRL-X_CTRL-T| - 5. keywords in the current and included files\t|i_CTRL-X_CTRL-I| - 6. tags\t|i_CTRL-X_CTRL-]| - 7. file names\t|i_CTRL-X_CTRL-F| - 8. definitions or macros\t|i_CTRL-X_CTRL-D| - 9. Vim command-line\t|i_CTRL-X_CTRL-V| - 10. User defined completion\t|i_CTRL-X_CTRL-U| - 11. omni completion\t|i_CTRL-X_CTRL-O| - 12. Spelling suggestions\t|i_CTRL-X_s| - 13. keywords in 'complete'\t|i_CTRL-N| |i_CTRL-P| 2. Cursor - Command-/ Find cursor - set cursorline - highlight CursorLine ctermbg=None ctermfg=Red - set cursorcolumn - highlight CursorColumn ctermbg=None ctermfg=Red - set nocursorline - set nocorsorcolumn - let g:netrw_winsize = 15 3. Search  - :vim /pattern/ file\tSearch for a pattern in files, See results with quickfix - :cope[n] \u0026quot; Open the quickfix window - :ccl[ose] \u0026quot; Close the quickfix window - :cn[ext] \u0026quot; Go to the next error - :cp[revious] \u0026quot; Go to the previous error - :col[der] \u0026quot; Go to older error list - :cnew[er] \u0026quot; Go to newer error list 4. Text  - d-i-\u0026quot;\tdelete all between \u0026quot;, i=inner - d-i-w\tdelete Removes the word - d-i-t\tdelete Removes the tag inside HTML tags - d-i-p\tdelete the paragraph - d-i-s\tdelete Removes the sentence - d-i-{\tdelete Removes all between { - d-i-X\tdelete Removes all between X charactes, X=' \u0026quot; { } [ ] - c-f-X\tChange until you FIND X. Find forward - c-F-X\tChange until you FIND X. Find backwards - +\tTo copy selected 'v' text to clipbaord - {count} C-G Show full path of the current file, use count=1 as default 5. Terminal  - Ctrl-B : term Opens a terminal windows horizontal by default - Ctrl-w N To scroll up and down, copy, etc. Vim style. - Press i or a To back to the terminal 6. Windows  - C-w \u0026gt;\tIncrease width - C-w \u0026lt;\tReduce width CTAGS TMUX 0. Help  - C-b ?\tHelp short cuts - C-b t Show time - C-b w\tChoose windows interactively - C-b :new New session 1. Panes  - C-d\tclose pane - C-b z\tFull screen pane - C-b o\tFocus next pane - C-b OPTION+ARROW KEY Resize pane - C-b [\tScroll, press q to leave Function vi emacs -------- -- ----- Half page down C-d M-Down Half page up C-u M-Up Next page C-f Page down Previous page C-b Page up Scroll down C-Down or C-e C-Down Scroll up C-Up or C-y C-Up Search again n n Search again in reverse N N Search backward ? C-r Search forward / C-s - C-b { Move pane unti-clockwise - C-b } Move pane clockwise - C-b C-o rotate window ‘up’ (i.e. move all panes) - C-b M-o rotate window ‘down’ - :move-pane -t \u0026lt;session_name\u0026gt; Move pane to another session - C-b ! move the current pane into a new separate window (‘break pane’) - C-b SPACE\ttoogle between layouts 2. Windows  - C-b c Create new windows - C-b p/n\tGo previous/next window - C-b #\tGo to windows # - C-b ,\tRename your Windows - C-b $\tRename session 3. Attach/Dettach  - C-b d\tDettach - tmux ls List - tmux attach -t # Attach 4. Copy paste  - Add to ~/.tmux.conf. bind P paste-buffer bind-key -t vi-copy 'v' begin-selection bind-key -t vi-copy 'y' copy-selection bind-key -t vi-copy 'r' rectangle-toggle Note that for a newer tmux version (\u0026gt;2.4), the last three lines should be replaced with: bind-key -T copy-mode-vi v send-keys -X begin-selection bind-key -T copy-mode-vi y send-keys -X copy-selection bind-key -T copy-mode-vi r send-keys -X rectangle-toggle - Select 'v' - Copy to clipboard: 'y' ( or the default enter key) - Paste: Ctrl+b P ( p is upper case ) 5. SSH AGENT  - Instead of eval $(ssh-agent -s): ps aux | grep ssh-agent export SSH_AGENT_ID=2223 ssh-add -k ~/.ssh/id_rsa iTerm2  https://gist.github.com/squarism/ae3613daf5c01a98ba3a  GIT  - PR CREATE: - gh pr list - gh pr create -t \u0026quot;TITLE \u0026quot; -b \u0026quot;Main body comments\u0026quot; - PR ADD REVIEWERS: - gh pr list -A percy - gh pr edit 4048 --add-assignee percy # primary reviewer - gh pr edit 4048 --add-reviewer user1,user2,user3 - PR REVIEW: - gh pr diff 5049 - gh pr view 4048 -c - gh pr comment 4048 -b ' Thanks' - PR MERGE: *only your pR* - gh pr merge 4048 ? What merge method would you like to use? [Use arrows to move, type to filter] \u0026gt; Create a merge commit Rebase and merge Squash and merge - gh pr merge 4048 -m - PR APPROVE: - gh pr list | grep 4078 - gh pr view 4078 - gh pr diff 4078 - gh pr review 4078 --comment -b \u0026quot;LGTM\u0026quot; - gh pr review 4078 --approve - PR CLOSE: - gh pr close {\u0026lt;number\u0026gt; | \u0026lt;url\u0026gt; | \u0026lt;branch\u0026gt;} [flags] - PR atlantis - gh pr comment 4097 -b \u0026quot;please apply\u0026quot; - gh pr comment 3234 -b '!merge' - MISC: - Need to find where is StdCmd defined? $ git grep 'class StdCmd' - Need to find what templates use favicon_iurl ? $ git grep favicon_iurl | grep .tmpl - Who touched the file last? $ git log /file/path - Who touched that line last? $ git blame /file/path - Who probably has the most knowledge of that file? $ whotobug -v /file/path $ git who /file/path - Reload/re create -$ git fetch -a -$ git reset --hard - List of branches by year - git branch -a --sort=-committerdate --format='%(refname)%09%(committerdate)' | awk '{ print $1,$6}' | grep 2020 | sed 's/refs\\/remotes\\/origin\\///g' | awk '{ print \u0026quot;git push origin --delete \u0026quot;$1}' \u0026gt; 2020.list Source graph  - srcGraph login - srcgraph search 'yatiri' net  - dig www.yahoo.com Vimium  - Ref: - https://github.com/philc/vimium/blob/master/README.md - Current page: d scroll down half a page u scroll up half a page f open a link in the current tab F open a link in a new tab r reload gs view source yy copy the current url to the clipboard yf copy a link url to the clipboard \u0026gt;\u0026gt; Move tab to the right \u0026lt;\u0026lt; Move tab to the left - Navigation o Open URL, bookmark, or history entry O Open URL, bookmark, history entry in a new tab b Open bookmark B Open bookmark in a new tab - Manipulating tabs: t create tab yt duplicate current tab x close current tab X restore closed tab (i.e. unwind the 'x' command) T search through your open tabs W move current tab to new window - Marks: ma, mA set local mark \u0026quot;a\u0026quot; (global mark \u0026quot;A\u0026quot;) `a, `A jump to local mark \u0026quot;a\u0026quot; (global mark \u0026quot;A\u0026quot;) `` jump back to the position before the previous jump -- that is, before the previous gg, G, n, N, / or `a - Extra: ]], [[ Follow the link labeled 'next' or '\u0026gt;' ('previous' or '\u0026lt;') - helpful for browsing paginated sites gi focus the first (or n-th) text input box on the page. Use \u0026lt;tab\u0026gt; to cycle through options. gu go up one level in the URL hierarchy gU go up to root of the URL hierarchy ge edit the current URL gE edit the current URL and open in a new tab jq  - jq .ResourceRecordSets[0,4] - jq .ResourceRecordSets[0:4] - Getting keys - jq '.ResourceRecordSets[0] | keys' - Getting lenght - jq '.ResourceRecordSets[0] | length' - Select all XTR entries - jq ' .ResourceRecordSets[] | select (.Type == \u0026quot;XTR\u0026quot;)' a - Reg exp, all names that start with 5 - jq ' .ResourceRecordSets[] | select (.Name |test (\u0026quot;^5\u0026quot;))' a pdb # https://web.stanford.edu/class/physics91si/2013/handouts/Pdb_Commands.pdf - l list - b list all break points - b 12 Set break point at line 12 - b myFunction\tSet breakpoint to the function called myFunction - p(n) print the value of variable n - cl clear all breakpoints - c Continue - python -m pdb main.py -s applicationengine --dry-run python  - venv: - python3 -m venv /path/to/new/virtual/environment - source env/bin/activate New bluetoooh keyboard  - # OPTION + 3 - ~ FN+z - | SHIFT+| - ` CONTROL+OPTION+ESC - screenshot CMD+SHIFT+4 AWS   Documents    - aws ssm list-documents --query 'DocumentIdentifiers[].Owner' | sort | uniq -c - aws ssm get-document --name CustomRunPuppetUpdateCommand   R53    - Get hosted zones - aws route53 list-hosted-zones - Get the most used: check the ID - To sort: - aws route53 list-hosted-zones --query 'sort_by(HostedZones[],\u0026amp;ResourceRecordSetCount)[]' - aws route53 get-hosted-zone --id /hostedzone/Z234asasdf2342S3 - List of hosted zones - aws route53 list-hosted-zones - List all record names in a specific hosted zone id - aws route53 list-resource-record-sets --hosted-zone-id \u0026quot;/hostedzone/asdfasdf\u0026quot; - Show specific record name - aws route53 list-resource-record-sets --hosted-zone-id \u0026quot;/hostedzone/fwfwfw\u0026quot; --query \u0026quot;ResourceRecordSets[?Name == '15.1.133.10.in-addr.arpa.']\u0026quot;   Load balancers    - aws elb describe-load-balancers - aws elb describe-load-balancers --query 'LoadBalancerDescriptions[0].Instances[]' - aws elb describe-load-balancers --query 'LoadBalancerDescriptions[0].ListenerDescriptions'   VPC    - aws ec2 describe-vpcs - Get subnets: - aws ec2 describe-subnets --query 'Subnets[].[CidrBlock,SubnetId]' --output table   RDS DB    - aws rds describe-db-instances | jq '.DBInstances[] | {DBInstanceIdentifier,DBInstanceArn}'   SSM    - Show one specific parameter aws ssm get-parameter --name \u0026quot;/eks-clusters/prometheus-test/secrets/cloudhealth-restful-key\u0026quot; --profile test - List all paremeters aws ssm describe-parameters   S3    - List all content including hideen files .dot $ aws s3 ls tf-resource-test/vpc --recursive "
},
{
	"uri": "http://example.org/research/swim/",
	"title": "Swim",
	"tags": [],
	"description": "",
	"content": "Objective and evaluation metrics I am planning to go back to swim and this time I have 60 minutes to practice swim as best as I can. The main objectives for this initial stage are:\n List the steps to execute in order to ensure leave my main door of my place and back to the same door within 60 minutes. Reach a swim speed of 1500 meters / 30 minutes = 50 meters per minute free style. This is because my shoulder requires recovering. Analyze whether the list of steps can be standarized in my dailly life.  Steps to ensure 60 minutes This is an iterative process and as it is expected, the initial results only set the start point or pilot scenario. Each section requires to be detailed as specific as possible to show what actions were executed during that period. These sub-parts required are part of the data collection, as they are going to help to list the steps required to have an optimized procedure.\nI divided the whole process in parts\nPART I; Go to pool  home-readerIn: Time spend from my door to swimming pool card reader. readerIn-room: Time spend from after passing the card reader until the changing room room-pool: Time spend from the changing room to the entrance of the pool  PART II: Swim Laps: Number of laps during 30 minutes  PART III: Go back home pool-room: Time spend from entrance pool until chaging-room room-shower-room: Time spend to have a shower and back to the changing room room-readerOut: Time spend form changing room to the exit card reader machines readerOut-home: Time spend from the exit card reaader in the pool to main door home  Data collection NOTES:\n Time in minutes and seconds     date-time home-readerIn readerIn-room room-pool laps pool-room room-shower-room room-readerOut readerOut-home     20211010 16:30 8 3 4 8 2 8 6 2                          Expanding steps PART I: Go to pool   home-readerIn: Time spend from my door to swimming pool card reader.\n 1.1 Find the best path to minize distance\n   readerIn-room: Time spend from after passing the card reader until the changing room\n 2.1 Meassure queue times\n  2.2 is there a time in the day where less people go to swim?\n   room-pool: Time spend from the changing room to the entrance of the pool\n 3.1 Time spend on taking out cloths\n  3.2 How swim gears are located in the bag\n  3.3 How cloths are places in the locker? the order matters to reduce searching\n  3.4 Steps and places to put clothes and swim gears\n  3.5 Procedure to un-dress in less time possible\n  3.6 Procedure to allocate the items in the bag to minimize time used at home\n   PART II: Swim Laps: Number of laps during 30 minutes  PART III: Go back home  pool-room: Time spend from entrance pool until chaging-room\n 5.1 When to take out my keys so I do not waste time at the door of the locker\n   room-shower-room: Time spend to have a shower and back to the changing room\n 6.1 How to order and locate shower items to it reduce time to get them and go to shower\n  6.2 How to shower properly in less time possible? list steps\n   room-readerOut: Time spend form changing room to the exit card reader machines\n 7.1 Procedure to dress in lest time\n  7.2 Procedure to place back all items in the back\n  7.3 Procedure to dry hair, dry feet and wear shocks and shoes before leaving the changing rooms\n  7.4 Where to put the swimming card to it is easy to get. Avoid searching\n   readerOut-home: Time spend from the exit card reaader in the pool to main door home\n 8.1 Look for the fastest path to reach home and normal speed.\n  8.2 Drink water while walking?\n   "
},
{
	"uri": "http://example.org/notes/regex/",
	"title": "Regex",
	"tags": [],
	"description": "",
	"content": "Control characters Control sequence Examples Metacharacters   [adb]\tSearch for places where a or b or c exists\n  [^adb]\tSearch for places where a or b or c DOES NOT exist\n  [a-f]\tSearch for places where a,b,c,d,e,f exists\n  [A-z]\tSearch for places where A-Z,a-z,and all ascii codes between Z-a\n  [9-0]\tInvalid. [0-9] is valid.\n  [.?] [.+]\tDot plus quantifier = slow\n  Reference  https://regexper.com/ https://regexcheatsheet.com/  "
},
{
	"uri": "http://example.org/notes/ruby/",
	"title": "Ruby",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/notes/vim/",
	"title": "Vim",
	"tags": [],
	"description": "",
	"content": "Vim Normal mode Press A Move to after last character on a line to insert a new character. Press a To insert text just after the cursor Type dw to delete a word. Type d$ to delete to the end of the line. Type w until the start of the next word, EXCLUDING its first character. Type e to the end of the current word, INCLUDING the last character. Type $ to the end of the line, INCLUDING the last character. Type 2w to move the cursor two words forward. Type 3e to move the cursor to the end of the third word forward. Type 0 (zero) to move to the start of the line. Type d2w to delete the two UPPER CASE words. Type 2dd to delete two lines. Press u to undo the last commands Press U to fix(undo) a whole line. Press Ctrl-R (keeping CTRL key pressed while hitting R) to redo the commands (undo the undo\u0026#39;s). Type p to put/paste previously deleted text after the cursor. Type r to replace the character at the cursor Type R to replace more than one character Press ce To change until the end of a word, c=change, e=end Type c$ To change until the end of the line ############################################## Type % to find a matching ),], or } Type :s/old/new/g to substitute \u0026#39;new\u0026#39; for \u0026#39;old\u0026#39; To change every occurrence of a character string between two lines, type :#,#s/old/new/g where #,# are the line numbers of the range of lines where the substitution is to be done. Type :%s/old/new/g to change every occurrence in the whole file. Type :%s/old/new/gc to find every occurrence in the whole file, with a prompt whether to substitute or not. Type o To insert a blank line below, o lower case Type o To insert a blank line above, O upper case :set ic\tIgnore case sensitive. \u0026#34;noic\u0026#34; to disable. :set hls Highlight search. \u0026#34;nohls\u0026#34; to disable :set is Increase or progresive search. \u0026#34;nois\u0026#34; to disable :set nocp Type :e and press CTRL-d to list all commands that start with \u0026#34;e\u0026#34; and TAB to select ############################################### Ctrl-g Location of the current file Type the number of the line you were on and then G . This will return you to the line you were on when you first pressed CTRL-G. / Search forwards ? Search backwards CTRL-o \u0026#34;o\u0026#34; lowercase to go back to where you came from. CTRL-I goes forward Press v + : w FILENAME Press v, select lines, Press :, and type \u0026#34;w test.txt\u0026#34;. This will save in test.txt the selected lines. Type \u0026#34;w! test.txt\u0026#34; to overwrite Type :r FILENAME It will insert the content of FILENAME in the cursor location For example type :r !ls to insert existing files ################################################ :help w :help c_CTRL-D :help insert-index :help normal-index :help user-manual :help vimrc-intro :help windows Vim with Netwr let g:netrw_banner = 0 let g:netrw_liststyle = 3 let g:netrw_browse_split = 4 let g:netrw_altv = 1 let g:netrw_winsize = 25 augroup ProjectDrawer autocmd! autocmd VimEnter * :Vexplore augroup END ---\t-----------------\t---- Map\tQuick Explanation\tLink ---\t-----------------\t---- \u0026lt;F1\u0026gt;\tCauses Netrw to issue help \u0026lt;cr\u0026gt;\tNetrw will enter the directory or read the file |netrw-cr| \u0026lt;del\u0026gt;\tNetrw will attempt to remove the file/directory |netrw-del| -\tMakes Netrw go up one directory |netrw--| a\tToggles between normal display, |netrw-a| hiding (suppress display of files matching g:netrw_list_hide) showing (display only files which match g:netrw_list_hide) c\tMake browsing directory the current directory |netrw-c| C\tSetting the editing window |netrw-C| d\tMake a directory |netrw-d| D\tAttempt to remove the file(s)/directory(ies) |netrw-D| gb\tGo to previous bookmarked directory |netrw-gb| gh\tQuick hide/unhide of dot-files |netrw-gh| \u0026lt;c-h\u0026gt;\tEdit file hiding list |netrw-ctrl-h| i\tCycle between thin, long, wide, and tree listings |netrw-i| \u0026lt;c-l\u0026gt;\tCauses Netrw to refresh the directory listing |netrw-ctrl-l| mb\tBookmark current directory |netrw-mb| mc\tCopy marked files to marked-file target directory |netrw-mc| md\tApply diff to marked files (up to 3) |netrw-md| me\tPlace marked files on arg list and edit them |netrw-me| mf\tMark a file |netrw-mf| mh\tToggle marked file suffices\u0026#39; presence on hiding list |netrw-mh| mm\tMove marked files to marked-file target directory |netrw-mm| mp\tPrint marked files |netrw-mp| mr\tMark files satisfying a shell-style |regexp| |netrw-mr| mt\tCurrent browsing directory becomes markfile target |netrw-mt| mT\tApply ctags to marked files |netrw-mT| mu\tUnmark all marked files |netrw-mu| mx\tApply arbitrary shell command to marked files |netrw-mx| mz\tCompress/decompress marked files |netrw-mz| o\tEnter the file/directory under the cursor in a new |netrw-o| browser window. A horizontal split is used. O\tObtain a file specified by cursor |netrw-O| p\tPreview the file |netrw-p| P\tBrowse in the previously used window |netrw-P| qb\tList bookmarked directories and history |netrw-qb| qf\tDisplay information on file |netrw-qf| r\tReverse sorting order |netrw-r| R\tRename the designed file(s)/directory(ies) |netrw-R| s\tSelect sorting style: by name, time, or file size |netrw-s| S\tSpecify suffix priority for name-sorting |netrw-S| t\tEnter the file/directory under the cursor in a new tab|netrw-tYXXY u\tChange to recently-visited directory |netrw-u| U\tChange to subsequently-visited directory |netrw-U| v\tEnter the file/directory under the cursor in a new |netrw-v| browser window. A vertical split is used. x\tView file with an associated program |netrw-x| %\tOpen a new file in netrw\u0026#39;s current directory |netrw-%| [3]\nFor inside a windows and in normal mode https://catswhocode.com/vim-commands/ gx Open URL in default browser gt Next tab Vim scripts Reference https://shapeshed.com/vim-netrw/\nhttps://learnvimscriptthehardway.stevelosh.com/\n[3] http://vimdoc.sourceforge.net/htmldoc/pi_netrw.html#netrw\n[4] https://iccf-holland.org/vim_books.html\n[5] https://vonheikemen.github.io/devlog/tools/using-netrw-vim-builtin-file-explorer/\nCommand line inside Vim [6] https://unix.stackexchange.com/questions/523090/vim-autostart-with-vexplore-and-terminal\n"
},
{
	"uri": "http://example.org/notes/mytools/",
	"title": "MyTools",
	"tags": [],
	"description": "",
	"content": "This page contains the list of tools I used in my laptop. It was used to set a ready-to-use host from IAC.\nA. OS I am taking here the case of Ubuntu, but this should work with similar distros of the same family.\nB. Tools  Update  $ sudo apt update -y $ sudo apt install git vim -y $ git config --global user.email percyyperezdante@gmail.com "
},
{
	"uri": "http://example.org/devopssec/3h/general/",
	"title": "General",
	"tags": [],
	"description": "",
	"content": "Introduction  Initial general references  Cyberthreat in real time Threat map check point EC Council Code of ethics Technet evaluation center    "
},
{
	"uri": "http://example.org/devops/cloudsecurity/",
	"title": "CloudSecurity",
	"tags": [],
	"description": "",
	"content": "Introduction to coud security Cloud deployment methods Software delivery mechanism  SaaS: Hands-off approach, where all hardware, software and security controls are managed by the provider, e.g. email. PaaS: Where you place your own source code on the top of the provider\u0026rsquo;s infrastructure or platform, e.g. Heroku. IaaS: Where you build your own infrastrucuture and deploy your code on the top.  Challenges and barriers when you move to the cloud include:\n Compliance and regulation of cloud providers. Application security. Identity and access management (IAM), which features include:  Unified interface for administration of policies and rulesets MFA, password policies and logging RBAC at granular level Programatic controls to manage IAM policies and updates applied through automated tools Single Sign-on (SSO) integration with providers Custom workflow creation and centralized audits capabilities Separation of duties, helps on auditing    Below some of the best practices: - Always craete individual users, never share credentials - Do not use root user for normal operation, remove any access key for root - Use groups to assign permissions to IAM users, not to individuals - Enable logging for all cloud API calls - Audit IAM accounts in regular bases - Restrict access to cloud resources using ip addresses - Turno on billing alerts to detect suspicious events - Requets to users to use multi-factor authentatication, including command lines interfaces - Assign users and groups to have emough access permissions\nMost of the cloud providers offer a Virtual Private Cloud (VPC) whic his a representation of a physical network or data center. It is used for communication between resources within a project. It also isolates resources from outside world.\nIn VPC, every resource receives an individual IP address. This includes: - Traffic to and from instances inside the VPC are controlled with firewall rules - Resources inside VPC communicate each other using IP addresses - Instances within VPC can communicate witht eh cloud providers APIs and services\nTo protect communication: - Virtual Private Networks (VPN): Offers encrypted traffic between two networks over public networks such as internet. - Web traffic Encryption: It is important to use TLS encruptio from cloud load balancers to any VMs or containers in the backend.\nNote that inmutable infrastructure in here means, that instances on the cloud are replaced rather than updated: destroy-versus-updatei\nData security To ensure object storage security: - Access control: - Grant permissions to all object requires access only - Do not make data public if it is not needed - Double check with your provider to ensure proper configuration - Use third-party audit tools to ensure no leaks - Logging: - Data retention: - Cloud providers have the ability to set user-defined retention policy - An object can be deleted once its age is greater than the retentino policy set\nPersistent disks - Backup disks periodically - Compress and store them in a location of your choice  Containers - To enforce the principle of least privilege, create an user with known ID int he dockerfile and run the application process as that user - Verify the integrity of an docker image using its SHA256 - Scan vulnerabilities on your docker images using command line tools such as Clair or Anchore in your CI/CD pipeline  Serveless security - Scan your code base with static or dynamic analysis tools - Ensure third party libraries are free of vulnerabilities - Threat model regularly - Construct a robust loggin pipeline - Apply the least privilege principle in your functions - Protect your code from DoS and Resrouce-exhaustion attacks - Consider the following resources: - [OWASP ServerlessGoat](https://www.owasp.org/index.php/OWASP_Serverless_Goat) - [OWASP Serverless Top 10](https://www.owasp.org/index.php/OWASP_Serverless_Top_10_Project)  Monitoring - App Metrics: - Who logged in, from where , when - Get attack payload submitted in the request - HTTP error codes - Response times - Vulnerability scanner detection - Host metrics: - Resource utilization, CPU, memory, threads - SSH access - Port opened, closed - Malware detection - File creation / deletion - Thrid party libraries installed - Network metrics: - Throughput - Resource exhaustion - Outbound network calls - DNS queries - Cloud API metrics: - Who logged into - AWS cloud trail: - Console sign-in events - Security group changes - CloudTrail logging changes - VPC changes  Analysis Here some overall indicatores: - Number of 500 erros per period of time on your backend - If number of loggin attemps with same user - Attemps to access another account\nNote that logs should offer high integrity and not able to be deleted or modified easly.\nGCP Introduction Some recommendations:\n The GCP cloud platform itself is PCI DSS compliant. This does not mean that a company building an application in GCP is also in compliance with PCI DSS. Note that GCP is responsible for the compliance and security of the cloud infrastructure, and the company using GCP is responsible for the data and applications built on top of the platform. Isolate development and production environments and restrict developer access to sensitive production data. Analyse application using static or dynamic scanning tools  Key concepts   Location:\n Glabal: When a resource is accessed across regions and zones, such as disk snapshots, networks, firewalls, images Regional: Resource that can only be accessed by other resource in the same region. It includes static external IP addresses, subnets and regional persistent disks Zonal: Resource hosted in a specific zone, such as CMs, disks.    Services:\n GCP services provides access to the resources.    Client libraries:\n App APIs provide access to services Admin APIs provide funtionality for resource management.    To consider about DoS attacks\n Use GCP virtual networks to isolate your resources from internet Use firewall to limit access Reduce number of public IP addresses you deploy In the evento of a DoS use autoscaling to absorv the load and stay online Use CDN to offload traffic Consider Cloud Armor for defense    Compliance:\n GCP is a PCI DSS certified cloud service provider and provides documentation If you are storing cardholder data in an GCP environment, you are responsible for managing your own PCI DSS compliance certification. PCI compliance is not inherited from GCP. A qualified PCI auditor must inspect your architecture to ensure it complies with the standard.    IAM   General features\n Unified interface for IAM administration Offers multi-factor authentication, password policies and logging RBAC to usres and services at granular level Programiatic controls Single Sign On capabilities Custom workflow creation and centralized audit    Google cloud identity and access management features:\n SSO: single sign on Pre configured and custom roles Built in audit trail Access through CLI, web interface and SDKs iIntegration to directory services such as Google Cloud Identity    Google cloud identity features:\n SSO support using SAML2.0 and OpenID connect Device managemente policy enforcement for a variety of device types, Android and iOS MFA _ Application catalog creation of thrird-party SaaS apps. Ability to integrate other cloud directoires as well as on-premise directoreis into Google Cloud Identity    Configuration :\n Roles: It is a collection of permissions assigned to a user, group or service account. This includes:  Primitive roles:  Viewer: Read-only Editor: All permissions to edit Owner: All permssions and acttions such as:  Managing roles and permissions for a project and resources Setting up billing     Predefined roles  More granular access than primitives E.g. Storage object Creator role: to create buckets, but can not delete or overwrite them   Custom roles.  Enforce the principle of least privilege: access only to what you need or is enough      Policies:  It is a list of members bound to roles A policy defines who has what kind of access   Service account:  For non-human access Generally uses hard code tokens to prove identity  GCP managed keys: Used by Cloud platform services ( App Engine), not downloadable and two week rotation User-managed keys: Created, downloaded and managed by users. Valid for 10 years.        IAM best practices:\n Organize IAM policies simliar to your company structure, each team responsable for its own app and services Grant roles to a group, not to a individual Grant roles with atomic scope needed Use labels to annotate, group and filter resources Audir regularly Enforce MFA Customize password complexity policy and SSO When possible set predefined roles or custom roles over primitive roles Restrict which users can create or modify services accounts Becareful when granting the owner role Rotate service account keys Label service account keys to decrease confusion Do not push service account keys to code repositories Minimize access to GCP audit logs for each user themselves, e.g. do not allow to delete logs Backup logs    Network and data security  VPC  General:  Traffic to and from instances withtin a VPC are controlled by firewalls rules Resources within a VPC are able to communciate each other using internal IP address Instances inside VPC are able to communicate with official Google APIs   Subnets:  VPC consist of a range of internal IP addresses partitioned into subnets VPC are global resources, subnets are regional. A VPC can containt multiple subnets in different regions Internal IP address are contained in a Subnet, not VPC. You need at least one subnet in a VPC before use it.   Networking peering  Allos for private connectivity across two VPC networks regarless of whether the VPC is in the same org or project   Access using IAM  Create or modify VPC is for admins only   Flow logs  To capture sample network flow traffic sent to and from a resource within a VPC subnet E.g. Stackdriver Logging Capture period is five secons internvals for TCP and UDP   Firewall:  Each VPC has own firewall for control traffic out of the VPC and between VMs inside VPC Default policy: deny-all   Data Storage:  Apply RBAC:  Enforce fine grained access control using IAM permissions Do not make bucket public Enable logging Data retention :  User-defined data retentino policy for new or existing buckets Locked data, which can never be removed, or after certain time Persistent disk: supports custom-supplied encryption keys, this should be backed up periodically. Google cloud SQL: It is a database-as-a-service that supports MYSQL and PostgreSQL.       Encrypting data in transit:  Cloud VPN:  GCP offers a VPN to connect your VPC to on-premises or other clouds infrastructures Encrypt traffic as it travels IDeal to secure low-volume data connections   Cloud Interconnect  For higher bandwidth and encrypted connections It is a way to extend your on-prem network to Google\u0026rsquo;s network Cloud VPN can not be used over Cloud Interconnect, you need to create a VPN gateway Using CLoud Load Balancing you can terminet TLS at the load balancer. This needs to set certs and keys on each VM with in VPC to ensure end-to-end encryption        References https://www.modsecurity.org/ http://appsensor.org/\nhttps://www.yubico.com/products/ https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html https://owasp.org/www-project-application-security-verification-standard/ https://www.npmjs.com/package/bcrypt https://www.synopsys.com/software-integrity.html https://pages.nist.gov/800-63-3/sp800-63-3.html https://wiki.owasp.org/index.php/Category:Access_Control https://owasp.org/www-project-secure-headers/#X-XSS-Protection https://owasp.org/www-community/HttpOnly\nhttps://owasp.org/www-project-top-ten/ https://bobby-tables.com/\n"
},
{
	"uri": "http://example.org/notes/ansible/",
	"title": "Ansible",
	"tags": [],
	"description": "",
	"content": "General Ansible cnetralizes model:\n  Adhoc example:\n$ ansible -m copy -a \u0026#34;src=orignalFile.txt dest=finalFile.txt\u0026#34; --check --diff localhost Where\n Check = to run in dry mode Diff = Shows the difference before and after apply ansible \u0026ldquo;copy\u0026rdquo; is a module, list of all ansible modules in here \u0026ldquo;m\u0026rdquo; module name \u0026ldquo;a\u0026rdquo; arguments  More examples   Playbook   A playbook uses a Yaml file as main place to read from.\n  A task can be visualise as:\n    To run playbook:\n$ ansile-playbook playbookFile.yml   Ansible-playbook calls the \u0026ldquo;setup\u0026rdquo; module, which is mask as \u0026ldquo;Gathering facts \u0026quot; in the logs, and it can be executed manually as:\n$ ansible -m setup localhost This helps to understand the current state of an environment.\nNOTE:\nYou can disable this feature \u0026ldquo;gathering facts\u0026rdquo; in the playbook, see below. It will speed up the run.\n  More output lines\nYou can add more verbose, \u0026ldquo;-v\u0026rdquo;, to the output:\n$ ansible-playbook playbook.yml -v $ ansible-playbook playbook.yml -vv $ ansible-playbook playbook.yml -vvv $ ansible-playbook playbook.yml -vvvv   Output meaning:\nTo understand the output logs, you can refer to the documentation of the module you are working on, or you can use ansible-doc to get information about a module from command line\n$ ansible-doc copy   Another example   Inventory   Ansible ignore the following extensions:\n$ ansible-config --list ... INVENTORY_IGNORE_EXTS: default: \u0026#39;{{(BLACKLIST_EXTS + (\u0026#39;\u0026#39;.orig\u0026#39;\u0026#39;, \u0026#39;\u0026#39;.ini\u0026#39;\u0026#39;, \u0026#39;\u0026#39;.cfg\u0026#39;\u0026#39;, \u0026#39;\u0026#39;.retry\u0026#39;\u0026#39;))}}\u0026#39; description: List of extensions to ignore when using a directory as an inventory source ...   Ansible does not recommend to use \u0026ldquo;.\u0026rdquo; as part of the name of a hosts or group of hosts\nNot replacing invalid character(s) \u0026#34;{\u0026#39;.\u0026#39;}\u0026#34; in group name (new_group_jenkins_agents.ol7) [WARNING]: Invalid characters were found in group names but not replaced, use -vvvv to see details Not replacing invalid character(s) \u0026#34;{\u0026#39;.\u0026#39;}\u0026#34; in group name (newgroup_jenkins_agents.ol7) Not replacing invalid character(s) \u0026#34;{\u0026#39;.\u0026#39;}\u0026#34; in group name (oldgroup_jenkins_agents.olh6)   Ansible graph presents hosts in different format:\n$ ansible-inventory --graph [WARNING]: Invalid characters were found in group names but not replaced, use -vvvv to see details @all: |--@backup_coordinator_group.ol7: | |--hostonw.dcnet |--@bitbucketgroup.ol6: | |--host2.dcnet | |--host2b.dcnet |--@bitbucketgroup.ol6: | |--host3.cdnet |--@health_checker_group.ol7: | |--host4.dcnet   To run on any of this groups:\n$ ansible -m command -a \u0026#34;ls -ltr \u0026#34; bitbucketgroup $ ansible -m command -a \u0026#34;ls -ltr \u0026#34; host4.dcnet   Some useful links\n Ansible error handling  Ansible use of varibles     Steps to consider to use inventories:\n Config ansible using \u0026ldquo;ansible.cfg\u0026rdquo; [default] # disale host_key_chkcings # https://docs.ansible.com/ansible/latest/user_guide/connection_details.html host_key_checking = False inventory=myDirectory # export ANSIBLE_INVENTORY = /path/directory/inventory # FYI : \u0026#34;vagrant ssh-config \u0026#34; is a great guide for configuring ansible to # connect directly to VMs created by vagrant     "
},
{
	"uri": "http://example.org/devops/aws/devopseng/codecommitcmd/",
	"title": "Codecommitcmd",
	"tags": [],
	"description": "",
	"content": "Below are suggested steps to work with CodeCommit from Command line\n  NOTE   You need to have AWSCodeCommitPowerUser  or similar in your AWS account profile in the section Permission policies*     Be sure you have a public key upload into your IAM profile\n  To check if you have already one or more public keys in AIM\n$ aws iam list-ssh-public-keys { \u0026#34;SSHPublicKeys\u0026#34;: [] }   If you have not any public key updated, then update one:\n$ aws iam upload-ssh-public-key \\  --user-name percy \\  --ssh-public-key-body file:///home/percy/.ssh/id_rsa.pub { \u0026#34;SSHPublicKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;percy\u0026#34;, \u0026#34;SSHPublicKeyId\u0026#34;: \u0026#34;DDDDHJCVLNXXXXX\u0026#34;, \u0026#34;Fingerprint\u0026#34;: \u0026#34;as:as:as:as:sa;as;ads\u0026#34;, \u0026#34;SSHPublicKeyBody\u0026#34;: \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC7NfDLDKNrUSU43MT+GRUME2cXkTs3Du4mnZqQSRNr2ZgD4y5N2En/9FNC6kHyOiEe8MCYIDAI/cZ017QmhPmYzp/n7fC4owMXIng6h0th097zUoYJ97r/P+gYufJB9rDo8NpmAnI/DPA4VNv+SfMm4TVBI1t4XbPiaIRhn3Nt972M0DDJsYg5IaC5xl22V3J/dqfwDvUpyNUsW0I60MtiTp1MRgGt5bmOdLmTaGsWffyIcMH5AT9MkjFbQ6OyG4VPQ/ohRyYS02CRzR6nmrqUu1wowoXyrBttVavg3c4B7053ig5D1OU8mwFeIm2e/yKXLQskUk3LnubMiKAyigpkpL/17bxFF19KGV9tmT/CBmXDPVAh+6awerasfdasfdauYX5LvoNGS3x5vmXuUmQJnNkGyV8bZu6h2erkqkF1ULtpCaLq2g+WXnA+ymOfXhmdDVWFtdvEZQ91E8M6ltgKcFb+wNFXRz5qiUwNCyTfkrtN+zHUOsvpIRMGL2U2vjyolAzjuVH8g+Vw5Sg96KSjv2qHiHdigbfRsI7ya+\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;UploadDate\u0026#34;: \u0026#34;2020-10-21T12:06:48+00:00\u0026#34; } } Now you can double check that you have your key uploaded\n$ aws iam list-ssh-public-keys { \u0026#34;SSHPublicKeys\u0026#34;: [ { \u0026#34;UserName\u0026#34;: \u0026#34;percy\u0026#34;, \u0026#34;SSHPublicKeyId\u0026#34;: \u0026#34;DDDDDDXXXHaADFDASDF\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;UploadDate\u0026#34;: \u0026#34;2020-10-21T12:06:48+00:00\u0026#34; } ] } For a specific user\n$ aws iam list-ssh-public-keys --user-name percy From this output we need the SSHPublicKeyId\n    Create a repository in CodeComimt\n  Minimal\n$ aws codecommit create-repository \\  --repository-name MyDemoRepo \\  --repository-description \u0026#34;A demo repository\u0026#34; { \u0026#34;repositoryMetadata\u0026#34;: { \u0026#34;accountId\u0026#34;: \u0026#34;2341234233434\u0026#34;, \u0026#34;repositoryId\u0026#34;: \u0026#34;27ada71-asfdasfdasdf-sdfasdfas\u0026#34;, \u0026#34;repositoryName\u0026#34;: \u0026#34;MyDemoRepo2\u0026#34;, \u0026#34;repositoryDescription\u0026#34;: \u0026#34;My demonstration repository2\u0026#34;, \u0026#34;lastModifiedDate\u0026#34;: \u0026#34;2020-10-21T12:52:05.031000+00:00\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;2020-10-21T12:52:05.031000+00:00\u0026#34;, \u0026#34;cloneUrlHttp\u0026#34;: \u0026#34;https://git-codecommit.us-east-2.amazonaws.com/v1/repos/MyDemoRepo2\u0026#34;, \u0026#34;cloneUrlSsh\u0026#34;: \u0026#34;ssh://git-codecommit.us-east-2.amazonaws.com/v1/repos/MyDemoRepo2\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:codecommit:us-east-2:12334345234523:MyDemoRepo2\u0026#34; } } From here we will need the cloneUrlSsh\n  Creating a repository with TAGS\n$ aws codecommit create-repository \\  --repository-name MyDemoRepo \\  --repository-description \u0026#34;A demo repository\u0026#34; --tags Team=SuperDev     Clone a remote repository\n$ git clone ssh://SSHPublicKeyId@cloneUrlSsh # For example $ git clone ssh://SDFSDFGDFIDFGSF@git-codecommit.us-east-2.amazonaws.com/v1/repos/MyDemoRepo Where you can get your SSHPublicKeyId and cloneUrlSsh from\n$ aws iam list-ssh-public-keys --user-name percy $ aws codecommit get-repository --repository-name MyDemoRepo   Testing\n$ cd MyDemoRepo $ echo \u0026#34; Jallalla ...\u0026#34;\u0026gt; test.txt $ git add . $ git commit -m \u0026#34;Jallalla message\u0026#34; $ git push origin master You can verify this by login to the AWS console and browsing the \u0026ldquo;CODE COMMIT\u0026rdquo; console\n  References:\n AWS codecommit User guide AWS CLI codecommit reference manual    "
},
{
	"uri": "http://example.org/devops/aws/devopseng/ciautomation/",
	"title": "CIAutomation",
	"tags": [],
	"description": "",
	"content": "II Designinh a Source control strategy with CodeCommit   Devops covers at least the following areas:\n Development workflows Test, build, and deploy applications Infrastructure as a code    Why AWS\n AWS is public cloud leader AWS is a full platform of integration with other services.    What is Devops\n It is a blanket term for tooling, practices, and ideas that allow organization sto build better software - faster. It includes:\n  Source control systems Agile software technologies Infrastructure as a code Automated testing Automated deployment    What is CI/CD\n CI= Continuous integraation. This includes:  Commit your code to SCM Build your code Run tests   Continuous Delivery includes all stages in CI plus the followings:  Deploy to QA for testing Request manual approval Deploy to Production   Continouos Deployment includes all stages mentioned before, except the \u0026ldquo;request for manaul approval \u0026quot;    Version control systems, VCS\n Distributed VCS:  Mercurial Git   Centralised VCS  SVN CVS Perforce      Git flow\n Complicated Great for maintain multiple versions of software Popular before web Github flow is well suited to web development: Vincent Driessen    Github flow\n Popular for web development Encourage best practices with frequent smaller merges Well suited for CI/CD practices.    Benefits of CodeCommit-\n You do not maintain the infra hosting your code user and team management Permission management Integration with CI/CD tooling and testing workflows Allow you code review and pull request approval workflow    Working with AWS CodeCommit\n  Using admin account create a CodeCommit repo\n  Add IAM user with specific permissions\n  Users can generate credentials and used them to get CodeCommit repo\n    How to use CodeCommit\n  Open the CodeCommit console\n  Click Create repository\n  Type repository name\n  Click Create, this will create a repo using the root account. WE need to create IAM\n  Go to the IAM console\n  Creat new user by clicking \u0026ldquo;Users\u0026rdquo;, left side\n  Type name of the user, select Programatic access and AWS management console access, click next\n  Add permissions to this user:\n You can create a group, click create group Enter group name, for example : developers Select policy:  You can select \u0026ldquo;poweruseraccess \u0026ldquo;policy if you do not know what to choose. Also you can select \u0026ldquo;codecommit power user\u0026rdquo; policy Click create group   Now that we have created a group, click it and it will add the user to this group You can add tags if you need group users. Click create user  NOTE: Once you click create, you will have ONLY ONE CHANGE TO copy - Secret access key - Password - Access key ID - Sigin link\n **----\u0026gt; Copy them to a safe place**      First push to CodeCommit\n Open the Signin as IAM user page Enter you account ID, IAM user name, and password Go then to the CodeCommit console  We need to create an SSH key for the repository we will work on: - Click on the repository - Click on SSH - From you local directory, generate a pair of SSH keys. - Upload the PUBLIC key in the AIM access manager - Open AIM console - CLick on users, and find your user - Click en security credentials - CLick on \u0026ldquo;upload ssh key public\u0026rdquo; - Copy the SSH KEY ID\n - Add an entry in your `.ssh/config` file with the following Host git-codecomit.*.amazonaws.com User YOUR AIM SSH KEY ID Identify ~/.ssh/id_rsa \u0026lt;-- private key Note: - .ssh/config permission should be 600 - Now you can clone from your local as: $ git clone ssh://git-codecommit.us-east-1.amazonaws.clm/v1/repos/test    Codecommit from command line  III Infrastrcuture as a code   Benefits:\n  Not expensive, cost management\n You can allocate cost for all resources. For example by tagging the resource You can have stimated costs for each stack Helps you to avoid leftover resources Offers policy compliance Less redundant work    Reproducible\n No UI to make mistakes in No series of commands to run correctly Use one config file and deploy Parametrize across different regions or stages     Secure\n Templates can be reviewed by other developers Create approved reusable templates Limit the way of changing infra Audits can focus on IaC changs and app logs      AWS cloud formation\n  What is?\n AWS service to manage infra as a code Uses Json or Yaml syntax Allows extend through AWS Lambda Controls the state of your infra by describing every single cloud resource in code    Basic concepts\n Templates: Information about the resources to create Stacks: List of AWS resources to create using templates AWS Resources    Tools\n AWS CDK AWS SAM £rd party frameworks    AWS cloud development kit\n Controling your infra using high-level or low-level constructs that consist of one or more cloud resources.    AWS serverless application model: LAMBDA\n Developing and deploying AWS lambda based applications    AWS code deploy\n Deploying application code to different cpmpute services with nuanced rollout strategies.    Permissions\n Need to set permission to work with CloudFormation Also permissions for all the services touched by CLoudFormation    Security for CloudFormation with CI/CD\n Build pipelines using CLoudFormation carry lots of permissions Have isolated and inaccessile build environments if possible Sexure the pipeline and how to modify it or its permissions Secure the git workflows that trigger build pipelines      Deploying CloudFormation\n Open the CloudFormation console CLick \u0026ldquo;Create Stack\u0026rdquo;, wiht \u0026ldquo;new resources\u0026rdquo; Click on \u0026ldquo;create template in Designer\u0026rdquo; \u0026lt;= you also can click on \u0026ldquo;Template is ready\u0026rdquo; Click on \u0026ldquo;template\u0026rdquo; bar, and there you can write your infra as a code in json format Once you copy your code in the template section, click \u0026ldquo;refresh\u0026rdquo; ( right top botton) Validate the template by clicking on the check mark on the top of the panel CLick upload CLick Next -\u0026gt; type a name for the stack -\u0026gt; click \u0026ldquo;next\u0026rdquo; CLick Next -\u0026gt; Click \u0026ldquo;create\u0026rdquo;  Now that the resource were created, to verify them:\n Click on \u0026ldquo;resources\u0026rdquo; Also you can see events, once all events are completed:  Go to the selected resource console, for example SNS Look for the topic, click in topics. You should be able to find your topic there.    Here an example of how looks like a template:\n    Deploying CloudFormatoin from CLI\n# To deploy a stack $ aws cloudformation deploy \\  --template-file myTemplate-stack.json \\  --stack-name demoStack # To delete a stack $ aws cloudformation delete-stack \\  --stack-name demoStack   Parameter store in Lambda functions\n# To create a parameter $ aws ssm put-parameter \\  --name MyParameter1 \\  --value Hello \\  --type SecureString \u0026lt;== this means encrypted # To get the value of a parameter with an encrypted value $ aws ssm get-parameter \\  --name MyParameter1 \\  # To get the value of a parameter with an decrypted value $ aws ssm get-parameter \\  --name MyParameter1 \\  --with-decryption # TO update the value of the paramter $ aws ssm put-parameter \\  --name MyParameter1 \\  --value HiHi \\  --type SecureString \\  --overwrite   Deploying parameters in Lambda function\n# Create a stack with AIM capabilities $ aws cloudformation deploy \\  --template-file myTemplate-stack.json \\  --stack-name demoStack --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \u0026lt;= allows you to use roles # To list Lambda functions $ aws lambda list-functions If the last command is too long, alternative way to find a specific is: - Open the Lambda console - Order the deployed lambda functions by \u0026ldquo;last modified\u0026rdquo;. Copy the name fo the lambda function\n ```bash # To retrieve the value of the lambda function $ aws lambda invoke \\ --function-name XXXXASFASXZXXXXXX \\ \u0026lt;-- name of the function copy from the lambda console result.txt \u0026lt;-- Will save output in a file ```    This is very useful if you want to hide secrets aways from your code or from being hardcoded  IV Building and testing   What is AWS CodeBuild\n CodeBuild automate building source code, execute test, and produce deployment artifacs. It is fully managed by AWS  No build server is needed, No maintenance to build servers is required AWS offers pre-configurated environments.  For different OS For different languages routines        CodeBuild concepts:\n It requires define a build project, which has the environment to build the source code: the build environment. It also requires define the buld specs. Specs show the specific process to execute the build.     V Applic  VI Creating Deployment Pipelines "
},
{
	"uri": "http://example.org/devops/aws/cloudpractitioner/coreservices/",
	"title": "CoreServices",
	"tags": [],
	"description": "",
	"content": "Methods to interact to AWS   AWS console\n Includes Mobule app Good for testing services or simple tests    AWS CLI\n Includes: Java, .net, Node.js, JS, PHP, Python, Ruby, Go, C++ automation    AWS SDK\n automation use it when you want to integreat to your app    AWS console   Two types of user: ROOT USRE and IAM user.\n Root has unlimited permissions IAM has limited permission    Multifactor authenticatoin\n  User infromation:\n My account MY org My services quota My billing dahsboard User infromation:  My account MY org My services quota My billing dahsboar Order and invoices Security credentials \u0026lt;=== Set password, and multifactor authentication, access keys      AWS region:\n you can select the one you need    Services:\n all service by category Route 53 is set as \u0026ldquo;GLOBAL\u0026rdquo; for region    AWS CLI  Get a key:  GO to my account -\u0026gt; Your security credentials -\u0026gt; Select Access keys -\u0026gt; Create new access key  This will generate a:  Access key ID Secret access key     DO NOT generate ROOT access key How to use this credentials $ aws --version # To config credentials $ aws configure --profile myteste AWS access key ID: xxxxddd AWS secret access key; xxxxxx Default region: us-east-1 Default output: json # Now you can execute aws commands $ aws s3 ls --profile myteste     Computer services  EC2 Elastick beantalk Lambda  EC2   EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developer\n  When to use:\n Web server Batch processing API server: web server Desktop in the cloud    Main concetps for EC2:\n  Instance types\n Defines the processor, memory and storage type Cannot change without downtime There are different categories:  General purposes Compute, memory and storage optimized accelerated computing, e.g. Machine Learning, access to GPU   Prices are based on instance type  The more resources you use, the more you pay      Root device type\n Two types:  Instance store: ephemeral sotrage that is physically attached to the host the VM you are running on  Data in Ephemeral instance just dessapear when you shutdown the EC2 instance   Elastic Block Store (EBS): Persistent storage that exists separately from the host the VM you are running on  Data in persistance, so data will NOT dessapear when you shutdown the EC2 instance     Usually you may use EBS.    Amazon Machine Image AMI\n Template for an EC2 instance including config, OS, and data AMI can be shared across accounts You can custom your AMI There are comercial AIM\u0026rsquo;s available in the AWS marketplace    Purchase options/types\n  On demand\n Pay by the second for the instance you lunched If you have inconsistent need for instances that can not be stoped without affecting the job    Reserved\n Purchase a discount instance in advance for 1-3 years Use reserve instance if your instance requires to be consistent and always needed     Spot\n You can leaverage unused EC2 capacity in a region for a large discount If you have batch processing when process can start/stop without affecting the job           Launching EC2 instances - Go to console.aws.amazon.com and search for EC2 - Select for \u0026quot;launch instance\u0026quot; - Select an image of your preference: AWS marketplace, community, your AMIs - Select instance type of your preference: - NOTE `12.micro is FREE tiere elegible` - Click on `Configure instance details` and you can select the paramteres you need - You can also `enable` the `auto asign public ip` - In the section `advance details-\u0026gt; user data`, you can copy the shell command to run in the new EC2 instance. - in the user data you can add shell comamnds such as: ``` yum install httpd -y service httpd start ``` - You can add a tag which is a {ID:value} - Configure the security group - For http you need to include SSH from your IP , and HTTP for everyone(0.0.0.0/0) - Review it and then launch it - YOu need to use a ssh key pair: public and private keys to ssh the EC2 instance  to terminate EC2 - Select the EC2 instance - Click Actions and then select instance state and then terminate  EBS Elastic Beanstalk - Support specific set of tech ![EBs general elastic beanstalk](/devops/aws/cloudpractitioner/ebsgeneral.png?width=50pc) - Feautures: - Monitoring - Deployment - Scaling - EC2 customization - When to use: ![EBs use case](/devops/aws/cloudpractitioner/ebsusecases.png?width=50pc)  How to laucnh EBS To launch - Visit the developer page for EBS and select the correct package you want to use - Donwload the package - Go to EBS console - Click get started - Type application name - Choose the platform - You can upload your code - click Create App  To delete ebs - In the console - Click in Actions - Select : Termiante environment - Enter the name of the app - Hit terminate  Lambda  Lambda lets you run code without provisioning or manging servers. You pay only for the compute time you consume. You can run code for virtually any type of app or backend service - all with zero admin\n In general the main feature of Lambda are:  Advantages:  Reduce maintenance requirements Enables fault tolerance Scales based on demand Pricing is based on usage    Scenarios   Case 1: One workload will leaverage at least for 5 years.\n It could use a all upfront reserved - 3 years    Case 2: Developer who does not know about infrastrcuture\n Then EBS could be one option    Case 3: If any workload can be stop and start at anytime\n The best option is SPOT, not reserved     Virtual Private Cloud VPC  VPC is a logically isolated section fothe AWS cloud where you can launch AWS resources in a virtual network that you define\n  It is a virtual network that you define and you can configure Support IPV4, IPV6 You can configure:  Ip address range subnets Route tables Network gateway   VPC support public and private subnets You can use NAT for private subnets Enables a connection to your data center You can connect VPC each other Support private connections to many AWS services  AWS direct connect  It is a cloud service solution that makes it easy to establicsk a dedicated network connection from yur data center to AWS.\n Route 53  It is Domain Name Service It is global AWs service, not regional It is highly available Enables global resource routing Changes are not instantaneous, it requires a couple of hours to be propagated across the world  Elastic load balancer  Elasticity is the ability for the infrastcuture, supporting an app, to grow and contract based on how much it is used at a point in time\n  Distribute traffic across multiple targets Integrates with EC2, ECS and Lambda Support one or more Avaialble Zones in a region Three types of load balancer:  Application load balancers ( ALB) Network load balancers ( NLB) Classic load balancers    Scaling on Amazon EC2  Vertical scaling, scale up  You enhance your existing instance to a larger instance with additional resources such as more RAM, more CPUs   Horizontal scaling, scale out  Add new instances to handle the demand of you application    CloudFront  It is a service that allows you CDN Your users get content from servers closer to them, which increase performance Support static and dynamic content It also utilizes the AWS edge location Includes security features:  AWS shield for DDoS AWS WAF, Web application firewall    Api gateway  Fully managed API management service. You provide webservices that other app can call and you can distribute them through CLoudFront Provides monitoring and metrics on API calls Support integration with VPC and on-premise private applications  Scenarions  Case 1: If you want your data center to work alongside AWS for specific workload.  Use Amazon direct connect   Case 2: If you want to optimize performance around the world and leverage CDN?  Use Amazon Cloud front     File Storage Services Amazon S3 Main features "
},
{
	"uri": "http://example.org/devops/aws/cloudpractitioner/fundamentals/",
	"title": "Fundamentals",
	"tags": [],
	"description": "",
	"content": "Main topics:   Creating an account\n  Examining traditional data center challenges:\n Large up-front investment Difficult to predict future traffic or demand Slow deploy of new data centers and servers Maintain data centers is expensive You own all security and compliance burden with all data centers    Benefits of cloud computing\n Trade capital expense for variable expenses: you pay what you use Scaling Stop to guess future needed capacity, traffic Speed and agility Stop spend money for maintenance Go global in minutes. Deploy new data centers in any part of the world Elasticity Realibility: failover, Agility:  Low cost of trying new business ideas Reduces time to maintain infrastructure Reduces risk for orgs aroud security and compliance Provides access to emerging techs      What is cloud computing:\n   It is the on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform via the internet with pat-as-you-go pricing.\n   Types of clouds\n Iaas you can full/maximun control of your infra SaaS you provide a software as a service, you do not worry about infra PaaS: Provide a plataform where you put your code and run it.    Cloud deployment models:\n Public cloud: such as AWS(pay as you go), GCP On-Premises: Private cloud in a private data center Hybrid: Use public and on-prem clouds    AWS Global infrastrucute Three types of infra:\nRegions  It is a specific gographic region Each region has a CLUSTER of data center AWS has 22 launched regions  Availability Zones  It is one or more data centers One region has Multiple avalability zones Minimun: one reqion has minimun two availability zones Each availability zone has at least ONE data center Therefore, one region has at least two data centers Each availability zone is located within the geographical area of the AWS region Availability zones have redundant power, netowrking, and connectivity. Globaly, there are 69 availability zones  What is availability:\n Extent to whcih an application is fulfiling its intended business purpose. Applications that are highly-available are built in a manner where a single failure wont lessen the application\u0026rsquo;s ability to be fully operational\n The first one is availability base on business needs. The second case is 100% availability, high availability\nNaming syntax For example: us-east-2b Where: - us = area - east = sub-area - 2 = Nubmer that identifies the region in that area - b = availabitlity zone\nFrom here: - the REGION NAME = us-east-2 - The availability zone = us-east-2b\nEdge locations  Part of a global content delivery network CDN These support only TWO services:  Amazon CLoudFront: which is AWS CDN Amazon Route 53: which is AWS DNS service There are more than 200 different location with edge locations It is the most predominant and common infra in AWS Edge locations server content from location that are closer to users    In practice   The AWs global infra : https://infrastructure.aws/\n  IN this link edge location = points of presence\n  If we need to store or service in multiple geographical areas/regions, then AWS region is needed\n  If the need is to optimize access to user content around the world, then AWS edge loction is needed.\n  If the need is to ensure availability, then AWS availability zones is needed.\n  Economics Difference between data center and cloud  Capitalized Expenditure (capEx): When building a data center, an organizatio invests in upfront costs for the building , servers, and supporting equipment. This type of expense to attain a fixed asset is referred to as a Capitalized Expenditure or CapEx\n  Operating Expenditure( OPEX): The regular day to day expenses of a business are considered Operating Expenditure or OpEx. After the initial build of a data center, ongiong connectivity, utility, and maintencance costs would be considered OpEx.\n  The problem with data centers is that at the beginning there is a \u0026ldquo;unused capacity \u0026quot; and just before increasing the capacity, there is a \u0026ldquo;demand over capacity\u0026rdquo; that forces to increase the size of the data centere. During the \u0026ldquo;deman over capacity\u0026rdquo; many users can not use our service because our infrastrcuture capacite reach its limits.  Costs in AWS   Use the \u0026ldquo;AWS cost explorer \u0026quot; tool, an interface for:\n Provides breakdown per service and cost tag Provides prediction for next three months of costs GIve recommendation for cost optimization Get access via API    \u0026ldquo;AWS budgets\u0026rdquo;: Utilizes data from AWS cost explorer to plan and track your usage across AWS services. It can track cost per serbice , serbice usage, reserved instance utilzization, and coverage, and Saving Pans utilizion and coverage.\n  AWS TCO calculator: Estimates what could be saved by using cloud.\n  AWS Simply Monthly calculator: Estaimtes costs of running specific AWS infra.\n  AWS resource tags:\n Tags are metadata attached to specific AWS resources. For example: webserver tag include all webservers. Common uses cases include department, env, or project. Tag include a name and a valur Costa allocation reports: includes costs grouped by tags You can use tags in the AWS explorer    AWS organization:\n Allows you to manage many account under one master acount It provides a consolidate billing for those account in the organization. Enables organizations to centralize logging and security standards across accounts.    AWS TCO calculator   TCO = Total cost ownership\n  Compares running your workload in the data center versus in AWS\n  WIth TCO you can:\n Estamate costs for a org to move to the cloud Download a summary report    Location: awstcocalculator.com\n  AWS Simple Monthly Calculator  Estimates the cost of running your workload in AWS, with no comparison Link: calculator.s3.amazonaws.com  Cost explorer   Login to \u0026ldquo;console.aws.amazon.com\u0026rdquo; -\u0026gt; Click in your name -\u0026gt; click \u0026quot; My billing dashboard\u0026rdquo;. Then Click on \u0026ldquo;cost explorer\u0026rdquo; and then \u0026ldquo;Launch cost explorer\u0026rdquo;\n  The cost explorer initially shows: current month cost and forecasted month end cost.\n  To get more details in the dashboard, click \u0026ldquo;Explore costs\u0026rdquo;\n  You can download CSV file\n  Scenarios   Case 1: company with many departments. Finance is asking to hav a separate costs between departments. Currently all resources are included within a single AWS account.\n IN this case we can use TAGS. Create a tag per department.    Case 2: Company think to move to the cloud, but they want to know if this will save them money:\n The solution is use TCO calculator and provide the report to the stakeholders    Case 3: Company is trying to know saving costs if they move a web service to the cloud.\n Use Simple MOnthly calculator    Suport AWS infra  Supporting tools:   AWS suuport\n Enables support for AWS resources for workloags running in the cloud Provided in different tiers based on need and scope Include tools to provide automated answers and recommendations    AWS personal health dashboard\n Provides alerts and remediation guidance when AWS is experiencing evnts that may impact you. For example,when there will be a outage in your region.\n   AWS tursted advisor\n Check your AWS usage against best practices Accessed from AWS console Different checks are provided base on the AWS support plan tier All AWS customer get access to seven core checks, which include:  Cost Optimization Performance Security Fault tolerance Service Limits        AWS Support plan tiers differences: Main four plans difference includes:\n Communication method Response time Cost Type of guidance offered  Main plans:\n  Basic support:\n To all customers Access to trusted advisor ( 7 core checks) 24x7 accesss to customer service: docs, forum, and white papers. No engineers Access to AWS personal health dashboard No monthly cost    Developer support:\n Include all from the basic support Email access support engineers for business Limited to 1 primary contact Start at $29 per month, but tied to AWS usage    Business support:\n Include all from developer support Full set of Trusted Advisor checks 24x7 phone, email and chat access support engineers Unlimited contacts PRovided thrid party software support Starts at $100 per month, tied to AWS usage    Enterprise support:\n Includes all from business support Dedicate technical account manager Include a concierge support team start from 15000$ per month, tied to AWS usage    There is a importat part called SUPPORT RESPONSE TIME\nIn here we have the following categories:\nGeneral guidance = General question to be answer System impaired = when somethng is not working as it should Production system impaired = Production system that is not performing at its desired capacity. Production system down = Production system that is completely un-functinal Business-critical system down = Core system for an organization that is completly non functional\nTrusted advisor tool  LInk: consoel.aws.amazon.com  Review recommendations Type \u0026ldquo;trsuted advisor\u0026rdquo; in \u0026ldquo;find SERvices2    Personal health dashboard Infra support scenarios  case 1: One workload is cirtical and requirest 24 hours call support a day.  The most effective criteria is Business support   case 2: The company needs to be able to call , text and email support if an issue occurs. Also needs response from support within 15 minutes.  Here applies: Enterprise support   Case 3: a developer does not need technical support, and he has access to trusted advisor core checks.  Here Basic support is enough    Review "
},
{
	"uri": "http://example.org/notes/restful/",
	"title": "Restful",
	"tags": [],
	"description": "",
	"content": "Design tips  URI format:  URI = scheme \u0026quot;://\u0026quot; authority \u0026quot;/\u0026quot; path [ \u0026quot;?\u0026quot; query] [\u0026quot;#\u0026quot; fragment\u0026quot;] URI = http://myserver.com/mypath?query=1#document   General rules for URI formating:\n A forward slash / is used to indicate a hierarchical relationship between resources A trailing forward slash / should not be included in URIs Hyphens - should be used to improve readability Underscores _ should not be used in URIs Lowercase lette    URI path design\n  Collections: Always plural noun\nGET /cats -\u0026gt; All cats in the collection GET /cats/1 -\u0026gt; Single document for a cat 1   Documents: Points to a single object.\nGET /cats/1 -\u0026gt; Single document for cat 1 GET /cats/1/kittens -\u0026gt; All kittens belonging to cat 1 GET /cats/1/kittens/1 -\u0026gt; Kitten 1 for cat 1   Controller: Used to map non standard CRUD\nPOST /cats/1/feed -\u0026gt; Feed cat 1 POST /cats/1/feed?food=fish -\u0026gt;Feed cat 1 a fish   Store: A source repository.\nPUT /cats/2     References  RFC 7231: Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content RFC 2616: Header Field Definitions Microsoft REST API guidelines OpenAPI specifications adopted by industry for documentation Swagger using YAML API Blueprint using Markdown RAML Restful API Modeling Language using YAML Calico project for container base networking  "
},
{
	"uri": "http://example.org/sre/mypostmortem/",
	"title": "Mypostmortem",
	"tags": [],
	"description": "",
	"content": "Random notes  Expose your code for one day to everyone. Ask people to execute disaster revocery of everything once or twice a year. blameless I do not think is a way to avoid future mistakes, I think it is a way to save time and effort, thefore reduce costs. We should test our selves at least once a year. to see how if we are ready or notfor disasters Postmorten of the month? Best postmorten of this motnh Postmorten award 2020? why not have one Postmorten day? Postmorten should be rewarded  "
},
{
	"uri": "http://example.org/notes/jfrog/",
	"title": "Jfrog",
	"tags": [],
	"description": "",
	"content": "1. How to install a local Jfrog (open source) The following shows how to install a local Jfrog (OSS) in a Linux-base host.\n Open https://jfrog.com/open-source/#artifactory Select the OS you are working with: Linux, Windows or Mac and click on \u0026ldquo;Click here to Download\u0026rdquo;  In our case we could directly download the tar file for Linux:\n$ mkdir myJfrog $ cd myJfrog $ wget https://api.bintray.com/content/jfrog/artifactory/org/artifactory/oss/jfrog-artifactory-oss/$latest/jfrog-artifactory-oss-$latest-linux.tar.gz?bt_package=jfrog-artifactory-oss $ ls -tlr total 811768 -rw-rw-r-- 1 percy percy 831242240 Aug 26 17:28 jfrog-artifactory-oss-7.7.3-linux.tar $ tar -zxvf jfrog-artifactory-oss-7.7.3-linux.tar $ cd artifactory-oss-7.7.3/app/bin $ ls -trl total 556 -rwxr-xr-x 1 percy percy 29279 Jul 13 13:17 systemYamlHelper.sh -rwxr-xr-x 1 percy percy 96527 Jul 13 13:17 systemDiagnostics.sh -rwxr-xr-x 1 percy percy 171933 Jul 13 13:17 migrate.sh -rwxr-xr-x 1 percy percy 143122 Jul 13 13:17 installerCommon.sh -rwxr-xr-x 1 percy percy 6879 Aug 12 10:55 uninstallService.sh -rwxr-xr-x 1 percy percy 4387 Aug 12 10:55 migrationZipInfo.yaml -rwxr-xr-x 1 percy percy 6539 Aug 12 10:55 migrationRpmInfo.yaml -rwxr-xr-x 1 percy percy 4073 Aug 12 10:55 migrationDockerInfo.yaml -rwxr-xr-x 1 percy percy 4283 Aug 12 10:55 migrationComposeInfo.yaml -rwxr-xr-x 1 percy percy 10606 Aug 12 10:55 installService.sh -rwxr-xr-x 1 percy percy 17401 Aug 12 10:55 artifactory.sh -rwxr-xr-x 1 percy percy 9365 Aug 12 10:55 artifactoryManage.sh -rwxr-xr-x 1 percy percy 1553 Aug 12 10:55 artifactory.default -rwxr-xr-x 1 percy percy 382 Aug 12 10:55 artifactoryctl -rwxr-xr-x 1 percy percy 32294 Aug 12 10:55 artifactoryCommon.sh $ ./artifactory.sh  Open in a browser: http://localhost:8082 To login you can use the default admin account: admin/password.  "
},
{
	"uri": "http://example.org/sre/review/",
	"title": "Review",
	"tags": [],
	"description": "",
	"content": "This section contains reviews or summaries from exisitng literature.\n1. SQL is no excuse to avoid devops @article{limoncelli2018sql, title={SQL is no excuse to avoid DevOps}, author={Limoncelli, Thomas A}, journal={Communications of the ACM}, volume={62}, number={1}, pages={46--49}, year={2018}, publisher={ACM New York, NY, USA} } Automation and a little discipline allow better testing, shorter release cycles, and reduced business risk.\nPrevious articles:\n “The Small Batches Principle,”Communications, July 2016), when something is risky there is a natural inclination to seek to do it less. McHenry Technique in The Practice of Cloud System Administration, Strata R. Chalup and Christina J. Hogan.  It is also called Expand/Contract in Release It!: Design and Deploy Production-Ready Software by Michael T. Nygard.   SQL migration tools:  https://github.com/bretcope/Mayflower.NET https://bitbucket.org/liamstask/goose/src/master/    Reliability: What is it, and how is it measured? @article{bruton2000reliability, title={Reliability: what is it, and how is it measured?}, author={Bruton, Anne and Conway, Joy H and Holgate, Stephen T}, journal={Physiotherapy}, volume={86}, number={2}, pages={94--99}, year={2000}, publisher={Elsevier} } how to quantify relibilityhow to measue how to seect mesure metrics how ti collect metrics how to validate the methodology of collecting metrics how to classify them how to aggregate them how to quantify errors errors in cpollectin errors in analysis errors agrregation errors per cases how to store them how to analize them how to filter them how to archive them how to validate them how to reuse them hoe to reproduce them how to transfort them how long this measuremens and realibility is valid how define good realibility? what is good? how important is reproucibility\nBaumgarter (1989) has identified two types of reliability, ie relative reliability and absolute reliability.\n3. Nines are not enough: meaningful metrics for clouds @inproceedings{mogul2019nines, title={Nines are not enough: meaningful metrics for clouds}, author={Mogul, Jeffrey C and Wilkes, John}, booktitle={Proceedings of the Workshop on Hot Topics in Operating Systems}, pages={136--141}, year={2019} }   This paper analyse: Why SLO are so hard to define?\n  However, SLOs are not free. We have to spend resources collecting and processing the data that allows us to com- pute an SLO’s predicate, without significantly interfering with “real\u0026rdquo; work – and without compromising security or privacy of cloud customers and their own users. We often must aggregate data to reduce costs, which loses fidelity. (These costs are what we mean by the “feasibility\u0026rdquo; of a set of SLOs.\n  how to collect sufficient data without bias,\n  When to triggerd the decision to claim an SLA?\n  How much data you need to collect?\n  How expensive it is?\n  we have uncertain knowledge of future workloads, and we also are unable to accurately model the performance of complex computer systems even if we did know the workloads.\n  By monitoring and collecting data, we can find the time when of SLA violation, which can lead to a financial penalty in benefit of the customer, if the SLA says it. SLA\u0026rsquo;s are very important from business and law point of view.\n  This paper present many interesting open questions.\n  4. Thinking about availabitliy in large service infrastrcuture. @inproceedings{mogul2017thinking, title={Thinking about availability in large service infrastructures}, author={Mogul, Jeffrey C and Isaacs, Rebecca and Welch, Brent}, booktitle={Proceedings of the 16th Workshop on Hot Topics in Operating Systems}, pages={12--17}, year={2017} } 5. Auditing to keep online storage services honest. @inproceedings{shah2007auditing, title={Auditing to Keep Online Storage Services Honest.}, author={Shah, Mehul A and Baker, Mary and Mogul, Jeffrey C and Swaminathan, Ram and others}, booktitle={HotOS}, year={2007} } 6. Sre tools as product This document, not paper, gives a general lines about SRE. One important message: reduce MTTR and toil.\n7. Debugging Incidents in Google\u0026rsquo;s Distributed Systems. @article{chan2020debugging, title={Debugging Incidents in Google's Distributed Systems}, author={Chan, Charisma and Cooper, Beth}, journal={Queue}, volume={18}, number={2}, pages={47--66}, year={2020}, publisher={ACM New York, NY, USA} } It introduces a general overview of SRE and present postmorten use cases.\n8. Generic mitigation Jennifer Mace: Spotlight on Cloud: Reducing the Impact of Service Outages with Generic Mitigations with Jennifer Mace https://www.oreilly.com/library/view/spotlight-on-cloud/0636920347927/\n"
},
{
	"uri": "http://example.org/sre/myapproach/",
	"title": "Myapproach",
	"tags": [],
	"description": "",
	"content": "What is SRE  SRE measures how reliable my serivice is from my customer\u0026rsquo;s perspective, not from my perspective. It seems most of the talks use current or historic data, logs, Why we do not estimate future data then ?  The reason is that usually you take generated logs, so it is hard to have a real-time observations. How to have real-time observations? how to estimate the future?    SLO  Defines a threshold, max or min value, for an SLI. Defines what are your service expectations. Examples:  Uptime 999% i.e. SLI-1 over the last month 99.95 of the time. 99% of valid requests in the past 28 days served in less than one second.    SLI   Includes the most relevant metrics to measure service of your system.\n  If you see historical data, you can not take overall view, instead every slot should be analysed. For example, if we have a pick in a specific day, maybe the traffic was high at that time and that is where we need to guarantee reliability. This should be consider even if it occurs only once.\n  Examples:\n One a minute, ssh into the target host, report 1 if it\u0026rsquo;s working 0 if not. The proportion of valir requests that were serverd within less than one second.     How percentiles should be treated? can we aggregate along the time? When can be aggreagted when not?\n  Can we averga percentiles of many modules and for a long time of data analysis?\n  If we do not use logs, what will be the benefit to counts events as soon as they happen, and evaluate SLI base on this fresh results?\n  Metrics have two faces:\n Enough to evaluate the customer perception. We can not use so many metrics and overflow it. Metrics should be enough to do a post mortem analysis and document. Postmortem analysis includes analysis of historic data logs, and also includes to collect metrics or logs in detail that can allow us to find the root of cause without the need to implement new metrics and wait for the second time the error to happen.    SLI templates: 1. The proportion of successfully retrived SKU IDs associated to products, for /api/getSKUs that have a valid ID retrieved from server-side and measure at the client-side. The proportion of .... for .... that have ... measured at ...\nThe following is an exampe for purchase items from a mobile device: https://developer.android.com/google/play/billing/integrate  1. The proportion of valid retrived SKU IDs associated to products, for /api/getSKUs that have a valid ID retrieved from server-side and measure at the client-side. - This shows the rate of SKU IDs presented to the customer. If we have 10 products in our store, this SLI specifies how many of them are presented to the customer. - The validity of a SKU ID should be evaluated at the client side as it defines that the received ID are the ones that matched the server-side. - The implementation should be instrumented at the client side, by counting the number of SKU IDs and validate them - This SLI is estimated every time the customer wants to see the available products in store. - Every time that the customer refresh, or re-visit this stage this rate should be updated. - This SLI is active while the customer is viewing the list of products and needs to be stopped when the customer moves to the next stage, which is select a specific product asking for more details. - This SLI needs to be implemented in the client-side - The gap for this SLI includes lost of connectivity due to client-side lost internet connection, and customer side device used for this application. 2. The proportion of HTTP get requests for /api/completePurchase that have 0 and 1 as a response code value measured at the client side. - This SLI shows the rate of successfully transactions, associating succesfull transaction to response code 0 and 1 : success and user presed back or cancel a dialog. - This SLI should be instrumented at the client side and count the number of totals reponse codes versus the total number of 0 and 1 responses. - This SLI is estimated every time the customer press the \u0026#34;buy\u0026#34; botton on the screen. - This SLI happens for the time the \u0026#34;buy\u0026#34; transaction is required. - The gap for this SLI includes lost of connectivity as in that case the application should mention that connectivity was lost. SLA  Examples:  If we do not meet our SLO in one month, you will get exactly one cake In the case of JRI: I will pay 10£ to the member of the team that points a failed SLO    Alerts   Set an alert ONLY when a manual internvention is required, not for infomative purposes. Alerts mean action and now. If this is not possible, you can separate alerts that requries action and alerts where no action is required.\n  Monitoring should be display in one place.\n   Customer perception  Collect data from customers, slack, twitter, emails, open public conversations and centralise them. Apply ML to analyse them and get insights for understand whether customer are happy or not with services.  How to build SLO How to maintain SLO  Refer to https://www.usenix.org/conference/srecon19emea/presentation/desai talk  NOTES\n  Get differencd between service satisfaction and content satisfaction.\n  how error budget leads with customer trust: i believe customers should be able to set our error budget, not us\n  Also postmortem should be quick and not generate historic toil.\n  "
},
{
	"uri": "http://example.org/sre/online/srecon2019/",
	"title": "SRECON 2019",
	"tags": [],
	"description": "",
	"content": "1. The SRE I Aspire to be by Yaniv Aknin at SRECON 2019  Yaniv A. is GCP quantitative reliability lead. SRE skill is the ability to measure or translate to numbers ( quantify ) SRE includes: \u0026ldquo;measurably optimse reliability vs cost\u0026rdquo; It is important to trade Operations and Reliability How can we build a more reliable logical disk? then we do not need to increase operations due to failure. One solution as use RAIDs, but cost more and more complexity! SRE is trading cost and reliability Remove alerts, reduce operations Fundamental: Monitoring, Alerting, Capacity planning, CI/CD \u0026amp; rollouts, and Load Balancing. Advanced: System Architecture, Distributed Algorithms, Networking, Operating systems. Pioneering: Product management, data science, business Acumen, UX research. Measurements are tied to the project priorities and operations are tied to these measurements.  2. SLOs for Data intensive Services by Yoann Fouquet Booking.com Youtube  Example for SLI/SLO: SLI = Quantitative measure: availability,\nSLO = SLI + target|threshold: availability (SLI) for 1 week over 99.99% SLO Booking.com: Availability, latency, and reservation success rate.\n` Missing SLOs: durability, freshness, accuracy, completeness, consistency. - \u0026ldquo;\u0026hellip;99.99% of search results are consistent \u0026hellip;\u0026rdquo; - \u0026ldquo;\u0026hellip;99.9% of reservations are available within in 10 seconds\u0026hellip;\u0026rdquo; - Booking.com group queries by latency. Benefits: mitagation for free, My opinion these SLOs/SLIs does not reflect the customer perception  3. Latency SLos Done right by Heinrich Hartman, Circonus   Presentation in here\n  What is latency?\n Key perfomance indicator for API. Latency is number one golden signal. Besides latency, another golden signals, indicators, include traffic, errors and saturation. This later became the RED method: Request, Errors, Durantion.    What is an SLI?\n A metric that quantifies the quality or reliability of your service.    What is an SLA?\n What happen if a published SLO is not met?    What is an SLO?\n A target value for the service level, as measured by SLI, set of expectations about how the service will perform. Percentile metrics need to be compute percentiles over:  Multiple weeks of data, better long time periods Multiple nodes ( potentially )   Percentiles can not be aggregated.    Methods to stimate SLOs:\n  Considering the task: \u0026ldquo;count all request over $period served faster than $threshold\u0026rdquo;\n  Methods:\n  log data:\n Execute a query in your stored logs You need to keep your logs for long times, which could be expensive ( you could use sampling ) You can do this with ssh+awk, ELK, Splunk, Honeycomb,etc    counter metrics\n It stores and uses counters of how often an event occurs: for example, \u0026quot; how many requests were faster than 2 ms\u0026rdquo;, so you can store this as short prometheus metric: \u0026ldquo;aws.www33get./latency_2ms\u0026rdquo;. Later you can sum/integrate these metrics across nodes, endpoint and time. This approach does not required store logs, instead store counters at specific time. Tools: prometheus histogram, graphite, datadog, vividcontext.    HDR historgram metrics\n  See below figure\n  WIth histograms you can aggregate latency histrograms over nodes, endpoints and time. Get total latency distribution over SLO timeframe ( weeks, months)\n  Count samples in bins below the thresholds to compute SLOs. You can sum or agregate two similar bins to get a final overll number.\n  Full flexibility in choosing thresholds, and aggregation intervals and levels\n  Cost effective ( 300b/histogram value).\n  Need HDR histogram instruments and metrics store\n  Tools: CIrconus,IronDB + graphite / Grafana\n        Conclusions:\n Percentiles metrics are not suitable for implementing latency SLOs Histogram metrics allow you to easily calculcate arbitraty latency SLOs If you do not have histrogram metrics:  Use historic recent logs Ad counter metrics for the threshold Aggregate couter metrics as needed        4. Evolution of Observability Tools at Pinterest by Naoman Abbas 5. How to SRE When Everything\u0026rsquo;s Already on Fire by Alex Hidalgo and Alex Lee, Squarespace 6. Advanced Napkin Math: Estimating System Performance from First Principles by Simon Eskildsen, Shopify 7. The Map Is Not the Territory: How SLOs Lead Us Astray, and What We Can Do about It by Narayan Desai, Google "
},
{
	"uri": "http://example.org/notes/crypto/",
	"title": "Crypto",
	"tags": [],
	"description": "",
	"content": "General  DES; The Data Encryption Standard uses a key of 56bits and a block size of 64 buts. It is not recommended, and insted you can consider other approaches such as AES, Salsa20.:w   "
},
{
	"uri": "http://example.org/devops/gcp/networking/",
	"title": "Networking",
	"tags": [],
	"description": "",
	"content": "VPC Virtual Private Cloud VPC is a set of GCP objects, such as:\n  Project\n Associated to services and networks Organize infrastructure resources Link objects and services with billing Contain networks, default up to 5 It containt entire networks    Network: can be shared with other projects or peered, are global, and no ip address range\n Default  Every project one subnet per region Default firewall rules   Auto  Default network One subnet per region Regional IP allocation Fixed /20 subnet per region Expandable to /16   Custom  No default subnets created Full control of IP ranges Regional IP allocation Expandable to any RFC 1918 size    To connect to a on-premises network you can use a VPN gateway service from VPC. This is possible because VPC is global.   Subnets\n They work at REGIONAL scale. VMs can be on same subnet but different zones One single firewall rule can apply to both VMs Every subnet has 4 reserved addresses:  x.0.0.0 Network x.0.0.1 Subnet gateway x.0.0.255 Broadcast x.0.0.254 Primary used by Google ( second-to-last address )   New subnets can not overlap with extisting subnets New subnets address should be inside the RFC 1918 address spaces Subnets can expand, can not shrink. Start with small subnet ranges  To expand a subnet: GCP console -\u0026gt; VPC Network -\u0026gt; Click subnet -\u0026gt; Click Edit -\u0026gt; Modify the range in \u0026ldquo;IP address range\u0026rdquo;   Automode can be expanded from /20 to /16 Avoid large subnets    Regions\n  Zones\n  Ip addresses\n  Internal\n Allocated from subnet range to VMs by DHCP DHCP lease is renewed every 24 hours VMs name + IP is registered with network scoeped DNS VMs only see the internal IP, not the external. The external IP is maped to the VM by VCP, but the OS of the VM does not know it.    External\n Assigned from pool ( external ips are ephemeral by default ) Reserved (static) and billed more then not arrached to running VM VM does not know external IP, it is mapped to internal Admins can also publish public DNS records pointing to the instance  Public DNS records are not published automatically   DNS zone can be hosted using Cloud DNS    IP alias:\n Primary subnet: defines the subnet Each VM gets its primary IP from this range Swcondary ranges provide an organizational tool The following files replace the _spf.google.com TXT records previously recommended to use for listing Google IP addresses.   https://www.gstatic.com/ipranges/cloud.json: Provides a JSON representation of Cloud IP addresses organized by region.\n  https://www.gstatic.com/ipranges/cloud_geofeed It is a standard geofeed formatted IP geolocation file that we share with 3rd-party IP geo providers like Maxmind, Neustar, and IP2Location.\n  https://www.gstatic.com/ipranges/goog.json and https://www.gstatic.com/ipranges/goog.txt are TXT and JSON formatted files respectively that include Google public prefixes in CIDR notation.\n  For more information as well as an example of how to use this information, refer to https://cloud.google.com/vpc/docs/configure-private-google-access#ip-addr-defaults\n        VMs\n Name resolution is handle by an internal DNS Each VM has a hostname that can be resolve to an internal AP address.  FQDN= hostname.c.ProjectID.internal      Routes\n A route is create when a network or subnet is created. COnnections are allowed or denied at the instance level Firewall rules are stateful, it means birectional communication If firewall and rules are deleted, the deny all ingress and allow all egress are still aplied.    Firewall rules\n Configure firewall by setting the following:  Direction:  Inbound connections are matched with \u0026ldquo;ingress\u0026rdquo; rules only Outbound connection sre matched with \u0026ldquo;egress\u0026rdquo; rules only   Source or destination:  For ingress: source = ip address, source tags or source service account For egress: destination = rule with one or more ranges of ip addresses.   Protocol, port Action: allow or deny Priority: Order on how rules are applied, first matching rule is applied. Rule assignment: All rules are assigned to all instances, but you can assign ceratin rules to especific instances only   Hierarchical firewall policies:  GCP uses \u0026ldquo;go to_next\u0026rdquo; to allow low level instances to evaluate the ingress/egress      Example:\n  Go to GCP console -\u0026gt; click on VPC Networks\n  In the left pane, click Firewall rules. Notice that there are 4 Ingress firewall rules for the default network:\n  default-allow-icmp =\u0026gt; This allows you to ping from a VM to the external IP only.\n  default-allow-rdp\n  default-allow-ssh ==\u0026gt; This allows you to ssh this VM. If you remove this and you wanted to ssh, you will see the following message:\n Please consider adding a firewall rule to allow ingress from the Cloud IAP for TCP forwarding netblock to the SSH port of your machine to start using Cloud IAP for TCP forwarding for better performance. Learn more Dismiss\n   default-allow-internal ==\u0026gt; This allows you to ping between VMs in the same subnet. The default-allow-internal firewall rule allows traffic on all protocols/ports within the default network.\n  These firewall rules allow ICMP, RDP, and SSH ingress traffic from anywhere (0.0.0.0/0) and all TCP, UDP, and ICMP traffic within the network (10.128.0.0/9). The Targets, Filters, Protocols/ports, and Action columns explain these rules\n    Note that if a VM has not network set, GCP does not allow to create it. You need to create a VPC network first, and that name should appear during the creation of the VM under the \u0026quot; Management, security, disks, networking, sole tenancy \u0026quot; tag.\n  To create a VPC network, you can choose auto mode with firewall rules:\n Click on VPC network, create VPC, Name it For subnet creation mode, click \u0026ldquo;automatic\u0026rdquo;: Auto mode networks create subnets in each region automatically. For Firewall rules, select all available rules. These are the same standard firewall rules that the default network had. The deny-all-ingress and allow-all-egress rules are also displayed, but you cannot check or uncheck them because they are implied. These two rules have a lower Priority (higher integers indicate lower priorities) so that the allow ICMP, internal, RDP and SSH rules are considered first. Click create. When VPC network is created note that a subnet was creatd for each region. NOTE If you ever delete the default network, you can quickly re-create it by creating an auto mode network as you just did.      Mutlple network interfaces:   VPC network is isolated by default:\n Communication inside VPC, though internal IP Communication across networks, using external IP    In multiple interfaces each NIC is attached to a different VPC network\n Usually 8 NICs are allowed peer VM  VM \u0026lt;= 2vCPU, you can have 2 NICs VM \u0026gt; 2 vCPI, you can have 1 NIC per vCPU ( max 8)       Excercise:\n  Command lines for network and subnets\nTo create a VPC netwok with custom subnets $ gcloud compute networks create myvpcnet --project=myproject --subnet-mode=custom --mtu=1460 --bgp-routing-mode=regional To create subnet $ gcloud compute networks subnets create mysubnet --project=myproject --range=10.130.0.0/20 --network=myvpcnet --region=us-central1 To create mynet network. another approach $ gcloud compute networks create mynet --subnet-mode=custom To create a subnet called subnet1 $ gcloud compute networks subnets create subnet1 --network=mynet --region=us-central1 --range=172.16.0.0/24 To list available VPC networks $ gcloud compute networks list NAME SUBNET_MODE BGP_ROUTING_MODE IPV4_RANGE GATEWAY_IPV4 default AUTO REGIONAL mynet1 CUSTOM REGIONAL mynet2 AUTO REGIONAL privatenet CUSTOM REGIONAL To list subnets $ gcloud compute networks subnets list  NOTE: Instances on this network will not be reachable until firewall rules are created. As an example, you can allow all internal traffic between instances as well as SSH, RDP, and ICMP by running. $ gcloud compute firewall-rules create \u0026lt;FIREWALL_NAME\u0026gt; --network privatenet --allow tcp,udp,icmp --source-ranges \u0026lt;IP_RANGE\u0026gt; $ gcloud compute firewall-rules create \u0026lt;FIREWALL_NAME\u0026gt; --network privatenet --allow tcp:22,tcp:3389,icmp  default and mynetwork are auto mode networks and create subnets in each region automatically. mynet1 and mynet2 are custom mode networks and start with no subnets, which gives you full control over subnet creation. To list available subnets. Note that auto mode networks create default subnets,and custom mode create only manually created subnets  $ gcloud compute networks subnets list --sort-by=NETWORK   Command line for firewall rules:\n$ gcloud compute firewall-rules create mynet-allow-icmp-ssh-rdp --direction=INGRESS --priority=1000 --network=mynet --action=ALLOW --rules=icmp,tcp:22,tcp:3389 --source-ranges=0.0.0.0/0 To list all available firewall rules $ gcloud compute firewall-rules list --sort-by=NETWORK   Command line to create VM:\n$ gcloud compute instances create myvm --zone=us-central1-c --machine-type=n1-standard-1 --subnet=privatesubnet-us To list all VMs $ gcloud compute instances list --sort-by=ZONE gcloud beta compute --project=be7ad6aa15 instances create managementnet-us-vm --zone=us-central1-c --machine-type=n1-standard-1 --subnet=managementsubnet-us --network-tier=PREMIUM --maintenance-policy=MIGRATE --service-account=310583-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --image=debian-10-buster-v20210122 --image-project=debian-cloud --boot-disk-size=10GB --boot-disk-type=pd-balanced --boot-disk-device-name=managementnet-us-vm --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any  NOte that two VMs in the same VPC can ping each other, but not necessarily other VPCs. To confirm this need to check the firewall rules. VPC networks are by default isolated private networking domains. However, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC peering or VPN. The number of interfaces allowed in an instance is dependent on the instance\u0026rsquo;s machine type and the number of vCPUs. The n1-standard-4 allows up to 4 network interfaces. Refer here for more information.    Cloud identity and Access management   IAM is a service to identify \u0026ldquo;who\u0026rdquo;,\u0026ldquo;can do what\u0026rdquo;, and \u0026ldquo;in which resource\u0026rdquo;.\n WHO: The members  Google accounts: Any individual that interacts with GCP, e.g. developer Service accounts: Account that belongs to an application Google groups: Collection of google accounts and service accounts. It has unique email addresses linked to each group. Ideal to apply policy to a group of users. G suite domains: Represents your organizational domain, e.g. example.com Cloud identity: Collection of accounts in your organization, but they do not have access to similar services as G suite domain.   CAN WHO WHAT: Roles  Basic roles:  Owner Editor Viewer Billing Administrator        A policy contains a set of roles and memebers\n  Parent policies overrides a children policy, for example if the parent policy is less restrictive, it will override a more restrictive policy.\n  Firewall   VPC network functions as a distributed firewall\n  Firewalls are applied to the network as a whole\n  Deny all ingress and allow all egress rules are implied\n  Firewall rules parameters:\n Target: which could be one of the following: ( where to apply a rule )  All instances in the network Specified target tags Specified services accounts   Source: ( the type of resource to apply the rule)  IP ranges Subnets Source tags Source accounts      Default\n  Exercise   In this exercise we create a A and B nginx webserver in a default VPC network.\n  Network tags are used by networks to identify which VM instances are subject to certain firewall rules and network routes. In a VM, you create a firewall rule to allow HTTP access for VM instances with the web-server tag. Alternatively, and during the creation of a VM using the GUI, you could select the Allow HTTP traffic checkbox, which would tag this instance as http-server and create the tagged firewall rule for tcp:80 for you.\n  To create a firewall, remember that a firewall is linked to a network tag. Network tags are used in each VM, so if a rule applies to a tag, then this firewall rule will apply to all VMs that are pointing to this network tag.\n In the GUI, go to VPC network -\u0026gt; firewall Select the create a firewall rule:  GIve a name: Network = default Targets = Specified target tags Source filter = IP ranges Source IP ranges = 0.0.0.0/0 Protocol and ports = Specified protocols and ports  tcp =80        Cloud IAM lets you authorize who can take action on specific resources, which give you full control and visibility to manage cloud resources centrally. The following roles are used in conjunction with single-project networking to independently control administrative access to each VPC network:\n  Network Admin: Permissions to create, modify, and delete networking resources, except for firewall rules and SSL certificates. Like read only.\n  Security Admin: Permissions to create, modify, and delete firewall rules and SSL certificates.\n  Explore these roles by applying them to a service account, which is a special Google account that belongs to your VM instance instead of to an individual end user. You will authorize a vm to use the service account to demonstrate the permissions of the Network Admin and Security Admin roles, instead of creating a new user.\n  Some suggested steps:\n SSH to the Vm you want to apply permissions, and you will find that the Compute Engine default service account does not have the right permissions to allow you to list or delete firewall rules. The same applies to other users who do not have the right roles. This is why it is need to create a \u0026ldquo;service account\u0026rdquo; $ gcloud compute firewall-rules list $ gcloud compute firewall-rules delete allow-http-web-server  To create a service account:  IAM \u0026amp; admin console -\u0026gt; service accounts Set the servie account name and click create For Select a role, select Compute Engine \u0026gt; Compute Network Admin. CLick continue and then done Once this is complete, stop the VM, and edit the VM config and select \u0026ldquo;Network admin\u0026rdquo; ( or the name of your service account you just created ) in the field \u0026ldquo;service account\u0026rdquo; Once you start your VM, you can ssh and try to run the last two gcloud commands and see if you do not have any error for \u0026ldquo;list\u0026rdquo; firewall rules. Note that \u0026ldquo;NEtwork adrmin oonly allows you read access\u0026rdquo; To change settings from \u0026ldquo;Network admin\u0026rdquo; to \u0026ldquo;security admin\u0026rdquo;:  AIM \u0026amp; admin console CLick on AIM -\u0026gt; Search for the service account and click in the pencil for editing Select Compute Enginte and Compute Security Admin insated of \u0026ldquo;Compute Network admin\u0026rdquo;          Shared VPC  Organization admin:  Organization is the root node Workplace or cloude indentiy super administrators assign Oranization Admins Nominates Shared VPC Admin ( compute.xpnAdmin)   Shared VPC admin  Enables shared VPC host project Attaches service projects Delegates access to some or all subnets in shared VPC networkd ( compute.networkUser) Shared VPC admin is the project owner for a given host project   Service Project Admin   Control over service project resources:\n Compute instance admin Project owner    Create resources in sahred VPC:\n VM instances Instance templates and groups Static internal IP Load balancers  Youtube Video on provisioning shared vpc\nGoogle docs for provisining shared VPC\n  How to create a Shread VPC\n An Organization Admin nominates a Shared VPC Admin. A Shared VPC Admin enables shared VPC for the host project. A Shared VPC Admin delegates access to some or all subnets of a shared VPC network by granting the Network User role. A Network User creates resources in his/her Service Project.      VPC peering  To establish a peer connection, the consumer network admin requires to establish connection to the producer network. Also and independently, the producer network admin requires to establish connection to the consumer network. Internal VM communicates using their private internal IP addresses. VPC peering worked wit compute engines, Kubernetes engine and APP engine environments Each VPC networks remain administratively independent and separate. Two auto mode VPC networks that only have the defatult subnet can not peer.   From labs:   VPC network peering gives you several advantages over using external IP addresses or VPNs to connect networks, including:\n Network Latency: Public IP networking results in higher latency than private networking. Network Security: Service owners do not need to have their services exposed to the public internet and deal with its associated risks. Network Cost: Google Cloud charges egress bandwidth pricing for networks using external IPs to communicate, even if the traffic is within the same zone. If, however, the networks are peered, they can use internal IPs to communicate and save on those egress costs. Regular network pricing still applies to all traffic.    In a peered VPC network, no subnet IP range can overlap with another subnet IP range. Therefore, verify that the CIDR blocks of the subnets of mynetwork and privatenet are non-overlapping. You can configure VPC network peering between mynetwork and privatenet because their subnets\u0026rsquo; CIDR blocks are non-overlapping. In google console click on VPC network\n  Usually verify that there is not a related peering network or route by clicking on VPC networkind peering, also verify that no route has a \u0026ldquo;peering connection\u0026rdquo; as the \u0026ldquo;next hop\u0026rdquo; tag.\n  To create peering network:\n You need project ID and the name of the VPC network to pair with  Deleting one side of the peering connection terminates the overall peering connection.      Managed instance groups  It is a group of VMs that were created base on a template, this means all these VMs are similar.  GCP console can help you to create a template: Compute Engine-\u0026gt; Instance template To create a instance group:  Choose a single zone or multizone template Define which ports will be allow Select the template you would like to use Define whether you want to autoscale Define whether you need a health check     Instance group can be resized in one single zone or regional, the last one is recommended. Manager can autoscale base on a template and can be integrated with a load balancer.  Load balancer over HTTP   Overview  From above diagram,the backend service contains:  Health checker Session afinity, it uses Round Robbin algorithm Timeout setting ( 30 sec default ) One or more backends:  An instance group ( managed and unmanaged )  Contains VMS   A balancing mode ( criteria to scale service considering CPU utilization or RPS )  Defines how to LB should behave when it is in a full usage.   A capacity scaler ( ceilling % of CPU / Rate targets )        HTTPS\n Requires a s HTTPS load balancer Up to 10 SSL certificatates ( per target proxy ) Creates an SSL certificate resource, which is used only with HTTPS LB    ARMOR\n It provides built-in defense against DDOS attacks. Uses security policies:  Deny or/and allow access to your LB: Block source IP or CIDR range Deny rule: 403, 404, 502 error code Priority: ruler order Customer rules L3 to L7      "
},
{
	"uri": "http://example.org/notes/shell/",
	"title": "Shell",
	"tags": [],
	"description": "",
	"content": "1. Sending message to pts terminal # List of opened terminals. percy@prec:~$ ls -tlr /dev/pts/ total 0 c--------- 1 root root 5, 2 May 21 06:18 ptmx crw--w---- 1 percy tty 136, 2 May 21 11:36 2 crw--w---- 1 percy tty 136, 0 May 21 11:52 0 crw--w---- 1 percy tty 136, 3 May 21 11:53 3 crw--w---- 1 percy tty 136, 4 May 21 11:53 4 crw--w---- 1 percy tty 136, 6 May 21 11:53 6 crw--w---- 1 percy tty 136, 5 May 21 11:53 5 percy@prec:~$ echo \u0026#34;How are you ?\u0026#34; \u0026gt; /dev/pts/6 You can use \u0026ldquo;ps\u0026rdquo; to identify each pts terminal.\n"
},
{
	"uri": "http://example.org/notes/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Reference The following notes were extracted, adjusted or extended from the following references.\n Kubernetes book by Nigel Poulton Kubernetes setup  Kubernetes the hard way Kubernetes network administraion  Google Kubernetes Engine: GKE docs AWS Elastic Container Service for Kubernetes EKS Kubectl docs  General   Kubernetes is a container orchestrator, which means to maintain, manage and support container that run applications. Kubernetes also manage workload placement, it offers an infrastructure abstraction and maintains a desire state of the infrastructure.\n  The cluster is made up of one or more masters, and a bunch of nodes.\n  Package and deploy a Kubernetes application is done via a Deployment. With Deployments, we start out with our application code and we containerize it. Then we define it as a Deployment via a YAML or JSON manifest file. This manifest file tells Kubernetes two important features:\n What our app should look like – what images to use, ports to expose, networks to join, how to perform update etc. How many replicas of each part of the app to run (scale)  Then we give the file to the Kubernetes master which takes care of deploying it n the cluste\n  The API server\n  The API Server (apiserver) is the frontend into the Kubernetes control plane. It exposes a RESTful API that preferentially consumes JSON. We POST manifest files to it, these get validated, and the work they define gets deployed to the cluster.\n The cluster store  The config and state of the cluster gets persistently stored in the cluster store, which is the only stateful component of the cluster and is vital to its operation.The cluster store is based on etcd, the popular distributed, consistent and watchable key-value store. As it is the single source of truth for the cluster, you should take care to protect it and provide adequate ways to recover it if things go wrong.\n The controller manager  They tend to sit in loops and watch for changes, the aim is to make sure the current state of the cluster matches the desired state.\n The scheduler  Watches and executes new workloads.\nArchitecture   The sole way to interact with a Kubernetes cluster is through API over HTTP using JSON.\n  The core API primitives of a kubernetes cluster includes:\n Pods Controllers Services Storage  PODS  Pods is a core API primitive that contains one or more containers. Pod is the most basic unit of work (atomicity) and it is a unit of scheduling. Pods are ephemeral, which means that a destroyed pod can not be re-deployed. Once a pod is destroyed, it never comes back. Kubernetes job is to keep up and ranning all pods and applications running on them. To do that:  Kubernetes tracks the state of the pods. Check health about whether the application inside the pod is running. Kubernetes checks the health of pods and applications through \u0026ldquo;liveness probes\u0026rdquo;, which are responses to health check requests.      Workloads Command lines   Kubectl:\n Relies on config file: $HOME/.kube/config. This file contains target cluster anme and credentials.  $ gcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME # Authorizes interact with GCP from command line $ kubectl config view # view current config $ kubectl COMMAND TYPE NAME FLAGS COMMAND= what do you want to do TYPE= on which type of object NAME=Name fo the object FLAGS=Extra information   GKE basics\n Create a cluster  $ export MY_zone=us-central2-a $ export MY_cluster=test-cluster-1 $ # To create a GKE cluster with default paramaters. You can choose more from [here](https://cloud.google.com/sdk/gcloud/reference/container/clusters/create) $ gcloud container clusters create $MY_cluster --num-nodes 3 --zone $MY_zone --enable-ip-alias $ # To increase the number of nodes. To verify this, from the GUI click on kubernetes enginet -\u0026gt; clusters $ gcloud container clusters resize $MY_cluster --zone $MY_zone --num-nodes=4   Connect to the cluster\nTo create a kubeconfig file:\n we need the credentials of the current user,to allow authentication and provide the endpoint details for a specific cluster, to allow communicating with that cluster through the kubectl command-line    $ # this command will create ~/.kube/config $ gcloud container clusters get-credentials $my_cluster --zone $my_zone You have to run above command to connect to a cluster created by another user or in another environment. This comamnd is also let you switch between the active context to a different cluster.\n Inspect the cluster  $ # To see the config $ kubectl config view $ # Print the cluster information for the active context $ kubectl cluster-info $ # To print the active context $ kubectl config current-context $ # To print details for all the cluster contexts in the kubeconfig file $ kubectl config get-contexts $ # To change context $ kubectl config use-context gke_${GOOGLE_CLOUD_PROJET}_us-west1-d_standart-cluster-3 $ # To view the resource usage across the nodes of the cluster $ kubectl top nodes $ # To view the resource usage acrossthe nodes of the cluster $ kubectl top nodes $ # To enable bash autocompletion for kubectl $ source \u0026lt;(kubectl completion bash) NOTE: The full name of the cluster, which includes the gke prefix, the project ID, the location, and the display name, all concatenated with underscores.\n Deploy pods  $ # To deploy nginx as a pod and named nginx-1 $ kubectl create deployment --image nginx nginx-1 $ # to view all the deployed Pods in the active context cluste $ kubectl get pods $ # export your pod name and verify your env variable $ export MY_nginx_pod=nginx-1-74ca2123b22-nvxx $ # See details of the pod you creatd $ kubectl describe pod $MY_nginx_pod $ # Copy a test.html into the container $ kubectl cp ~/test.html $MY_nginx_pod:/usr/share/nginx/html/test.htmly $ # Expose the Pod through services $ kubectl expose pod $MY_nginx_pod --port 80 --type LoadBalancer $ # To view details of a service. Repeat this a couple of times before see the external IP $ kubectl get services $ # verify that we can access the test.html $ curl http://EXTERNAL_IP_ADDRESS/test.html $ # To view the resources beign used by the nginx pod $ kubectl top pods  To introspec GKE pods  $ # Clone the following $ $ cat ak8s/shell/newNginxPod.yaml apiVersion: v1 kind: Pod metadata: name: new-nginx labels: name: new-nginx spec: containers: - name: new-nginx image: nginx ports: - containerPort: 80 $ kubectl apply -f ./newNginxPod.yaml $ kubectl get pods $ # Connect to the container, and make some modifications to test.html $ kubectl exec -it newNginx /bin/bash $ # enable port forwarding to connect to the pod from cloud shell $ # From 10082 of the cloud shell to port 80 of the nginx container $ kubectl port-forward newNginx 10082:80 $ # From a second cloud shell $ curl http://127.0.0.1:10081/test.html  View logs  $ # In a new cloud shell $ kubectl logs newNginx -f --timestamps     "
},
{
	"uri": "http://example.org/notes/go/",
	"title": "Go",
	"tags": [],
	"description": "",
	"content": "References The following notes were extracted, adjusted or extended from the following references.\n Go mastering by Mihalis Tsoukalos  General 1. Go inserts only a semicolon at the end of a \u0026ldquo;{\u0026quot;\nfunc main() { // \u0026lt;-- this will trigger error .... } func main(){ // \u0026lt;-- this will NOT trigger error .... } 2. Install and clean packages\n$ go get -v github.com/mastsoud/go/package_name ..... $ go clean -i -v -x package_name $ rm -rf ~/go/src/github.com/mastsoud/go/package_name 3. Stdin/out/err\n   Go Unix     os.Stdin stdin \u0026ndash;\u0026gt; /dev/stdin \u0026ndash;\u0026gt; /proc/self/fd/0   os.Stdout stdout \u0026ndash;\u0026gt; /dev/stdout \u0026ndash;\u0026gt; /proc/self/fd/1   os.Stderr stderr \u0026ndash;\u0026gt; /dev/stderr \u0026ndash;\u0026gt; /proc/self/fd/2    4. Reading input\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main(){ var f *os.File f = os.Stdin defer f.Close() scanner := bufio.NewScanner(f) for scanner.Scan() { fmt.Println(\u0026#34;\u0026gt;\u0026#34;, scanner.Text()) } } 5. Logs\n rsyslogd configuration  $ grep -v \u0026#34;#\u0026#34; /etc/rsyslog.conf  Syslog  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;log/syslog\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) func main() { programName := filepath.Base(os.Args[0]) sysLog, err := syslog.New(syslog.LOG_INFO|syslog.LOG_LOCAL7,programName) if err != nil { log.Fatal(err) } else { log.SetOutput(sysLog) } log.Println(\u0026#34;LOG_INFO + LOG_LOCAL7: Logging in Go!\u0026#34;) sysLog, err = syslog.New(syslog.LOG_MAIL, \u0026#34;Some program!\u0026#34;) if err != nil { log.Fatal(err) } else { log.SetOutput(sysLog) } log.Println(\u0026#34;LOG_MAIL: Logging in Go!\u0026#34;) fmt.Println(\u0026#34;Will you see this?\u0026#34;) } 6. Errors\n Error types  package main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) func returnError(a, b int) error { if a == b { err := errors.New(\u0026#34;Error in returnError() function!\u0026#34;) return err } else { return nil } } func main() { err := returnError(1, 2) if err == nil { fmt.Println(\u0026#34;returnError() ended normally!\u0026#34;) fmt.Println(err) } err = returnError(10, 10) if err == nil { fmt.Println(\u0026#34;returnError() ended normally!\u0026#34;) } else { fmt.Println(err) } if err.Error() == \u0026#34;Error in returnError() function!\u0026#34; { fmt.Println(\u0026#34;!!\u0026#34;) } }  Typical handling of errors  if err != nil { fmt.Println(err) or log.Println(err) or panic(err) os.Exit(10) }  Example  package main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { if len(os.Args) == 1 { fmt.Println(\u0026#34;Please give one or more floats.\u0026#34;) os.Exit(1) } arguments := os.Args var err error = errors.New(\u0026#34;An error\u0026#34;) k := 1 var n float64 for err != nil { if k \u0026gt;= len(arguments) { fmt.Println(\u0026#34;None of the arguments is a float!\u0026#34;) return } n, err = strconv.ParseFloat(arguments[k], 64) k++ } min, max := n, n for i := 2; i \u0026lt; len(arguments); i++ { n, err := strconv.ParseFloat(arguments[i], 64) if err == nil { if n \u0026lt; min { min = n } if n \u0026gt; max { max = n } } } fmt.Println(\u0026#34;Min:\u0026#34;, min) fmt.Println(\u0026#34;Max:\u0026#34;, max) } 7. Using docker\n Dockerfile  FROM golang:alpine RUN mkdir /files COPY hw.go /files WORKDIR /file RUN go build -o /files/hw hw.go ENTRYPOINT [\u0026#34;/files/hw\u0026#34;] $ docker build -t go_hw:v1 . $ docker run go_hw:v1 Go internals 1. Go compiler\n Compiling source file and generate Object code.  $ go tool compile sourceFile.go $ ls -ltr sourceFile.o # This is not executable.  Genereate an object file instaed of object code  $ go tool compile -pack sourceFile.go $ ls -ltr sourceFile.a  The following will list the content of *.a file.  $ ar t sourceFile.a __.PKGDEF _go_.o  To detect trace conditions  $ go tool compile -race sourceFile.a  Showing assembly code  $ go tool compile -S sourceFile.go os.(*File).close STEXT dupok nosplit size=26 args=0x18 locals=0x0 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tTEXT\tos.(*File).close(SB), DUPOK|NOSPLIT|ABIInternal, $0-24 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$0, $-2 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$1, $-2 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tFUNCDATA\t$0, gclocals·e6397a44f8e1b6e77d0f200b4fba5269(SB) 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tFUNCDATA\t$1, gclocals·69c1753bd5f81501d95132d08af04464(SB) 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tFUNCDATA\t$2, gclocals·9fb7f0986f647f17cb53dda1484e0f7a(SB) 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$0, $1 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$1, $1 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tMOVQ\t\u0026#34;\u0026#34;..this+8(SP), AX 0x0005 00005 (\u0026lt;autogenerated\u0026gt;:1)\tMOVQ\t(AX), AX 0x0008 00008 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$0, $0 0x0008 00008 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$1, $0 0x0008 00008 (\u0026lt;autogenerated\u0026gt;:1)\tMOVQ\tAX, \u0026#34;\u0026#34;..this+8(SP) 0x000d 00013 (\u0026lt;autogenerated\u0026gt;:1)\tXORPS\tX0, X0 ... 2. Garbage collector\n Example of GC  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func printStats(mem runtime.MemStats) { runtime.ReadMemStats(\u0026amp;mem) fmt.Println(\u0026#34;mem.Alloc:\u0026#34;, mem.Alloc) fmt.Println(\u0026#34;mem.TotalAlloc:\u0026#34;, mem.TotalAlloc) fmt.Println(\u0026#34;mem.HeapAlloc:\u0026#34;, mem.HeapAlloc) fmt.Println(\u0026#34;mem.NumGC:\u0026#34;, mem.NumGC) fmt.Println(\u0026#34;-----\u0026#34;) } func main() { var mem runtime.MemStats printStats(mem) for i := 0; i \u0026lt; 10; i++ { s := make([]byte, 50000000) // memory allocation if s == nil { fmt.Println(\u0026#34;Operation failed!\u0026#34;) } } printStats(mem) for i := 0; i \u0026lt; 10; i++ { s := make([]byte, 100000000) // MORE memory allocation if s == nil { fmt.Println(\u0026#34;Operation failed!\u0026#34;) } time.Sleep(5 * time.Second) } printStats(mem) } $ go run gColl.go  To get MORE information  $ GODEBUG=gctrace=1 go run gColl.go gc 1 @0.027s 0%: 0.048+0.72+0.007 ms clock, 0.38+0.18/0.62/1.1+0.057 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 2 @0.047s 0%: 0.006+0.56+0.007 ms clock, 0.055+0.39/0.69/0.79+0.061 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 3 @0.060s 1%: 0.16+1.0+0.011 ms clock, 1.2+1.3/1.1/0.13+0.093 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 4 @0.069s 1%: 0.016+0.54+0.016 ms clock, 0.13+0.16/0.61/1.0+0.12 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 5 @0.073s 1%: 0.002+0.37+0.003 ms clock, 0.023+0/0.37/1.0+0.028 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 6 @0.076s 1%: 0.016+0.41+0.011 ms clock, 0.13+0.14/0.60/0.40+0.088 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 7 @0.079s 1%: 0.002+0.34+0.010 ms clock, 0.020+0.14/0.36/0.82+0.081 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P # command-line-arguments gc 1 @0.001s 10%: 0.002+1.3+0.011 ms clock, 0.018+0.82/1.4/2.0+0.092 ms cpu, 5-\u0026gt;6-\u0026gt;6 MB, 6 MB goal, 8 P gc 2 @0.010s 7%: 0.004+3.4+0.011 ms clock, 0.036+0.42/5.3/3.8+0.090 ms cpu, 13-\u0026gt;14-\u0026gt;13 MB, 14 MB goal, 8 P gc 3 @0.044s 4%: 0.017+4.8+0.011 ms clock, 0.13+0.18/8.2/18+0.093 ms cpu, 25-\u0026gt;25-\u0026gt;23 MB, 26 MB goal, 8 P mem.Alloc: 125896 Taking the example of \u0026ldquo;4-\u0026gt;4-\u0026gt;0\u0026rdquo;:\n - The first number is the heap size when the garbage collector is about to run. - The second value is the heap size when the garbage collector ends its operation. - The last value is the size of the live heap  3. GC internals\nGC in slices\nGC in maps\nGC in maps without pointers\nGC in spliting maps\nGC comparisons`\nGC unsafe code\nGC unsafe package\nDefer keyword\nDefer keyword using logging\nPanic\nRecover\nStrace\ndtrace\nGo environment\nNode trees\nGo build\nWebAssembly code\nData types 1. Slices\n  Slices are passed by reference to functions.\n  Slices are often used, more than arrays.\n  mySlcie := []int{1,23,4} mySlice := make([]int, 20) // Go initilise with default values mySlice = append(mySlice, 2134) len(mySlice) fmt.Println(mySlice[1:3])  Re-slicing may cause some problems.  Reslice do not copy values, reslice keeps reference from the orignal slice. Therefore, if you make any change of values in the reslice, they will also change values in the original slice.\npackage main import \u0026#34;fmt\u0026#34; func main() { s1 := make([]int, 5) reSlice := s1[1:3] // Reslice not copy from s1, it make reference fmt.Println(s1) fmt.Println(reSlice) reSlice[0] = -100 // This means also s[1]=-100 reSlice[1] = 123456 // This also means s[2]=123456 fmt.Println(s1) fmt.Println(reSlice) } Output\n$ go run reslice.go [0 0 0 0 0] [0 0] [0 -100 123456 0 0] [-100 123456]   If the length and the capacity of a slice have the same values and you try to add another element to the slice, the capacity of the slice will be doubled whereas its length will be increased by one.\n  Byte slices\n  s := make([]byte,5) **2. Copy slices** - Becareful using copy(destination, source), as copy() copies the minimum number of len(dst) and len(src) elements **3. Sort slice** **4. Appending arrays to slices** **5. Maps** - Declaration ```bash iMap = make(map[string]int) delete(mapName, Key) for key, value := range iMap { fmt.Println(key, value) } The bad thing is that if you try to get the value of a map key that does not exist in the map, you will end up getting zero, which gives you no way of determining whether the result was zero because the key you requested was not there or because the element with the corresponding key actually had a zero value. This is why we have _, ok in maps.\n_, ok := iMap[\u0026#34;doesItExist\u0026#34;] if ok { fmt.Println(\u0026#34;Exists!\u0026#34;) } else { fmt.Println(\u0026#34;Does NOT exist\u0026#34;) } Please note that you cannot and should not make any assumptions about the order the map pairs are going to be displayed on your screen because that order is totally random.\nThe next Go code will not work because you have assigned the nil value to the map you are trying to use:\naMap := map[string]int{} // var aMap map[string]int aMap = nil fmt.Println(aMap) aMap[\u0026#34;test\u0026#34;] = 1 Saving the preceding code to failMap.go and trying to compile it will generate the next error message:\n$ go run failMap.go map[] panic: assignment to entry in nil map This means that trying to insert data to a nil map will fail. However, looking up, deleting, finding the length, and using range loops on nil maps will not crash your code.\n6. Constants\n7. Pointers\nWhen working with pointers, you need * to get the value of a pointer, which is called dereferencing the pointer, and \u0026amp; to get the memory address of a non-pointer variable\npackage main import ( \u0026#34;fmt\u0026#34; ) func getPointer(n *int) { *n = *n * *n fmt.Println(\u0026amp;n) } func returnPointer(n int) *int { v := n * n return \u0026amp;v } func main(){ n:=3 getPointer(\u0026amp;n) fmt.Println(n) k := returnPointer(12) fmt.Println(*k) fmt.Println(k) } Pointers allow you to share data, especially between Go functions. Pointers can be extremely useful when you want to differentiate between a zero value and a value that is not set\nTime\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main(){ fmt.Println(time.Now()) time.Sleep(time.Second*2) t:=time.Now() fmt.Println(t.Day(),t.Month(), t.Year()) t2:=time.Now() fmt.Println(t2.Sub(t)) fmt.Println(\u0026#34;time units\u0026#34;, time.Nanosecond , time.Microsecond , time.Millisecond , time.Minute , time.Hour) }  Parsing date +time from a string:     CODE Meaning     2006 Year   Jan Month   02 Day   15 Hour   04 Minute   05 Second    package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main(){ test:=\u0026#34;20201025 15:20:33\u0026#34; d,_:=time.Parse(\u0026#34;20060102 15:04:05\u0026#34;,test) fmt.Println(d.Hour(),d.Minute(),d.Second()) fmt.Println(d.Year(),d.Month(),d.Day()) }  Measure time execution  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main(){ start:=time.Now() time.Sleep(time.Second) fmt.Println(time.Since(start)) } Composite Types 1. Struct\npackage main import ( \u0026#34;fmt\u0026#34; ) type XY struct{ x int y int } func returnPointer(x,y int) *XY { x++ y++ return \u0026amp;XY{x,y} } func returnStruct(x,y int) XY { x-- y-- return XY{x,y} } func main(){ s1:=returnPointer(3,4) s2:=returnStruct(3,4) fmt.Println((*s1).x) // You need a pointer reference in this case fmt.Println(s2.x) } 2. Tuples\npackage main import ( \u0026#34;fmt\u0026#34; ) func retThree(x int) (int, int, int) { return 2 * x, x * x, -x } func main() { fmt.Println(retThree(10)) n1, n2, n3 := retThree(20) fmt.Println(n1, n2, n3) n1, n2, n3 = n3,n2,n1 //swap fmt.Println(n1, n2, n3) } **3. Json\u0026#34;\u0026#34; **4. XML\u0026#34;\u0026#34; - ### Data Structures ### Functions **1. Return values of function ```bash func namedMinMax(x, y int) (min, max int) { if x \u0026gt; y { min = y max = x } else { min = x max = y } return } Note that the return as this function has named return values in its signature, the min and max parameters are automatically returned in the order in which they were put into the function definition.\n2. FUcntions with pointer parameters\nfunc getPtr(v *float64) float64 { return *v * *v } func main() { x := 12.2 fmt.Println(getPtr(\u0026amp;x)) } 3. Functions that return pointers\nfunc returnPtr(x int) *int { y := x * x return \u0026amp;y } func main() { sq := returnPtr(10) fmt.Println(\u0026#34;sq value:\u0026#34;, *sq) fmt.Println(\u0026#34;sq memory address:\u0026#34;, sq) } 4. Functions that return other functions\nfunc funReturnFun() func() int { i := 0 return func() int { i++ return i * i } } func main() { i := funReturnFun() j := funReturnFun() mt.Println(\u0026#34;1:\u0026#34;, i()) fmt.Println(\u0026#34;2:\u0026#34;, i()) fmt.Println(\u0026#34;j1:\u0026#34;, j()) fmt.Println(\u0026#34;j2:\u0026#34;, j()) fmt.Println(\u0026#34;3:\u0026#34;, i()) } Executing returnFunction.go will produce the following output:\n$ go run returnFunction.go 1: 1 2: 4 j1: 1 j2: 4 3: 9 As you can see from the output of returnFunction.go , the value of i in funReturnFun() keeps increasing and does not become 0 after each call either to i() or j() .\n5. Functions that accept other functions as paramters\nfunc function1(i int) int { return i + i } func function2(i int) int { return i * i } func funFun(f func(int) int, v int) int { return f(v) } func main() { fmt.Println(\u0026#34;function1:\u0026#34;, funFun(function1, 123)) fmt.Println(\u0026#34;function2:\u0026#34;, funFun(function2, 123)) fmt.Println(\u0026#34;Inline:\u0026#34;, funFun( func(i int) int { return i * i*i }, 123)) } Executing funFun.go will produce the next output:\n$ go run funFun.go function1: 246 function2: 15129 Inline: 1860867 6. Varadic functions\nfunc varFunc(input ...string) { fmt.Println(input) } func oneByOne(message string, s ...int) int { # accepts a single string and a variable number of integer arguments. fmt.Println(message) sum := 0 for i, a := range s { fmt.Println(i, a) sum = sum + a } s[0] = -1000 return sum } func main() { arguments := os.Args varFunc(arguments...) sum := oneByOne(\u0026#34;Adding numbers...\u0026#34;, 1, 2, 3, 4, 5, -1, 10) fmt.Println(\u0026#34;Sum:\u0026#34;, sum) s := []int{1, 2, 3} sum = oneByOne(\u0026#34;Adding numbers...\u0026#34;, s...) fmt.Println(s) } The input function argument is a slice and will be handled as a slice inside the varFunc() function. The \u0026hellip; operator used as \u0026hellip;Type is called the pack operator, whereas the unpack operator ends with \u0026hellip; and begins with a slice. A variadic function cannot use the pack operator more than once.\nBuilding and executing variadic.go will generate the following output:\n$ ./variadic 1 2 [./variadic 1 2] Adding numbers... 0 1 1 2 2 3 3 4 4 5 5 -1 6 10 Sum: 24 Adding numbers... 0 1 1 2 2 3 [-1000 2 3] 7. Packages\n  How to deploy own package.\n Verify where go stores package source code.  $ echo $GOPATH /home/percy/go  Create a directory inside ~/go/src. This folder will be installed by \u0026ldquo;go install\u0026rdquo;  $ mkdir -p /home/percy/go/src/mypackagefolder $ touch /home/percy/go/src/mypackagefolder/packageCode.go  Copy the package source code inside packageCode.go. For example  package packageCodeImplementation // This is called by external functions import ( \u0026#34;fmt\u0026#34; ) func A() { fmt.Println(\u0026#34;This is function A!\u0026#34;) } const MY=123  Install mypackageFolder, not the packageCode.go.  $ go install mypackageFolder Note that \u0026ldquo;go install mypackageFolder\u0026rdquo; will create a \u0026ldquo;mypackageFolder.a\u0026rdquo; file at \u0026ldquo;/home/percy/go/pkg/linux_amd64/\u0026rdquo;\n Import mypackageFolder from another go code, for example.  package main import ( // Imports the FOLDER where the package is located. \u0026#34;mypackageFolder\u0026#34; // this calls mypackageFolder, not the packageCode and not packageCodeImplementaion. \u0026#34;fmt\u0026#34; ) func main() { fmt.Println(\u0026#34;Using packageCode!\u0026#34;) // this calls the pacakgeCodeImplemetation, not the mypackageFolder nor packageCode packageCodeImplementation.A() fmt.Println(packageCodeImplemenation.MY) }   NOTE: The following applies to FUNCTIONS, VARIABLES, TYPES, ETC.\n   Name comment     fmt.Println Functions that start with UPPER case are PUBLIC   fmt.myPrint It starts with LOWER case, it is PRIVATE    8. Init()\nEvery Go package can optionally have a private function named init() that is automatically executed at the beginning of the execution time. The init() function is a private function by design, which means that it cannot be called from outside the package in which it is contained. Additionally, as the user of a package has no control over the init() function, you should think carefully before using an init() function in public packages or changing any global state in init() .\nInit() is executed only once at the time where the import calls the package. For example:\npackage a import ( \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;init() a\u0026#34;) } func FromA() { fmt.Println(\u0026#34;fromA()\u0026#34;) } package b import ( \u0026#34;a\u0026#34; \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;init() b\u0026#34;) } func FromB() { fmt.Println(\u0026#34;fromB()\u0026#34;) a.FromA() } package main import ( \u0026#34;a\u0026#34; \u0026#34;b\u0026#34; \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;init() manyInit\u0026#34;) } func main() { a.FromA() b.FromB() } Will results as:\n$ go run manyInit.go init() a init() b init() manyInit fromA() fromB() fromA() 9. Modules\nGo modules allow you to write things outside of GOPATH. Modules are used to specify dependencies and their locations.\nFor example\npercy@prec:m$ mkdir test percy@prec:m$ cd test percy@prec:m$ touch test.go percy@prec:m$ cat test.go package main import ( v1 \u0026#34;github.com/percyperezdante/gomod\u0026#34; ) func main() { v1.Version() } requires github.com/percyperezdante/gomod which is in github.com and contain the following:\npackage gomod import ( \u0026#34;fmt\u0026#34; ) func Version() { fmt.Println(\u0026#34;Version 1.0.0\u0026#34;) } If you run at this stage you will get a similar error as the following:\ntest.go:3:5: cannot find package \u0026#34;github.com/percyperezdante/gomod\u0026#34; in any of: /usr/local/go/src/github.com/percyperezdante/gomod (from $GOROOT) /home/percy/go/src/github.com/percyperezdante/gomod (from $GOPATH) To execute test.go you could run the following:\n$ cd test $ export GO111MODULE=on $ go mod init anyname $ vim go.mod $ cat go.mod module anyname go 1.14 require github.com/percyperezdante/gomod v1.0.0 And then\n$ go run test.go NOTE  To remove this module:  $ go env GOPATH /usr/local/go $ cd $GOPATH/pkg/mod/github.com/percyperezdante $ ls -ltr dr-x------ 2 percy percy 4096 May 21 08:47 gomod@v1.0.0 dr-x------ 2 percy percy 4096 May 21 08:58 gomod@v1.1.0 $ rm -rf gomod@v1.0.0 $ cd $GOPATH/pkg/mod/cache/download/github.com/percyperezdante $ rm -rf gomod  To keep all dependencies in the current directory  $ cd test $ go mod init anyname $ go mod vendor Go downloads all dependencies in inside the test/vendor folder\n10. Type assertions\nA type assertion is the x.(T) notation, where x is an interface type and T is a type. Additionally, the actual value stored in x is of type T and T must satisfy the interface type of x.\nfunc main() { var myInt interface{} = 123 // This declares and defines an interface k, ok := myInt.(int) if ok { fmt.Println(\u0026#34;Success:\u0026#34;, k) } v, ok := myInt.(float64) if ok { fmt.Println(v) } else { fmt.Println(\u0026#34;Failed without panicking!\u0026#34;) } i := myInt.(int) // This is int and = 123 fmt.Println(\u0026#34;No checking:\u0026#34;, i) j := myInt.(bool) // This will fail as it myInt is int not boolean fmt.Println(j) } "
},
{
	"uri": "http://example.org/devops/gcp/fundamentals/",
	"title": "Fundamentals",
	"tags": [],
	"description": "",
	"content": "Introduction GCP offers four main services:\n- Compute - Storage - Big data - Machine learning  The fundamentals course covers mainly the first two plus networking.\nCloud computing is an on-demand infrastrucuture available under the following characteristics:\n- Compute resouces on-demand self-service: no human intervention - Access from anywhere in the internet - Resource pooling: GCP provides share-base resources - Rapid elasiticity: Get more resources quickly as needed - Measured service: pay for what you consume or use  There are serveral GCP computing services, such as:\n- IaaS: where you pay for what you allocated - PaaS: where you pay for what you use  GCP allocation:\n- Zone: Deploymenat area, not geographically related. A zone is a single failure domain witdh in a region. - Region: Group of zone, independen geographical areas.  The basics To work in GCP you organise your work load in projects. These projects organize GCP resorues with common basic objectives. Access to GCP is through IAM, GCP ID and Access Management, and it defifnes who can do what.\nUser interfaces to access and interact with GCP includes:\n- Web interface ( GCP console and generally used at the begining ) - SDK ( gcloud, gsutil for cloud storage, bq for Big query ) - Command line ( cloud shell ) - RESTful API ( uses JSON and OAuth 2.0 authentication and authorization ) - Mobile App  All resources in GCP are organised into projects.Optionally, this projects can be organised into folder and subfolders. All these projects, folders and subfolders that belongs to an organisation can be brought under an organisation node. Each project is separated compartiment and usually have the following id:\n- Project ID: Global inmutable unique name chosen by you. - Project name: Can be mutable and chosen by you. - Project number: Global inmutable number given by GCP.  On the other hand, a policy is set on a resource, where each policy contains a set of roles and role members. Resources inherit policies from parents, where a less restrictive policy overrides a more restrictive resource policy. For example, if a organisation node policy is set to read only, but a project policy is set to read and write, then read and write is possible for that project. Take in mind this when you design your policies.\nThe Identity and Access Management, IAM, helps to manage access rights to currents users of a project. There are three parts:\n- Who: Identifies the user or resource, such as google or service account, group, domain. - How/when: This uses AIM Role, a collection of permissions: primitive, predefined, and custom. - Primitive are broad an include: owner, editor , viewer and billing administrator. - Predefined: Pre designed rules that can be used. - Custom: Where you design and set your own roles. - What: GCP Resource.  Note to give access permissions to a VM you need to use a service account.\nGCP Launcher is a quick tool for deployment that contains a pre-packaged and ready-to-deploy solutions. Some of these solutions are offered by Google and others by third-party vendors. GCP upgrades on the VMs do not update installed packages, but GCP allows you to maintain them.\nVirtual Private Cloud (VCP) network VCP is a set of one or many virtual machines interconnected through a virtual network inside your project in GCP. It is similar to a traditional network where you can define your own firewall rules to restrict access to internal resources or create static routes to redirect traffic to specific destination. An important feature of VCP in GCP is its global scope, which allow your virtual machines to be reachable globaly. It is possible to allocate resources in different zones or expand resources, such as storage or network, to any virtual machine inside your VCP. One tip at this stage is to use preemptible VMs which allows you in some extend to reduce costs.\nA VM can be creatd by console UI or by command line, gcloud. If there is not a pre-define image of your interest, you can customise your own image. Additionally, in cases such as intensive data analysis, you can use a local SSD disk. However, you need to store the data of permanent value in different place as GCP will not leave data on your local SSD disk after all process has finished. For this last case, you can use GCP persistent disks.\nAt booting time, you can als provide to GCP start-up scripts or metadata to initialise your VM. It is possible to define the number of CPUs and memory size for each VM, for example, at the time of this writing, the maximum number of CPU to provision was 96, and the maximum memory size in data was 624 GB. To complete tasks such as intensive data analytics, GCP offers auto-scale, which automatically deploys new VMs base on the load of your task. Additionally, GCP offers cloud load balancing base on the incoming traffic. The options that GCP offer for cross regional load balancing are:\n- Global HTTP(s): Layer 7 load balancing based on load. It routes different URLs to different backends. - Global SSL proxy: Layer 4 load balancing for non-HTTPS SSL traffic. - Global TCP proxy: Layer 4 load balancing of non-SSL TCP traffic. - Regional: Load balancing of any traffic, TCP and UDP on any port. - Regional internal: Load balancing of traffic inside a VPC. All options have this option.  Inside a VCP, GCP already set up a firewall and routers, you do not need to provision these resources manually. However, you can configurate them as you need. This feature comes automatically because VCP networks belong to GCP projects. In case you need to allow communication between two VCP from different GCP project, you can use VCP peering. Additonally, you can difine access rules between VCPs using VCP sharing, which define who and what can be accessed from one VCP to another.\nOn the other hand, GCP offers also Cloud DNS, which can be manage programatically through its API or by command line. You can manage zones, which includes edit, add, remove DNS entries. Finally, Cloud CDN is able to cache your content close to your clients.\nNote that in the case you want to interconnect with external networks or your own local network, GCP offers the following options:\n- VPN: Through a cloud router using Boarder Gateway protocol. This means you access your GCP through internet. - Direct peering: This is private connection between your network and GCP. This is not cover GCP SLA. - Carried peering: Connection through the largest partner network of service providers. - Dedicated Interconnect: Connects nx10G transport circuit for private cloud traffic.  How to create a VM using UI console\n1. Login to GCP console --\u0026gt; Click product and services --\u0026gt; Compute Engine --\u0026gt; VM instances --\u0026gt; create. 2. Edit name, zone, VM specs as you need --\u0026gt; Click create. How to create a VM by command line\n1. Click on Activate Google Cloud shell ( icon on the top bar ) 2. $gcloud compute zones list # List all available zones 3. $gcloud config set compute/zone us-central1-c # Sets the zone  4. $gcloud compute instances create \u0026#34;MYVM\u0026#34; \\ --machine-type \u0026#34;n1-standard-1\u0026#34; \\ --image-project \u0026#34;debian-cloud\u0026#34; \\ --image \u0026#34;debian-9-stretch-v20170918\u0026#34; \\ --subnet \u0026#34;default\u0026#34; Basic inspection\n- You can ssh into your VM by click on the SSH in the console interface. - You can ssh from one VM to another by ssh \u0026lt;NAME_VM\u0026gt; directly. - You can install a web server, such as nginx-light, and use http to retrieve content. - Use sudo to have admin privileges without a password. Storage You can store data inside the VMs you ship in GCP. However, GCP can store structured, unstructured, transactional and relational data throught the following services:\n- Cloud storage - CLoud SQL - Cloud Spanner - Cloud Data Store - Google Big Table Cloud storage Cloud storage uses object storages to store your data. Object storage is not same as the traditional file storage or block storage. Instead, the whole object is stored by associating it to a key, a key that has a URL form.\nCloud storage is a set of buckets, that are inmutable, which means that you can not edit them, but you can create a new version of them. Cloud storage always encrypt your data in the server side.\nEach bucket has a unique id and location. You can move one bucket from one location to another in order to optimise latency. To control access to your buckets you can use Cloud IAM, which in general is sufficient. Each of the access control lists, ACLs, have two parts: one to specify the user or group of users and the other to specify the type of permissions associated to these users or groups.\nAdditionaly, you can turn on versioninng, which allows you track all modifcations of you object storage. However, if you turn off your versioning, you will always have one version of your object storage, which means that the old version will be replaced by the new one.\nGCP offers four Cloud storage classes:\n- Regional(99.95% availability): Let your store your data in a specific region. Cheaper but less redundant. Examples include: Europe west, asia east. - Multi-regional(99.90% availability): Stores your date in a at least two geographica regions separated by at lease 160 km. Examples include: EU, Asia. Ideal for data frequently accessed. - Nearline(99.00% availability): It is a low cost and ideal for unfrequently accessed data, such as once a month or lest on average. - Coldline(99.00% availability): It is a very low cost and ideal for data accessed less than once a year, such as archiving, online backups and disaster recovery. There are several methods to transfer your data into Cloud storage. These methods include gs-util, drag and drop, online transfer storage service and the offline transfer appicanes tools. Cloud Storage also works with other GCP services to transfer your data, these services include import export tables using BigQuery and cloud SQL, Object storage, logs and backups from App engines, scripts and images from Compute Engine.\nGoogle Big Table\nIt is a NOSQL Big Data database service. Bigtable can scale to billonw of rows and thousands of columns allowing you to store petabytes of data. It is ideal for storing large set of data with low latency, it support high throughput to both read and write, which make it a good choice for operational and analytical analysis.\nIt uses the same open source API as HBase. The advantages of using Bigtable over Hbase are:\n- Scalability: especially when query rates per time increases, GCP manages to scale up your cluster through a machine counter. - Administration: These tasks are transparent to the user, and GCP manages all operational work such as updates, and patches. - Encryption: Data is encrypted in both in-flight and at rest. Also IAM permissions can be applied to RBAC to Bigtable data. - GCP: Bigtable is same data base used by Google\u0026#39;s core services such as search, analytics and maps. Bigtable can be accessed by the following patterns:\n- App API: You can write and read from Bigtable using service layer like VMs, HBase REST server or Java server through HBase client. - Streaming: You can use popular tools such as Cloud Dataflow, Spark Streaming and Storm. - Batches: You can read and write using Hadoop Mapreduce, Dataflow, or Spark. CLoud SQL\nThis is a RDBMS service. Offers MySQL or PostgreSQL databases as service. The benefits of using Cloud SQL rather than set my own database in the cloud, is that Cloud SQL offers:\n- Automatic replication: read, failover and external replicas. This means that Cloud SQL can replicate data within multiple zones without failover. - Backups: Cloud SQL offers you backup your data on-demand or base on schedules. The backup could be vertical, by changing the machine type, or horizontal via read replicas. - Security: Cloud SQL includes network firewalls, customer data encryption when data is in internal Google\u0026#39;s networks. - Visibility: Cloud SQL instances are accessible by other GCP or external services. Cloud SQL can be used with App Engine application. Compute engine instances can be authorized to access Cloud SQL using external IP address and also can be configured with a preferred zone. Additionaly, Cloud SQL can be adminnistrated by external tools or can be set external replicas.  Cloud Spanner\nCloud Spanner offers an horizontal scalability for Cloud SQL. It offers transactional consistency at global scale, schemas , SQL and automatic synchronous replication for high availability. Cloud Spinner can be consider when you have an outgrown any relational database or you need glocal data transactional consistency such as the cases of financial application and inventory applications.\nCloud DataStore\nIt is another high scalable NOSQL database service. Its main case is to store structured data from App Engine applications. Cloud DataStore automaticaly handles replications and sharding.\nSummary\nThe following figures present a summary of technical differences between GCP services and their common use cases.\nNote: These figures are screenshots took during the course. Credits to GCP training material\nHow to get started with Cloud Storage and Cloud SQL  Create a VM with an Apache web server and upload an image to a new bucket.  - Create a VM as it was described on section \u0026#34;how to create a VM ...\u0026#34;. Before click create please add a start up script that install a web server in this VM by expanding the link \u0026#34;management, disks, networking, SSH keys\u0026#34; and look for the \u0026#34;Startup script\u0026#34;. You can copy the following: apt update apt isntall apache2 php php-mysql -y service apache2 restart - Create a bucket and upload any image to this bucket. You can click on cloud shell and type: $ gsutil mb -l EU gs://$DEVSHELL_PROJECT_ID # This creates a bucket with an UNIQUE ID,my google project id $ gsutil cp gs://cloud-training/gcpfxi/my-excellent-blog.png my-image-blog.png # Copy an image to my local VM $ ls # This should show that the images is in your VM now. $ gsutil cp my-image-blog.png gs://$DEVSHELL_PROJECT_ID/my-image-blog.png # This uploads my image to my bucket. $ gsutil acl ch -u allUsers:R gs://$DEVSHELL_PROJECT_ID/my-excellent-blog.png # modify access permissions $ gsutil ls gs://$DEVSHELL_PROJECT_ID # will show you the content of your bucket, not VM. Also you can see open your bucket using the GUI and see if it is there. To do so, click on Storage -\u0026gt; Browser -\u0026gt; Click on the name of the bucket. In here check the box for \u0026#34;Share plublicly\u0026#34;. Please, copy the link somewehre we will use it to point from the index.php page of this webserver. Create SQL instance  - Click on Products and Services -\u0026gt; Storage -\u0026gt; SQL - \u0026gt; Create Instance -\u0026gt; MySQL - Give a name of this instance and set a password. Remember to choose the same ZONE as the VM on step 1. Click create. - Click on the name and create an account: click on users -\u0026gt; create users -\u0026gt; type user name and password -\u0026gt; Click create. - Restrict the access to this SQL instance to VM on point 1. Click authorization -\u0026gt; add network -\u0026gt; Give a name -\u0026gt; Copy the publick IP address of the VM on point 1. Remember add /32 as mask for the public IP address, this is to protect from broad internet access. Configure your Apache main page  - Login to the VM created on point 1 using SSH. - Edit the /var/www/html/index.php and add php code to connect to the DB of point 2 and show the image uploaded in point 1. - Restart Apache and open in a browser \u0026#34;public ip address of VM\u0026#34;/index.php. You should be able to see the image we uploaded on point 1 and see a connection succed message to the SQL instance. As a quick example here some tips: html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Welcome to my excellent blog\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;img src=\u0026#39;https://storage.googleapis.com/qwiklabs-gcp-0005e186fa559a09/my-excellent-blog.png\u0026#39;\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to my excellent blog\u0026lt;/h1\u0026gt; \u0026lt;?php $dbserver = \u0026#34;CLOUDSQLIP\u0026#34;; $dbuser = \u0026#34;blogdbuser\u0026#34;; $dbpassword = \u0026#34;DBPASSWORD\u0026#34;; // In a production blog, we would not store the MySQL // password in the document root. Instead, we would store it in a // configuration file elsewhere on the web server VM instance. $conn = new mysqli($dbserver, $dbuser, $dbpassword); if (mysqli_connect_error()) { echo (\u0026#34;Database connection failed: \u0026#34; . mysqli_connect_error()); } else { echo (\u0026#34;Database connection succeeded.\u0026#34;); } ?\u0026gt; \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Containers in the Cloud You can build container images though tools such as docker or cloud build. However, more tasks are needed to have a reliable and scalable system, therefore more considerations are needed, such as: service discovery, application configuration, managing updates and monitoring.\n Introduction to GKE  Kubernetes is an open source orchestrator for containers, it helps to manage and scale your containers. Kubernetes lets you deploy containers inside nodes called cluster, where a cluster is a set of master nodes and containter nodes. In kubernetes, a node is a computing instance, in contrast, in GCP, a node is a VM running inside a compute engine.\nTo create a Kubernetes cluster in GKE, you can type the following command. Note that Kubernetes create \u0026ldquo;pods\u0026rdquo; to locate one or more than one containers of a Kubernetes cluster, which means a container runs inside a pod.\n``bash $ gcloud container clusters create k1 # This creates a Kubernetes cluster called k1\nHere some useful basic commands: ```bash $ kubectl run nginx --image=nginx:1.15.16 $ kubectl get pods $ kubectl expose deployments nginx --port=80 --type=LoadBalancer # creates public IP to be accessed by public Note that by exposing to the public, GKE creates a service that uses a public IP address. This service is the end point for any request from outside, and is this service that re-directs any outside request to the respective pod. The advantage to use service IP address instead of the pod\u0026rsquo;s IP address is that pods need manages in the case IP addresses change, whereasa service do the manage for you.\n$ kubectl scale services In case you need more resources, you can scale your infrastructure by:\n$ kubectl scale nginx --replica=3 $ kubectl autoscale nginx --min=10 --max=15 --cpu=80 $ kubectl get pods -l \u0026#34;app=nginx\u0026#34; -o yaml # declarative eay to use yaml file for configuration. $ kubectl get replicasets # replicas states $ kubectl get pods $ kubectl get deployments # To verify that replicas are running Hybrid and multi-cloud computing: Anthos  Anthos allows you to move some components of your on-premises application to the cloud, while keeping the rest on your own local infrastrcuture. Anthos allows both, cloud and on-prem, stay in sync, and offers a rich set of tools to manage services on-prem and on cloud, monitoring, migrations of apps from VMs int your clusters, and maintain policies across all clusters.\nApplications in the Cloud  App Engine: PaaS  This could be useful when you want to focus effort on implement your code rather than in the platform where you deploy your application. App engine is a better solution for a web app rather than a longer-running batch processing. Beside run your application, App engines offers other services to your application, such ash NoSQL databases, load balancing, loggin and authetication.\n The App Standard Engine  Run times services includes: java, python, php, and go. If you code in anothe language, Standard environment is not the rigth place, instead you can choose the flexible environment. In standard environment you use sandboxes, which no writes in local files, if you need data persistance, you can write down to a data service instead. Also all request time out at 60s, and has a limit of thrid-party software.\n The App Engine Flexible environment  You deploy your application inside containers, and GCP manage them for you.\n Could Endpoints and Apigee Edge  Cloud endpints helps you to create and mantain APIs, distribute API managemente through a console and expose your API using RESTfil interface. Apigee also helps you secure and monetize APIs. Apigee contains analytics, monetization and a developer portal. It is usually used by business developers when they want to expose legacy code though APIs to another business customers. Instead of replacing the monoitic application all in once, it peal layer by layer or component by component until complete all migration. It usually implementes microservices to expose APIs until the legacy code can be retired.\n Notes   App Engine manages the hardware and networking infrastructure required to run your code. It is possible for and App Engine application\u0026rsquo;s daily billing to drop to zero. Three advatnages for App Engine Standard: scaling is finer-grained, billing can drop to zero if your application is idle, and GCP provides and maintain runtime binaries. In case you want to do business analytics and billing on a customer-facing API, Apigee Edge is a good choice. In case you want to support developers who are building services in GCP through API logging and monitoring, Cloud Endpoints is good choice. Below some gcloud command to deploy an app in App Engine:  To run the app, not deploy it yet:\n$ gcloud auth list $ gcloud config list project $ gcloud components install app-engine-python $ gcloud app create --project=$DEVSHELL_PROJECT_ID $ git clone https://github.com/GoogleCloudPlatform/python-docs-samples $ cd python-docs-samples/appengine/standard_python37/hello_world $ sudo apt-get install virtualenv $ source venv/bin/activate $ pip install -r requirements.txt $ python main.py To run and deploy the app:\n$ cd python-docs-samples/appengine/standard_python37/hello_world $ gcloud app deploy $ gcloud app browse Developing, Deploying and Monitoring in the Cloud  Development   GCP includes a cloud source repository, it provides a git for development. It also includes a source viewer. Payments is in intervals of 100 milliseconds. GCP allows you to create triggers to update new events in your application.   Infrastrcuture as a code   GCP provides you a deployment manager for re-deployment by usign a .yaml template file. This .yaml template file describes your environment which are read by deployment manager. Deployment manager create the resources declared in the .yaml file.   Monitoring   GCP uses stackdriver for monitoring, logging, debug, error reporting, and trace.   Notes   Example of a yaml file to deploy a box in GCP.  $ export MY_ZONE=us-central1-a $ gsutil cp gs://cloud-training/gcpfcoreinfra/mydeploy.yaml test.yaml $ sed -i -e \u0026#39;s/PROJECT_ID/\u0026#39;$DEVSHELL_PROJECT_ID/ test.yaml $ sed -i -e \u0026#39;s/ZONE/\u0026#39;$MY_ZONE/ test.yaml $ cat test.yaml resources: - name: my-vm type: compute.v1.instance properties: zone: us-central1-a machineType: zones/ZONE/machineTypes/n1-standard-1 metadata: items: - key: startup-script value: \u0026#34;apt-get update\u0026#34; disks: - deviceName: boot type: PERSISTENT boot: true autoDelete: true initializeParams: sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/debian-9-stretch-v20180806 networkInterfaces: - network: https://www.googleapis.com/compute/v1/projects/$DEVSHELL_PROJECT_ID/global/networks/defaul accessConfigs: - name: External NAT type: ONE_TO_ONE_NAT $ gcloud deployment-manager deployments create HalloWOrld --config test.yaml # To update VM in caste test.yaml files is modified: $ gcloud deployment-manager deployments update HalloWOrld --config test.yaml  To create CPU Load  $ dd if=/dev/urandom | gzip -9 \u0026gt;\u0026gt; /dev/null \u0026amp; Big Data and Machine Learning in the Cloud   MLlib is to run classification algorithms. - TensorFlow - Used for classification, regresion, recommendation, anomaly detection, image and bideo analysis and text analytics.\n  Cloud Dataflow: - Manage data pipelines - \u0026ldquo;write code once and get batch and streaming\u0026rdquo; - ETL (extract, transform, load) pipelines to move, filter, enrich, shate data - Data analysis usgin streaming\n  BIg query: - Fully managed data warehouse - Provides near real-time interactive analysis of peta-bytes of datasets using SQL 2011 syntax - Computes with a terabit network\n  Data Pub/Sub - Supports many-to-many async messaging - Apps components make push/pull subscriptions to topics\n  Cloud datalab - Interactive data exploration for large-scale data exploration, transformation, analysis, and visualization - Built on Jupyter(Ipython) - Integration with BigQuery, Compute Engine and Cloud Storage using Python, SQL, and JS\n  Cloud vision - Analyse content of images with REST API - Logo detection - Label detection - Extract text\n  Cloud NLP - Return text from audio in real time - High accuracity, even with noise - It can do syntax analysis - Breaking down sentences supplied by our users into tokens - Identify the nouns, verbs, adjectives, and other parts of speech and figure out the relationships among the words\n  Cloud translation API - Translate arbitrary string in to another language\n  Cloud video intelligence API - Annotate content of videos - Detect sceme changes - Flag inappropiate events\n  Summary "
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week4/",
	"title": "SRE MMR Week4",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week3/",
	"title": "SRE MMR Week3",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week2/",
	"title": "SRE MMR Week2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week1/",
	"title": "SRE MMR Week1",
	"tags": [],
	"description": "",
	"content": "Introduction to SRE How does SRE differ from Devops? Devops and SRE point to similar goals, which is break down organizational barriers to deliver software features faster.\nTraditionally, developers are responsable for features and operations for stability. Developers want to move faster to release new features and operations want to move slow to keep all service stable. As a commong results, tension between teams appears during realease times. Devops and SRE practices aim to break downs this tensions.\nIn general, Devops approach include the following areas:\n- Reduce organizational silos: --\u0026gt; Break down tension promotes collaboration between teams. - Accept failure as normal - Implement gradual change: --\u0026gt; Small and incremental changes are easy to review and maintain. - Leverage tooling and automation - Measure as much as you can On the other side, SRE appraoch to same areas are as following:\n- Reduce organizational silos: --\u0026gt; Operation responsability to release in production is shared with developers. - Accept failure as normal: --\u0026gt; Blameless postmordem. Failure is expected and it is hold by an error budget. - Implement gradual change: --\u0026gt; Small and incremental release, such as Canary releases. - Leverage tooling and automation: --\u0026gt; Measure toil and automate to minimise manual intervention. - Measure as much as you can: --\u0026gt; Measure toil, reliability and service. Therefore, one initial conclusion is that:\n SRE is a concrete class that implements Devops\n What is CRE? It stands for Customer Reliability Engineering and it is focus on breaking down organizational barries between service prodivers and customers. In this context, failure is pre-accepted as a conditions to enhance future cases, error budget.This pre-condition helps to minimise panic when a down time happens at the service level. Another important criteria is to implement measurements that can offer to everyone, in both sides, visibility of on how the service performs.\nHow can they help you be more reliable? In various cases, customers use provider\u0026rsquo;s services, through APIs. in different ways that are not expected by the service provider, and providers do not like to breaks the customer expectation. This sceneario is easly visible the number of customers scale up.\nCRE helps to providers to openinly communicate with customers. Clear communication of how your service was designed to behave, indirectly means to expose provider\u0026rsquo;s SRE practices, such is their SLOs, to the customers. By teaching or helping the customers to build their own SRE environment, we are teaching them how to interact correctly with our system or platform. Consequently, the real scope and limitations are presented clearly to the customer, which formilise what the customer can expect as a final result.\nIn conclusion, by sharing SRE practices with customers, not only new features can be released faster, it also enhance customer satisfactione.\nWhy Are SLOs Important for Your Organization? Building new features quickly leads to a negative correlation between development velocity and system reliability. This means that development velocity implies the posibility to break another features that can also affect our customers. Therefore, measuring SLOs gives real indications of possibel effects on the expectation of customers when a new feature is released.You can plan proactively by estimating risks to your reliability from the roll-out of new features in terms of time to detection, time to resolution, and impact percentage. Also, if you have enough error budget you can reduce the effort to cover any potential risk.\nTargeting Reliability Introduction Promises, Promises. SLOs vs SLAs. Happiness Test. How Do We Measure Reliability? Edge Cases How Reliable Should a Service Be? Setting Targets for Reliability. Iterate! Operating for Reliability Introduction When Do We Need to Make a Service More Reliable? Error Budgets. Trading off Reliability Against Features How Do We Make a Service More Reliable? "
},
{
	"uri": "http://example.org/sre/online/",
	"title": "Online resources",
	"tags": [],
	"description": "",
	"content": "The following pages contain notes that I took from courses online. These notes are in here as way to summaries my understanding and record them as personal notes.\n* Site Reliability Engineering: Measuring and Managing Reliability by Google Cloud SRE-MMR This course is divided in four weeks.\n Week 1:  Introduction to SRE Targeting Reliability Operating for Reliability   Week 2 Week 3 Week 4  * SRE CON 2019 Dublin Notes about some of the talks at SRE Dublin 2019\n"
},
{
	"uri": "http://example.org/sre/references/",
	"title": "References",
	"tags": [],
	"description": "",
	"content": "This page present a list of references related to SRE.\nBooks   Beyer, B., Jones, C., Petoff, J. and Murphy, N.R., 2016. Site Reliability Engineering: How Google Runs Production Systems. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;.\n A online available version can be read from here.    Beyer, B., Murphy, N.R., Rensin, D.K., Kawahara, K. and Thorne, S., 2018. The site reliability workbook: Practical ways to implement SRE. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;.\n Online version of this book can be found in here.    Jennifer P., JC van W.,Preston Y., Jessie Y., Jesus C., and Myk T.,2020. Training Site Reliability Engineers. \u0026ldquo;O\u0026rsquo;Reilly Media, Inc.\u0026quot;.\n Online version available at here.     Papers   Sloss, B.T., Nukala, S. and Rau, V., 2019. Metrics that matter. Communications of the ACM, 62(4), pp.88-88.\n Online read in here.     Online resources   Unknown author, unkown year, SLO Workshop Google Cloud, https://www.usenix.org/sites/default/files/conference/protected-files/srecon18emea_slides_fong-jones.pdf.\n  Google Cloud, unkown year, Site Reliability Engineering: Measuring and Managing Reliability, Coursera online courses. https://www.coursera.org/learn/site-reliability-engineering-slos/home/info\n  "
},
{
	"uri": "http://example.org/tools/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "This page presents suggestions on how to install tools of interest. It is ordered alphabetically.\nA D  Docker  $ sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg \\  lsb-release $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null $ sudo apt update $ sudo apt-get install docker-ce docker-ce-cli containerd.io $ docker run hello-world # For testing CASE 1: If you have permissions errors, such as: docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create: dial unix /va r/run/docker.sock: connect: permission denied. You could add your $USER as part of the docker group, and re-login or restart ubuntu if need it :(\n $ sudo usermod -aG docker $USER $ groups $ newgrp docker # To login as part of docker group and to avoid restart. Temporal solution. $ docker run hello-world # For testing G   GO\n $ git clone https://github.com/golang/go.git $ cd go/src $ ./all.bash # \u0026lt;-- This should generate ../bin/go file. $ cd .. $ export GOPATH=`pwd` # \u0026lt;-- Add this to your profile such as ~/.bashrc $ cd bin $ export GOBIN=`pwd` # \u0026lt;-- Add this to your profile such as ~/.bashrc $ go version    GIT\n $ git config --global --list $ git config --global user.email \u0026quot;xxx@ee.com\u0026quot; $ git config --global user.name \u0026quot;xx yyy\u0026quot; $ cat ~/.gitconfig [user] email = xxx@yy.com name = xxx YYY signingkey = ASDF2320xXXX    Packer\n $ git clone https://github.com/hashicorp/packer.git $ cd packer $ go build -o bin/packer . # \u0026lt;-- This will create ./bin/packer executable file $ ./bin/packer version $ sudo ln -s `pwd`/bin/packer /usr/local/bin/packer # \u0026lt;-- Optional $ packer --help    M   Mps-youtube\n $ git clone https://github.com/mps-youtube/mps-youtube.git $ vim ~/.config/mps-youtube/config.json # Be sure you are using the right API_KEY $ ./mpsyt    R   Ruby\n$ sudo apt install ruby-full $ ruby -v\n  Ranger\n  $ git clone https://github.com/ranger/ranger.git $ mkdir ~/.local/share/ranger $ touch~/.local/share/ranger/tagged $ ./setup.py build $ ./ranger.py V   Vi File Manager\n $ mkdir temp $ cd temp $ git clone https://github.com/vifm/vifm.git $ cd vifm $ dpkg -l | grep libncursesw5-dev # \u0026lt;-- be sure you have this package $ ./configure $ sudo make install $ vifm . # \u0026lt;-- Enjoy vi File Manager    Vimium\n This extension is available in Google Chrome and Firefox. It provides keyboard shorcuts for navigation.    Z "
},
{
	"uri": "http://example.org/research/nmanet/",
	"title": "NManet",
	"tags": [],
	"description": "",
	"content": "Very brief overview NDN for MANETs approach, represented as nMANET, aims to offer an alternative perspective on how the characteristics of NDN can be utilised to solve the limitations of MANETs. nMANET has its roots in Named Data Network NDN, an instance of Content Centric Networks CCN. In contrast with traditional TCP/IP networks, CCN enables content addressing instead of host based communication, and secures the content instead of securing the communication channel between hosts. Therefore the content can be obtained from the intermediate caches or final information producers.\nThough NDN has proven to be an effective design in wired networks, it does not perfectly fit in Mobile Adhoc Networks. This is due to the high mobility of mobile devices and their resource constrains such as remaining energy in batteries. nMANET intends to fill this gap by developing a prototype called JNFD and Mini-JNFD.\nMore details about the project will be released in the last trimester of 2020\n"
},
{
	"uri": "http://example.org/devops/images/",
	"title": "Building Images",
	"tags": [],
	"description": "",
	"content": "This page presents a set of templates to build images through tools such as packer. The content of these templates is the result of a combination of available resources online.\nPacker This sections present cases about how to build linux-base images. More details in the Ajayu github repository.\n Oracle Linux 6.10 using a DVD .iso file. Jenkins and Capistrano V2 base on OL6. It is also available in Vagrant Cloud  "
},
{
	"uri": "http://example.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]