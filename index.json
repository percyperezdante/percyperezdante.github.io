[
{
	"uri": "http://example.org/projects/",
	"title": "Projects",
	"tags": [],
	"description": "",
	"content": "Projects This page list some of the small projects I worked in during my free time in ad-hoc mode.\nWawa A baby monitoring trail. Wawa, from the Aymara Baby, is a project inspired in how to help a mum to keep one eye in her newborn baby while she is executing some activity inside the house, in particular in the case when the newborn baby is sleeping and you want to monitor your baby. Qilquiri Qilquiri, from the Aymara Writer, is a helper that generates snipes of common Java-base code from a Vim session. For example, create a class signature by just presing two keys. Achacachi Achachachi is the name of a small town located on the Altiplano in the South American Andes. This project is a small set of scripts to re-deploy my laptop configuration. When my laptop broken or simply needs to be re-installed, \u0026#34;I need to back to Achacachi\u0026#34; to get all the configuration back as before. "
},
{
	"uri": "http://example.org/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": "Devops Infrastructure as a code  Building images: This section presents examples of building images using Packer from HashiCorp.  Cloud Computing  GCP notes Short notes from GCP training sessions.  "
},
{
	"uri": "http://example.org/sre/",
	"title": "SRE",
	"tags": [],
	"description": "",
	"content": "Site Reliability Engineering This page is in its initial stage. I am trying to collect and read references which can be found at references.\n"
},
{
	"uri": "http://example.org/notes/",
	"title": "Notes",
	"tags": [],
	"description": "",
	"content": "General notes This page containts brief examples and explanation of different topics of interests from my students.\nMore details will be added gradually.\nA K  Kubernetes  G  Golang  S  Shell tricks  "
},
{
	"uri": "http://example.org/devops/aws/cloudpractitioner/",
	"title": "CloudPractitioner",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "http://example.org/teaching/resources/",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": "This site present a list of resources shared by or to my students during the teaching and training sessions. I am placing here as a way to remember them and share with people with similar interest.\nA M  Math for programmers by Simon Robinson  This course covers the maths behind how your computer stores and manipulates data. You\u0026#39;ll learn how to read binary and hexadecimal, how both integers and floating point numbers are stored and the limitations of using them. Advice on best practices... "
},
{
	"uri": "http://example.org/devops/gcp/",
	"title": "GCP",
	"tags": [],
	"description": "",
	"content": "Googgle Cloud Platform Notes. This section contains notes about different GCP topics.\nGoogle Cloud Platform Fundamentals: Core Infrastructure "
},
{
	"uri": "http://example.org/tools/",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": "Tools This page is just to remember the references of the tools I think are useful and the tools recommened by friends. It helps me to not searching it again from scratch and also safe time when a friend ask me for more information.\nCategories  Browsing and plugins Text editors Terminals  "
},
{
	"uri": "http://example.org/devops/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "http://example.org/reproducibility/",
	"title": "Reproducibility",
	"tags": [],
	"description": "",
	"content": "Reproducibility The idea to promote reproducibility was inspired by my last research work, nMANET. At many points of my research, I was stopped by many issues, which were required help from experts. Helpers need to reproduce the issue locally, which not always is a trivial task. However, if there is a method to let the helper reproduce the issue in a fast and painless manner, the discusion of the problem became more predominant compared with the effort of reproduce the problem. I am planning to fill this page with more details during my free time, which is the reason of why I tag this site as under construction. "
},
{
	"uri": "http://example.org/teaching/",
	"title": "Teaching",
	"tags": [],
	"description": "",
	"content": "Teaching This site present my teaching and knowledge transfer activities inside and outside the university. Some resources are showed in here in case of interest.\n2020  A practical overview of Vagrant A technical session for Jenkins and Vagrant Using plugins in Jenkins  2019  Introduction to Continuous Integration and pipelines. Introduction to Jenkins  2017/2018 academic teaching activities St-Andrews University. Tutorials: CS2006 Advanced Programming Projects Demonstrations: CS2002 Computer architectures CS2006 Advanced Programming Projects Lecturer assistance: CS3102 Data Communications and Networks. CS2002 Computer architectures CS2006 Advanced Programming Projects  Past years academic teaching activities St-Andrews University. Semester One 2017/2018 Tutorials CS1002 Object Oriented Programming CS1005 Computer Science in Everyday Life CS1003 Programming with Data   Semester One 2017/2018 Demonstrations: CS1002 Object Oriented Programming Exercise classes: CS1002 Object Oriented Programming CS2003 The Internet and the Web: Concepts and Programming   Semester Two 2016/2017 Tutorials: CS1003 Programming with Data Demonstrations: CS1006 Programming Projects CS2002 Computer Systems CS2006 Advanced Programming Projects   Semester One 2016/2017 Tutorials: CS1005 Computer Science in Everyday Life Demonstrations: CS1002 Object Oriented Programming CS2003 The Internet and the Web: Concepts and Programming  "
},
{
	"uri": "http://example.org/research/",
	"title": "Research",
	"tags": [],
	"description": "",
	"content": "Research This section presents the research topics of interest.\nnMANET nManet is the short name for NDN applied in Manets. NDN stands for Named Data Neworking and Manets stands for Mobile Ad-hoc Networks. In this research, nMANET aims to offer an alterntive to ad-hoc mobile exchange of information in emergency scenarios.\n"
},
{
	"uri": "http://example.org/blog/",
	"title": "Blog",
	"tags": [],
	"description": "",
	"content": "Blog This blog is empty for now, but I am planning to write one blog about:\n Cooking rice. Share my life location. Books I read  Hope it come soon.\n"
},
{
	"uri": "http://example.org/funding/",
	"title": "Funding",
	"tags": [],
	"description": "",
	"content": "Demetria \u0026amp; Mario foundation This is a foundation created by Demetria Inocencia Aruni Rojas de Perez and Mario Hermogenes Perez Limachi with the intention to support education and culture for Aymara children in the community of Viacha in La Paz - Bolivia. The Demetria \u0026amp; Mario foundation is in its initial stage and it is planned to start its first intervention by the end of 2020.\nI decided to join this foundation after the talk provided by the founders in Viacha-Bolivia, which explained the importance to invest in education of Aymara Children as a way to preserve Aymara culture and let the future of Viacha coexists with new technoligies that can enhance the life standards of people within the community.\nMore details are planned to appear in the comming months and if you are interested to join, please feel free to contact me.\n"
},
{
	"uri": "http://example.org/devops/aws/cloudpractitioner/coreservices/",
	"title": "CoreServices",
	"tags": [],
	"description": "",
	"content": "Methods to interact to AWS   AWS console\n Includes Mobule app Good for testing services or simple tests    AWS CLI\n Includes: Java, .net, Node.js, JS, PHP, Python, Ruby, Go, C++ automation    AWS SDK\n automation use it when you want to integreat to your app    AWS console   Two types of user: ROOT USRE and IAM user.\n Root has unlimited permissions IAM has limited permission    Multifactor authenticatoin\n  User infromation:\n My account MY org My services quota My billing dahsboard User infromation:  My account MY org My services quota My billing dahsboar Order and invoices Security credentials \u0026lt;=== Set password, and multifactor authentication, access keys      AWS region:\n you can select the one you need    Services:\n all service by category Route 53 is set as \u0026ldquo;GLOBAL\u0026rdquo; for region    AWS CLI  Get a key:  GO to my account -\u0026gt; Your security credentials -\u0026gt; Select Access keys -\u0026gt; Create new access key  This will generate a:  Access key ID Secret access key     DO NOT generate ROOT access key How to use this credentials $ aws --version # To config credentials $ aws configure --profile myteste AWS access key ID: xxxxddd AWS secret access key; xxxxxx Default region: us-east-1 Default output: json # Now you can execute aws commands $ aws s3 ls --profile myteste     Computer services  EC2 Elastick beantalk Lambda  EC2   EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developer\n  When to use:\n Web server Batch processing API server: web server Desktop in the cloud    Main concetps for EC2:\n  Instance types\n Defines the processor, memory and storage type Cannot change without downtime There are different categories:  General purposes Compute, memory and storage optimized accelerated computing, e.g. Machine Learning, access to GPU   Prices are based on instance type  The more resources you use, the more you pay      Root device type\n Two types:  Instance store: ephemeral sotrage that is physically attached to the host the VM you are running on  Data in Ephemeral instance just dessapear when you shutdown the EC2 instance   Elastic Block Store (EBS): Persistent storage that exists separately from the host the VM you are running on  Data in persistance, so data will NOT dessapear when you shutdown the EC2 instance     Usually you may use EBS.    Amazon Machine Image AMI\n Template for an EC2 instance including config, OS, and data AMI can be shared across accounts You can custom your AMI There are comercial AIM\u0026rsquo;s available in the AWS marketplace    Purchase options/types\n  On demand\n Pay by the second for the instance you lunched If you have inconsistent need for instances that can not be stoped without affecting the job    Reserved\n Purchase a discount instance in advance for 1-3 years Use reserve instance if your instance requires to be consistent and always needed     Spot\n You can leaverage unused EC2 capacity in a region for a large discount If you have batch processing when process can start/stop without affecting the job           Launching EC2 instances - Go to console.aws.amazon.com and search for EC2 - Select for \u0026quot;launch instance\u0026quot; - Select an image of your preference: AWS marketplace, community, your AMIs - Select instance type of your preference: - NOTE `12.micro is FREE tiere elegible` - Click on `Configure instance details` and you can select the paramteres you need - You can also `enable` the `auto asign public ip` - In the section `advance details-\u0026gt; user data`, you can copy the shell command to run in the new EC2 instance. - in the user data you can add shell comamnds such as: ``` yum install httpd -y service httpd start ``` - You can add a tag which is a {ID:value} - Configure the security group - For http you need to include SSH from your IP , and HTTP for everyone(0.0.0.0/0) - Review it and then launch it - YOu need to use a ssh key pair: public and private keys to ssh the EC2 instance  to terminate EC2 - Select the EC2 instance - Click Actions and then select instance state and then terminate  EBS Elastic Beanstalk - Support specific set of tech ![EBs general elastic beanstalk](/devops/aws/cloudpractitioner/ebsgeneral.png?width=50pc) - Feautures: - Monitoring - Deployment - Scaling - EC2 customization - When to use: ![EBs use case](/devops/aws/cloudpractitioner/ebsusecases.png?width=50pc)  How to laucnh EBS To launch - Visit the developer page for EBS and select the correct package you want to use - Donwload the package - Go to EBS console - Click get started - Type application name - Choose the platform - You can upload your code - click Create App  To delete ebs - In the console - Click in Actions - Select : Termiante environment - Enter the name of the app - Hit terminate  Lambda  Lambda lets you run code without provisioning or manging servers. You pay only for the compute time you consume. You can run code for virtually any type of app or backend service - all with zero admin\n In general the main feature of Lambda are:  Advantages:  Reduce maintenance requirements Enables fault tolerance Scales based on demand Pricing is based on usage    Scenarios   Case 1: One workload will leaverage at least for 5 years.\n It could use a all upfront reserved - 3 years    Case 2: Developer who does not know about infrastrcuture\n Then EBS could be one option    Case 3: If any workload can be stop and start at anytime\n The best option is SPOT, not reserved     Virtual Private Cloud VPC  VPC is a logically isolated section fothe AWS cloud where you can launch AWS resources in a virtual network that you define\n  It is a virtual network that you define and you can configure Support IPV4, IPV6 You can configure:  Ip address range subnets Route tables Network gateway   VPC support public and private subnets You can use NAT for private subnets Enables a connection to your data center You can connect VPC each other Support private connections to many AWS services  AWS direct connect  It is a cloud service solution that makes it easy to establicsk a dedicated network connection from yur data center to AWS.\n Route 53  It is Domain Name Service It is global AWs service, not regional It is highly available Enables global resource routing Changes are not instantaneous, it requires a couple of hours to be propagated across the world  Elastic load balancer  Elasticity is the ability for the infrastcuture, supporting an app, to grow and contract based on how much it is used at a point in time\n  Distribute traffic across multiple targets Integrates with EC2, ECS and Lambda Support one or more Avaialble Zones in a region Three types of load balancer:  Application load balancers ( ALB) Network load balancers ( NLB) Classic load balancers    Scaling on Amazon EC2  Vertical scaling, scale up  You enhance your existing instance to a larger instance with additional resources such as more RAM, more CPUs   Horizontal scaling, scale out  Add new instances to handle the demand of you application    CloudFront  It is a service that allows you CDN Your users get content from servers closer to them, which increase performance Support static and dynamic content It also utilizes the AWS edge location Includes security features:  AWS shield for DDoS AWS WAF, Web application firewall    Api gateway  Fully managed API management service. You provide webservices that other app can call and you can distribute them through CLoudFront Provides monitoring and metrics on API calls Support integration with VPC and on-premise private applications  Scenarions  Case 1: If you want your data center to work alongside AWS for specific workload.  Use Amazon direct connect   Case 2: If you want to optimize performance around the world and leverage CDN?  Use Amazon Cloud front     File Storage Services Amazon S3 Main features "
},
{
	"uri": "http://example.org/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "Welcome My name is Percy, and this site has a minimal content about my interests, activities, and notes I recorded for myself or for people with similar interests.\nContact  percyperezdante [at] gmail (dot) com Github Linkedin, Vagrant Cloud Twitter: @percyperezd  Interests My main interests include to develop, experiment and test new technologies related to:\n nMANET Reproducibility Teaching Automation Continuous Integration and Continuous Delivery Site Reliability Engineering DevOps Software Development Enginering Cloud computing Distributed systems Security  NOTE: nMANET is an approach that intends to offer an alternative to solve the limitations of Mobile Ad-hoc Networks in emergency scenarios such as earthquakes and social riots.\nnMANET intends to show whether future Internet architectures such as Name Data based Networking protocol can be applied in Mobila Ad-hoc networks. To achieve this, I am designing, implementing and testing JNFD, which is the prototype of nMANET, and that is functional in Andriod and Linux base mobile devices. The source code of JNFD is open source and it has been tested in smartphones such as MotoE, MotoG, and Samsumg Note4, and also in laptops and through JNFD simulators such as Mini-JNFD.\nActivities   Sports. I like to play football and basketball, but also I often swim. I currenly swiming for the Diabetes UK charity, swim22. We formed a great team of colleges who are willing to swim approximatelly 230 Km all together. As a team we would love to have your contributions if possible of course.\n  Devops days Edinburgh 2020. I am member of the organizers for the 2020 Devops days in Edinburgh. Devops days happen here in Edinburhg once a year and is an international conference that include technical talks about software development, IT infrastructure operations, and a mix of them.\n  Sadly, this event is cancelled due to COVID-19.\nTooling  Jfrog Artifactory  Notes This section contains a set of basic tutorials I wrote for my students. They are simple notes I took from books and online resources. These notes are grouped in the following categories.\n  Minimal infrastructure\n  Software Development\n  Infrastrucuture as a code\n  Devops \u0026amp; SRE\n  "
},
{
	"uri": "http://example.org/devops/aws/cloudpractitioner/fundamentals/",
	"title": "Fundamentals",
	"tags": [],
	"description": "",
	"content": "Main topics:   Creating an account\n  Examining traditional data center challenges:\n Large up-front investment Difficult to predict future traffic or demand Slow deploy of new data centers and servers Maintain data centers is expensive You own all security and compliance burden with all data centers    Benefits of cloud computing\n Trade capital expense for variable expenses: you pay what you use Scaling Stop to guess future needed capacity, traffic Speed and agility Stop spend money for maintenance Go global in minutes. Deploy new data centers in any part of the world Elasticity Realibility: failover, Agility:  Low cost of trying new business ideas Reduces time to maintain infrastructure Reduces risk for orgs aroud security and compliance Provides access to emerging techs      What is cloud computing:\n   It is the on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform via the internet with pat-as-you-go pricing.\n   Types of clouds\n Iaas you can full/maximun control of your infra SaaS you provide a software as a service, you do not worry about infra PaaS: Provide a plataform where you put your code and run it.    Cloud deployment models:\n Public cloud: such as AWS(pay as you go), GCP On-Premises: Private cloud in a private data center Hybrid: Use public and on-prem clouds    AWS Global infrastrucute Three types of infra:\nRegions  It is a specific gographic region Each region has a CLUSTER of data center AWS has 22 launched regions  Availability Zones  It is one or more data centers One region has Multiple avalability zones Minimun: one reqion has minimun two availability zones Each availability zone has at least ONE data center Therefore, one region has at least two data centers Each availability zone is located within the geographical area of the AWS region Availability zones have redundant power, netowrking, and connectivity. Globaly, there are 69 availability zones  What is availability:\n Extent to whcih an application is fulfiling its intended business purpose. Applications that are highly-available are built in a manner where a single failure wont lessen the application\u0026rsquo;s ability to be fully operational\n The first one is availability base on business needs. The second case is 100% availability, high availability\nNaming syntax For example: us-east-2b Where: - us = area - east = sub-area - 2 = Nubmer that identifies the region in that area - b = availabitlity zone\nFrom here: - the REGION NAME = us-east-2 - The availability zone = us-east-2b\nEdge locations  Part of a global content delivery network CDN These support only TWO services:  Amazon CLoudFront: which is AWS CDN Amazon Route 53: which is AWS DNS service There are more than 200 different location with edge locations It is the most predominant and common infra in AWS Edge locations server content from location that are closer to users    In practice   The AWs global infra : https://infrastructure.aws/\n  IN this link edge location = points of presence\n  If we need to store or service in multiple geographical areas/regions, then AWS region is needed\n  If the need is to optimize access to user content around the world, then AWS edge loction is needed.\n  If the need is to ensure availability, then AWS availability zones is needed.\n  Economics Difference between data center and cloud  Capitalized Expenditure (capEx): When building a data center, an organizatio invests in upfront costs for the building , servers, and supporting equipment. This type of expense to attain a fixed asset is referred to as a Capitalized Expenditure or CapEx\n  Operating Expenditure( OPEX): The regular day to day expenses of a business are considered Operating Expenditure or OpEx. After the initial build of a data center, ongiong connectivity, utility, and maintencance costs would be considered OpEx.\n  The problem with data centers is that at the beginning there is a \u0026ldquo;unused capacity \u0026quot; and just before increasing the capacity, there is a \u0026ldquo;demand over capacity\u0026rdquo; that forces to increase the size of the data centere. During the \u0026ldquo;deman over capacity\u0026rdquo; many users can not use our service because our infrastrcuture capacite reach its limits.  Costs in AWS   Use the \u0026ldquo;AWS cost explorer \u0026quot; tool, an interface for:\n Provides breakdown per service and cost tag Provides prediction for next three months of costs GIve recommendation for cost optimization Get access via API    \u0026ldquo;AWS budgets\u0026rdquo;: Utilizes data from AWS cost explorer to plan and track your usage across AWS services. It can track cost per serbice , serbice usage, reserved instance utilzization, and coverage, and Saving Pans utilizion and coverage.\n  AWS TCO calculator: Estimates what could be saved by using cloud.\n  AWS Simply Monthly calculator: Estaimtes costs of running specific AWS infra.\n  AWS resource tags:\n Tags are metadata attached to specific AWS resources. For example: webserver tag include all webservers. Common uses cases include department, env, or project. Tag include a name and a valur Costa allocation reports: includes costs grouped by tags You can use tags in the AWS explorer    AWS organization:\n Allows you to manage many account under one master acount It provides a consolidate billing for those account in the organization. Enables organizations to centralize logging and security standards across accounts.    AWS TCO calculator   TCO = Total cost ownership\n  Compares running your workload in the data center versus in AWS\n  WIth TCO you can:\n Estamate costs for a org to move to the cloud Download a summary report    Location: awstcocalculator.com\n  AWS Simple Monthly Calculator  Estimates the cost of running your workload in AWS, with no comparison Link: calculator.s3.amazonaws.com  Cost explorer   Login to \u0026ldquo;console.aws.amazon.com\u0026rdquo; -\u0026gt; Click in your name -\u0026gt; click \u0026quot; My billing dashboard\u0026rdquo;. Then Click on \u0026ldquo;cost explorer\u0026rdquo; and then \u0026ldquo;Launch cost explorer\u0026rdquo;\n  The cost explorer initially shows: current month cost and forecasted month end cost.\n  To get more details in the dashboard, click \u0026ldquo;Explore costs\u0026rdquo;\n  You can download CSV file\n  Scenarios   Case 1: company with many departments. Finance is asking to hav a separate costs between departments. Currently all resources are included within a single AWS account.\n IN this case we can use TAGS. Create a tag per department.    Case 2: Company think to move to the cloud, but they want to know if this will save them money:\n The solution is use TCO calculator and provide the report to the stakeholders    Case 3: Company is trying to know saving costs if they move a web service to the cloud.\n Use Simple MOnthly calculator    Suport AWS infra  Supporting tools:   AWS suuport\n Enables support for AWS resources for workloags running in the cloud Provided in different tiers based on need and scope Include tools to provide automated answers and recommendations    AWS personal health dashboard\n Provides alerts and remediation guidance when AWS is experiencing evnts that may impact you. For example,when there will be a outage in your region.\n   AWS tursted advisor\n Check your AWS usage against best practices Accessed from AWS console Different checks are provided base on the AWS support plan tier All AWS customer get access to seven core checks, which include:  Cost Optimization Performance Security Fault tolerance Service Limits        AWS Support plan tiers differences: Main four plans difference includes:\n Communication method Response time Cost Type of guidance offered  Main plans:\n  Basic support:\n To all customers Access to trusted advisor ( 7 core checks) 24x7 accesss to customer service: docs, forum, and white papers. No engineers Access to AWS personal health dashboard No monthly cost    Developer support:\n Include all from the basic support Email access support engineers for business Limited to 1 primary contact Start at $29 per month, but tied to AWS usage    Business support:\n Include all from developer support Full set of Trusted Advisor checks 24x7 phone, email and chat access support engineers Unlimited contacts PRovided thrid party software support Starts at $100 per month, tied to AWS usage    Enterprise support:\n Includes all from business support Dedicate technical account manager Include a concierge support team start from 15000$ per month, tied to AWS usage    There is a importat part called SUPPORT RESPONSE TIME\nIn here we have the following categories:\nGeneral guidance = General question to be answer System impaired = when somethng is not working as it should Production system impaired = Production system that is not performing at its desired capacity. Production system down = Production system that is completely un-functinal Business-critical system down = Core system for an organization that is completly non functional\nTrusted advisor tool  LInk: consoel.aws.amazon.com  Review recommendations Type \u0026ldquo;trsuted advisor\u0026rdquo; in \u0026ldquo;find SERvices2    Personal health dashboard Infra support scenarios  case 1: One workload is cirtical and requirest 24 hours call support a day.  The most effective criteria is Business support   case 2: The company needs to be able to call , text and email support if an issue occurs. Also needs response from support within 15 minutes.  Here applies: Enterprise support   Case 3: a developer does not need technical support, and he has access to trusted advisor core checks.  Here Basic support is enough    Review "
},
{
	"uri": "http://example.org/notes/restful/",
	"title": "Restful",
	"tags": [],
	"description": "",
	"content": "Design tips  URI format:  URI = scheme \u0026quot;://\u0026quot; authority \u0026quot;/\u0026quot; path [ \u0026quot;?\u0026quot; query] [\u0026quot;#\u0026quot; fragment\u0026quot;] URI = http://myserver.com/mypath?query=1#document   General rules for URI formating:\n A forward slash / is used to indicate a hierarchical relationship between resources A trailing forward slash / should not be included in URIs Hyphens - should be used to improve readability Underscores _ should not be used in URIs Lowercase lette    URI path design\n  Collections: Always plural noun\nGET /cats -\u0026gt; All cats in the collection GET /cats/1 -\u0026gt; Single document for a cat 1   Documents: Points to a single object.\nGET /cats/1 -\u0026gt; Single document for cat 1 GET /cats/1/kittens -\u0026gt; All kittens belonging to cat 1 GET /cats/1/kittens/1 -\u0026gt; Kitten 1 for cat 1   Controller: Used to map non standard CRUD\nPOST /cats/1/feed -\u0026gt; Feed cat 1 POST /cats/1/feed?food=fish -\u0026gt;Feed cat 1 a fish   Store: A source repository.\nPUT /cats/2     References  RFC 7231: Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content RFC 2616: Header Field Definitions Microsoft REST API guidelines OpenAPI specifications adopted by industry for documentation Swagger using YAML API Blueprint using Markdown RAML Restful API Modeling Language using YAML Calico project for container base networking  "
},
{
	"uri": "http://example.org/sre/mypostmortem/",
	"title": "Mypostmortem",
	"tags": [],
	"description": "",
	"content": "Random notes  Expose your code for one day to everyone. Ask people to execute disaster revocery of everything once or twice a year. blameless I do not think is a way to avoid future mistakes, I think it is a way to save time and effort, thefore reduce costs. We should test our selves at least once a year. to see how if we are ready or notfor disasters Postmorten of the month? Best postmorten of this motnh Postmorten award 2020? why not have one Postmorten day? Postmorten should be rewarded  "
},
{
	"uri": "http://example.org/notes/jfrog/",
	"title": "Jfrog",
	"tags": [],
	"description": "",
	"content": "1. How to install a local Jfrog (open source) The following shows how to install a local Jfrog (OSS) in a Linux-base host.\n Open https://jfrog.com/open-source/#artifactory Select the OS you are working with: Linux, Windows or Mac and click on \u0026ldquo;Click here to Download\u0026rdquo;  In our case we could directly download the tar file for Linux:\n$ mkdir myJfrog $ cd myJfrog $ wget https://api.bintray.com/content/jfrog/artifactory/org/artifactory/oss/jfrog-artifactory-oss/$latest/jfrog-artifactory-oss-$latest-linux.tar.gz?bt_package=jfrog-artifactory-oss $ ls -tlr total 811768 -rw-rw-r-- 1 percy percy 831242240 Aug 26 17:28 jfrog-artifactory-oss-7.7.3-linux.tar $ tar -zxvf jfrog-artifactory-oss-7.7.3-linux.tar $ cd artifactory-oss-7.7.3/app/bin $ ls -trl total 556 -rwxr-xr-x 1 percy percy 29279 Jul 13 13:17 systemYamlHelper.sh -rwxr-xr-x 1 percy percy 96527 Jul 13 13:17 systemDiagnostics.sh -rwxr-xr-x 1 percy percy 171933 Jul 13 13:17 migrate.sh -rwxr-xr-x 1 percy percy 143122 Jul 13 13:17 installerCommon.sh -rwxr-xr-x 1 percy percy 6879 Aug 12 10:55 uninstallService.sh -rwxr-xr-x 1 percy percy 4387 Aug 12 10:55 migrationZipInfo.yaml -rwxr-xr-x 1 percy percy 6539 Aug 12 10:55 migrationRpmInfo.yaml -rwxr-xr-x 1 percy percy 4073 Aug 12 10:55 migrationDockerInfo.yaml -rwxr-xr-x 1 percy percy 4283 Aug 12 10:55 migrationComposeInfo.yaml -rwxr-xr-x 1 percy percy 10606 Aug 12 10:55 installService.sh -rwxr-xr-x 1 percy percy 17401 Aug 12 10:55 artifactory.sh -rwxr-xr-x 1 percy percy 9365 Aug 12 10:55 artifactoryManage.sh -rwxr-xr-x 1 percy percy 1553 Aug 12 10:55 artifactory.default -rwxr-xr-x 1 percy percy 382 Aug 12 10:55 artifactoryctl -rwxr-xr-x 1 percy percy 32294 Aug 12 10:55 artifactoryCommon.sh $ ./artifactory.sh  Open in a browser: http://localhost:8082 To login you can use the default admin account: admin/password.  "
},
{
	"uri": "http://example.org/sre/review/",
	"title": "Review",
	"tags": [],
	"description": "",
	"content": "This section contains reviews or summaries from exisitng literature.\n1. SQL is no excuse to avoid devops @article{limoncelli2018sql, title={SQL is no excuse to avoid DevOps}, author={Limoncelli, Thomas A}, journal={Communications of the ACM}, volume={62}, number={1}, pages={46--49}, year={2018}, publisher={ACM New York, NY, USA} } Automation and a little discipline allow better testing, shorter release cycles, and reduced business risk.\nPrevious articles:\n “The Small Batches Principle,”Communications, July 2016), when something is risky there is a natural inclination to seek to do it less. McHenry Technique in The Practice of Cloud System Administration, Strata R. Chalup and Christina J. Hogan.  It is also called Expand/Contract in Release It!: Design and Deploy Production-Ready Software by Michael T. Nygard.   SQL migration tools:  https://github.com/bretcope/Mayflower.NET https://bitbucket.org/liamstask/goose/src/master/    Reliability: What is it, and how is it measured? @article{bruton2000reliability, title={Reliability: what is it, and how is it measured?}, author={Bruton, Anne and Conway, Joy H and Holgate, Stephen T}, journal={Physiotherapy}, volume={86}, number={2}, pages={94--99}, year={2000}, publisher={Elsevier} } how to quantify relibilityhow to measue how to seect mesure metrics how ti collect metrics how to validate the methodology of collecting metrics how to classify them how to aggregate them how to quantify errors errors in cpollectin errors in analysis errors agrregation errors per cases how to store them how to analize them how to filter them how to archive them how to validate them how to reuse them hoe to reproduce them how to transfort them how long this measuremens and realibility is valid how define good realibility? what is good? how important is reproucibility\nBaumgarter (1989) has identified two types of reliability, ie relative reliability and absolute reliability.\n3. Nines are not enough: meaningful metrics for clouds @inproceedings{mogul2019nines, title={Nines are not enough: meaningful metrics for clouds}, author={Mogul, Jeffrey C and Wilkes, John}, booktitle={Proceedings of the Workshop on Hot Topics in Operating Systems}, pages={136--141}, year={2019} }   This paper analyse: Why SLO are so hard to define?\n  However, SLOs are not free. We have to spend resources collecting and processing the data that allows us to com- pute an SLO’s predicate, without significantly interfering with “real\u0026rdquo; work – and without compromising security or privacy of cloud customers and their own users. We often must aggregate data to reduce costs, which loses fidelity. (These costs are what we mean by the “feasibility\u0026rdquo; of a set of SLOs.\n  how to collect sufficient data without bias,\n  When to triggerd the decision to claim an SLA?\n  How much data you need to collect?\n  How expensive it is?\n  we have uncertain knowledge of future workloads, and we also are unable to accurately model the performance of complex computer systems even if we did know the workloads.\n  By monitoring and collecting data, we can find the time when of SLA violation, which can lead to a financial penalty in benefit of the customer, if the SLA says it. SLA\u0026rsquo;s are very important from business and law point of view.\n  This paper present many interesting open questions.\n  4. Thinking about availabitliy in large service infrastrcuture. @inproceedings{mogul2017thinking, title={Thinking about availability in large service infrastructures}, author={Mogul, Jeffrey C and Isaacs, Rebecca and Welch, Brent}, booktitle={Proceedings of the 16th Workshop on Hot Topics in Operating Systems}, pages={12--17}, year={2017} } 5. Auditing to keep online storage services honest. @inproceedings{shah2007auditing, title={Auditing to Keep Online Storage Services Honest.}, author={Shah, Mehul A and Baker, Mary and Mogul, Jeffrey C and Swaminathan, Ram and others}, booktitle={HotOS}, year={2007} } 6. Sre tools as product This document, not paper, gives a general lines about SRE. One important message: reduce MTTR and toil.\n7. Debugging Incidents in Google\u0026rsquo;s Distributed Systems. @article{chan2020debugging, title={Debugging Incidents in Google's Distributed Systems}, author={Chan, Charisma and Cooper, Beth}, journal={Queue}, volume={18}, number={2}, pages={47--66}, year={2020}, publisher={ACM New York, NY, USA} } It introduces a general overview of SRE and present postmorten use cases.\n8. Generic mitigation Jennifer Mace: Spotlight on Cloud: Reducing the Impact of Service Outages with Generic Mitigations with Jennifer Mace https://www.oreilly.com/library/view/spotlight-on-cloud/0636920347927/\n"
},
{
	"uri": "http://example.org/sre/myapproach/",
	"title": "Myapproach",
	"tags": [],
	"description": "",
	"content": "What is SRE  SRE measures how reliable my serivice is from my customer\u0026rsquo;s perspective, not from my perspective. It seems most of the talks use current or historic data, logs, Why we do not estimate future data then ?  The reason is that usually you take generated logs, so it is hard to have a real-time observations. How to have real-time observations? how to estimate the future?    SLO  Defines a threshold, max or min value, for an SLI. Defines what are your service expectations. Examples:  Uptime 999% i.e. SLI-1 over the last month 99.95 of the time. 99% of valid requests in the past 28 days served in less than one second.    SLI   Includes the most relevant metrics to measure service of your system.\n  If you see historical data, you can not take overall view, instead every slot should be analysed. For example, if we have a pick in a specific day, maybe the traffic was high at that time and that is where we need to guarantee reliability. This should be consider even if it occurs only once.\n  Examples:\n One a minute, ssh into the target host, report 1 if it\u0026rsquo;s working 0 if not. The proportion of valir requests that were serverd within less than one second.     How percentiles should be treated? can we aggregate along the time? When can be aggreagted when not?\n  Can we averga percentiles of many modules and for a long time of data analysis?\n  If we do not use logs, what will be the benefit to counts events as soon as they happen, and evaluate SLI base on this fresh results?\n  Metrics have two faces:\n Enough to evaluate the customer perception. We can not use so many metrics and overflow it. Metrics should be enough to do a post mortem analysis and document. Postmortem analysis includes analysis of historic data logs, and also includes to collect metrics or logs in detail that can allow us to find the root of cause without the need to implement new metrics and wait for the second time the error to happen.    SLA  Examples:  If we do not meet our SLO in one month, you will get exactly one cake In the case of JRI: I will pay 10£ to the member of the team that points a failed SLO    Alerts   Set an alert ONLY when a manual internvention is required, not for infomative purposes. Alerts mean action and now. If this is not possible, you can separate alerts that requries action and alerts where no action is required.\n  Monitoring should be display in one place.\n   Customer perception  Collect data from customers, slack, twitter, emails, open public conversations and centralise them. Apply ML to analyse them and get insights for understand whether customer are happy or not with services.  How to build SLO How to maintain SLO  Refer to https://www.usenix.org/conference/srecon19emea/presentation/desai talk  NOTES\n  Get differencd between service satisfaction and content satisfaction.\n  how error budget leads with customer trust: i believe customers should be able to set our error budget, not us\n  Also postmortem should be quick and not generate historic toil.\n  "
},
{
	"uri": "http://example.org/sre/online/srecon2019/",
	"title": "SRECON 2019",
	"tags": [],
	"description": "",
	"content": "1. The SRE I Aspire to be by Yaniv Aknin at SRECON 2019  Yaniv A. is GCP quantitative reliability lead. SRE skill is the ability to measure or translate to numbers ( quantify ) SRE includes: \u0026ldquo;measurably optimse reliability vs cost\u0026rdquo; It is important to trade Operations and Reliability How can we build a more reliable logical disk? then we do not need to increase operations due to failure. One solution as use RAIDs, but cost more and more complexity! SRE is trading cost and reliability Remove alerts, reduce operations Fundamental: Monitoring, Alerting, Capacity planning, CI/CD \u0026amp; rollouts, and Load Balancing. Advanced: System Architecture, Distributed Algorithms, Networking, Operating systems. Pioneering: Product management, data science, business Acumen, UX research. Measurements are tied to the project priorities and operations are tied to these measurements.  2. SLOs for Data intensive Services by Yoann Fouquet Booking.com Youtube  Example for SLI/SLO: SLI = Quantitative measure: availability,\nSLO = SLI + target|threshold: availability (SLI) for 1 week over 99.99% SLO Booking.com: Availability, latency, and reservation success rate.\n` Missing SLOs: durability, freshness, accuracy, completeness, consistency. - \u0026ldquo;\u0026hellip;99.99% of search results are consistent \u0026hellip;\u0026rdquo; - \u0026ldquo;\u0026hellip;99.9% of reservations are available within in 10 seconds\u0026hellip;\u0026rdquo; - Booking.com group queries by latency. Benefits: mitagation for free, My opinion these SLOs/SLIs does not reflect the customer perception  3. Latency SLos Done right by Heinrich Hartman, Circonus   Presentation in here\n  What is latency?\n Key perfomance indicator for API. Latency is number one golden signal. Besides latency, another golden signals, indicators, include traffic, errors and saturation. This later became the RED method: Request, Errors, Durantion.    What is an SLI?\n A metric that quantifies the quality or reliability of your service.    What is an SLA?\n What happen if a published SLO is not met?    What is an SLO?\n A target value for the service level, as measured by SLI, set of expectations about how the service will perform. Percentile metrics need to be compute percentiles over:  Multiple weeks of data, better long time periods Multiple nodes ( potentially )   Percentiles can not be aggregated.    Methods to stimate SLOs:\n  Considering the task: \u0026ldquo;count all request over $period served faster than $threshold\u0026rdquo;\n  Methods:\n  log data:\n Execute a query in your stored logs You need to keep your logs for long times, which could be expensive ( you could use sampling ) You can do this with ssh+awk, ELK, Splunk, Honeycomb,etc    counter metrics\n It stores and uses counters of how often an event occurs: for example, \u0026quot; how many requests were faster than 2 ms\u0026rdquo;, so you can store this as short prometheus metric: \u0026ldquo;aws.www33get./latency_2ms\u0026rdquo;. Later you can sum/integrate these metrics across nodes, endpoint and time. This approach does not required store logs, instead store counters at specific time. Tools: prometheus histogram, graphite, datadog, vividcontext.    HDR historgram metrics\n  See below figure\n  WIth histograms you can aggregate latency histrograms over nodes, endpoints and time. Get total latency distribution over SLO timeframe ( weeks, months)\n  Count samples in bins below the thresholds to compute SLOs. You can sum or agregate two similar bins to get a final overll number.\n  Full flexibility in choosing thresholds, and aggregation intervals and levels\n  Cost effective ( 300b/histogram value).\n  Need HDR histogram instruments and metrics store\n  Tools: CIrconus,IronDB + graphite / Grafana\n        Conclusions:\n Percentiles metrics are not suitable for implementing latency SLOs Histogram metrics allow you to easily calculcate arbitraty latency SLOs If you do not have histrogram metrics:  Use historic recent logs Ad counter metrics for the threshold Aggregate couter metrics as needed        4. Evolution of Observability Tools at Pinterest by Naoman Abbas 5. How to SRE When Everything\u0026rsquo;s Already on Fire by Alex Hidalgo and Alex Lee, Squarespace 6. Advanced Napkin Math: Estimating System Performance from First Principles by Simon Eskildsen, Shopify 7. The Map Is Not the Territory: How SLOs Lead Us Astray, and What We Can Do about It by Narayan Desai, Google "
},
{
	"uri": "http://example.org/notes/crypto/",
	"title": "Crypto",
	"tags": [],
	"description": "",
	"content": "General  DES; The Data Encryption Standard uses a key of 56bits and a block size of 64 buts. It is not recommended, and insted you can consider other approaches such as AES, Salsa20.:w   "
},
{
	"uri": "http://example.org/devops/gcp/networking/",
	"title": "Networking",
	"tags": [],
	"description": "",
	"content": "Insert MAP\nVPC Virtual Private Cloud INSERT VPC\n Projects   Key organisers in VPC, and it is linked to billing Projects contains entire networks Default quota is 5 networks for one project.   Networks   No IP address range Networks are global Contain subnwtowrks Type: default, auto or custom. Custom networks can not be converted to auto mode networks  ipaddress\n"
},
{
	"uri": "http://example.org/notes/shell/",
	"title": "Shell",
	"tags": [],
	"description": "",
	"content": "1. Sending message to pts terminal # List of opened terminals. percy@prec:~$ ls -tlr /dev/pts/ total 0 c--------- 1 root root 5, 2 May 21 06:18 ptmx crw--w---- 1 percy tty 136, 2 May 21 11:36 2 crw--w---- 1 percy tty 136, 0 May 21 11:52 0 crw--w---- 1 percy tty 136, 3 May 21 11:53 3 crw--w---- 1 percy tty 136, 4 May 21 11:53 4 crw--w---- 1 percy tty 136, 6 May 21 11:53 6 crw--w---- 1 percy tty 136, 5 May 21 11:53 5 percy@prec:~$ echo \u0026#34;How are you ?\u0026#34; \u0026gt; /dev/pts/6 You can use \u0026ldquo;ps\u0026rdquo; to identify each pts terminal.\n"
},
{
	"uri": "http://example.org/notes/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Reference The following notes were extracted, adjusted or extended from the following references.\n Kubernetes book by Nigel Poulton  General   The cluster is made up of one or more masters, and a bunch of nodes.\n  Package and deploy a Kubernetes application is done via a Deployment. With Deployments, we start out with our application code and we containerize it. Then we define it as a Deployment via a YAML or JSON manifest file. This manifest file tells Kubernetes two important features:\n What our app should look like – what images to use, ports to expose, networks to join, how to perform update etc. How many replicas of each part of the app to run (scale)  Then we give the file to the Kubernetes master which takes care of deploying it n the cluste\n  The API server\n  The API Server (apiserver) is the frontend into the Kubernetes control plane. It exposes a RESTful API that preferentially consumes JSON. We POST manifest files to it, these get validated, and the work they define gets deployed to the cluster.\n The cluster store  The config and state of the cluster gets persistently stored in the cluster store, which is the only stateful component of the cluster and is vital to its operation.The cluster store is based on etcd, the popular distributed, consistent and watchable key-value store. As it is the single source of truth for the cluster, you should take care to protect it and provide adequate ways to recover it if things go wrong.\n The controller manager  They tend to sit in loops and watch for changes, the aim is to make sure the current state of the cluster matches the desired state.\n The scheduler  Watches and executes new workloads.\n"
},
{
	"uri": "http://example.org/notes/go/",
	"title": "Go",
	"tags": [],
	"description": "",
	"content": "References The following notes were extracted, adjusted or extended from the following references.\n Go mastering by Mihalis Tsoukalos  General 1. Go inserts only a semicolon at the end of a \u0026ldquo;{\u0026quot;\nfunc main() { // \u0026lt;-- this will trigger error .... } func main(){ // \u0026lt;-- this will NOT trigger error .... } 2. Install and clean packages\n$ go get -v github.com/mastsoud/go/package_name ..... $ go clean -i -v -x package_name $ rm -rf ~/go/src/github.com/mastsoud/go/package_name 3. Stdin/out/err\n   Go Unix     os.Stdin stdin \u0026ndash;\u0026gt; /dev/stdin \u0026ndash;\u0026gt; /proc/self/fd/0   os.Stdout stdout \u0026ndash;\u0026gt; /dev/stdout \u0026ndash;\u0026gt; /proc/self/fd/1   os.Stderr stderr \u0026ndash;\u0026gt; /dev/stderr \u0026ndash;\u0026gt; /proc/self/fd/2    4. Reading input\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main(){ var f *os.File f = os.Stdin defer f.Close() scanner := bufio.NewScanner(f) for scanner.Scan() { fmt.Println(\u0026#34;\u0026gt;\u0026#34;, scanner.Text()) } } 5. Logs\n rsyslogd configuration  $ grep -v \u0026#34;#\u0026#34; /etc/rsyslog.conf  Syslog  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;log/syslog\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) func main() { programName := filepath.Base(os.Args[0]) sysLog, err := syslog.New(syslog.LOG_INFO|syslog.LOG_LOCAL7,programName) if err != nil { log.Fatal(err) } else { log.SetOutput(sysLog) } log.Println(\u0026#34;LOG_INFO + LOG_LOCAL7: Logging in Go!\u0026#34;) sysLog, err = syslog.New(syslog.LOG_MAIL, \u0026#34;Some program!\u0026#34;) if err != nil { log.Fatal(err) } else { log.SetOutput(sysLog) } log.Println(\u0026#34;LOG_MAIL: Logging in Go!\u0026#34;) fmt.Println(\u0026#34;Will you see this?\u0026#34;) } 6. Errors\n Error types  package main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) func returnError(a, b int) error { if a == b { err := errors.New(\u0026#34;Error in returnError() function!\u0026#34;) return err } else { return nil } } func main() { err := returnError(1, 2) if err == nil { fmt.Println(\u0026#34;returnError() ended normally!\u0026#34;) fmt.Println(err) } err = returnError(10, 10) if err == nil { fmt.Println(\u0026#34;returnError() ended normally!\u0026#34;) } else { fmt.Println(err) } if err.Error() == \u0026#34;Error in returnError() function!\u0026#34; { fmt.Println(\u0026#34;!!\u0026#34;) } }  Typical handling of errors  if err != nil { fmt.Println(err) or log.Println(err) or panic(err) os.Exit(10) }  Example  package main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { if len(os.Args) == 1 { fmt.Println(\u0026#34;Please give one or more floats.\u0026#34;) os.Exit(1) } arguments := os.Args var err error = errors.New(\u0026#34;An error\u0026#34;) k := 1 var n float64 for err != nil { if k \u0026gt;= len(arguments) { fmt.Println(\u0026#34;None of the arguments is a float!\u0026#34;) return } n, err = strconv.ParseFloat(arguments[k], 64) k++ } min, max := n, n for i := 2; i \u0026lt; len(arguments); i++ { n, err := strconv.ParseFloat(arguments[i], 64) if err == nil { if n \u0026lt; min { min = n } if n \u0026gt; max { max = n } } } fmt.Println(\u0026#34;Min:\u0026#34;, min) fmt.Println(\u0026#34;Max:\u0026#34;, max) } 7. Using docker\n Dockerfile  FROM golang:alpine RUN mkdir /files COPY hw.go /files WORKDIR /file RUN go build -o /files/hw hw.go ENTRYPOINT [\u0026#34;/files/hw\u0026#34;] $ docker build -t go_hw:v1 . $ docker run go_hw:v1 Go internals 1. Go compiler\n Compiling source file and generate Object code.  $ go tool compile sourceFile.go $ ls -ltr sourceFile.o # This is not executable.  Genereate an object file instaed of object code  $ go tool compile -pack sourceFile.go $ ls -ltr sourceFile.a  The following will list the content of *.a file.  $ ar t sourceFile.a __.PKGDEF _go_.o  To detect trace conditions  $ go tool compile -race sourceFile.a  Showing assembly code  $ go tool compile -S sourceFile.go os.(*File).close STEXT dupok nosplit size=26 args=0x18 locals=0x0 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tTEXT\tos.(*File).close(SB), DUPOK|NOSPLIT|ABIInternal, $0-24 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$0, $-2 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$1, $-2 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tFUNCDATA\t$0, gclocals·e6397a44f8e1b6e77d0f200b4fba5269(SB) 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tFUNCDATA\t$1, gclocals·69c1753bd5f81501d95132d08af04464(SB) 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tFUNCDATA\t$2, gclocals·9fb7f0986f647f17cb53dda1484e0f7a(SB) 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$0, $1 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$1, $1 0x0000 00000 (\u0026lt;autogenerated\u0026gt;:1)\tMOVQ\t\u0026#34;\u0026#34;..this+8(SP), AX 0x0005 00005 (\u0026lt;autogenerated\u0026gt;:1)\tMOVQ\t(AX), AX 0x0008 00008 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$0, $0 0x0008 00008 (\u0026lt;autogenerated\u0026gt;:1)\tPCDATA\t$1, $0 0x0008 00008 (\u0026lt;autogenerated\u0026gt;:1)\tMOVQ\tAX, \u0026#34;\u0026#34;..this+8(SP) 0x000d 00013 (\u0026lt;autogenerated\u0026gt;:1)\tXORPS\tX0, X0 ... 2. Garbage collector\n Example of GC  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func printStats(mem runtime.MemStats) { runtime.ReadMemStats(\u0026amp;mem) fmt.Println(\u0026#34;mem.Alloc:\u0026#34;, mem.Alloc) fmt.Println(\u0026#34;mem.TotalAlloc:\u0026#34;, mem.TotalAlloc) fmt.Println(\u0026#34;mem.HeapAlloc:\u0026#34;, mem.HeapAlloc) fmt.Println(\u0026#34;mem.NumGC:\u0026#34;, mem.NumGC) fmt.Println(\u0026#34;-----\u0026#34;) } func main() { var mem runtime.MemStats printStats(mem) for i := 0; i \u0026lt; 10; i++ { s := make([]byte, 50000000) // memory allocation if s == nil { fmt.Println(\u0026#34;Operation failed!\u0026#34;) } } printStats(mem) for i := 0; i \u0026lt; 10; i++ { s := make([]byte, 100000000) // MORE memory allocation if s == nil { fmt.Println(\u0026#34;Operation failed!\u0026#34;) } time.Sleep(5 * time.Second) } printStats(mem) } $ go run gColl.go  To get MORE information  $ GODEBUG=gctrace=1 go run gColl.go gc 1 @0.027s 0%: 0.048+0.72+0.007 ms clock, 0.38+0.18/0.62/1.1+0.057 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 2 @0.047s 0%: 0.006+0.56+0.007 ms clock, 0.055+0.39/0.69/0.79+0.061 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 3 @0.060s 1%: 0.16+1.0+0.011 ms clock, 1.2+1.3/1.1/0.13+0.093 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 4 @0.069s 1%: 0.016+0.54+0.016 ms clock, 0.13+0.16/0.61/1.0+0.12 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 5 @0.073s 1%: 0.002+0.37+0.003 ms clock, 0.023+0/0.37/1.0+0.028 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 6 @0.076s 1%: 0.016+0.41+0.011 ms clock, 0.13+0.14/0.60/0.40+0.088 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 7 @0.079s 1%: 0.002+0.34+0.010 ms clock, 0.020+0.14/0.36/0.82+0.081 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P # command-line-arguments gc 1 @0.001s 10%: 0.002+1.3+0.011 ms clock, 0.018+0.82/1.4/2.0+0.092 ms cpu, 5-\u0026gt;6-\u0026gt;6 MB, 6 MB goal, 8 P gc 2 @0.010s 7%: 0.004+3.4+0.011 ms clock, 0.036+0.42/5.3/3.8+0.090 ms cpu, 13-\u0026gt;14-\u0026gt;13 MB, 14 MB goal, 8 P gc 3 @0.044s 4%: 0.017+4.8+0.011 ms clock, 0.13+0.18/8.2/18+0.093 ms cpu, 25-\u0026gt;25-\u0026gt;23 MB, 26 MB goal, 8 P mem.Alloc: 125896 Taking the example of \u0026ldquo;4-\u0026gt;4-\u0026gt;0\u0026rdquo;:\n - The first number is the heap size when the garbage collector is about to run. - The second value is the heap size when the garbage collector ends its operation. - The last value is the size of the live heap  3. GC internals\nGC in slices\nGC in maps\nGC in maps without pointers\nGC in spliting maps\nGC comparisons`\nGC unsafe code\nGC unsafe package\nDefer keyword\nDefer keyword using logging\nPanic\nRecover\nStrace\ndtrace\nGo environment\nNode trees\nGo build\nWebAssembly code\nData types 1. Slices\n  Slices are passed by reference to functions.\n  Slices are often used, more than arrays.\n  mySlcie := []int{1,23,4} mySlice := make([]int, 20) // Go initilise with default values mySlice = append(mySlice, 2134) len(mySlice) fmt.Println(mySlice[1:3])  Re-slicing may cause some problems.  Reslice do not copy values, reslice keeps reference from the orignal slice. Therefore, if you make any change of values in the reslice, they will also change values in the original slice.\npackage main import \u0026#34;fmt\u0026#34; func main() { s1 := make([]int, 5) reSlice := s1[1:3] // Reslice not copy from s1, it make reference fmt.Println(s1) fmt.Println(reSlice) reSlice[0] = -100 // This means also s[1]=-100 reSlice[1] = 123456 // This also means s[2]=123456 fmt.Println(s1) fmt.Println(reSlice) } Output\n$ go run reslice.go [0 0 0 0 0] [0 0] [0 -100 123456 0 0] [-100 123456]   If the length and the capacity of a slice have the same values and you try to add another element to the slice, the capacity of the slice will be doubled whereas its length will be increased by one.\n  Byte slices\n  s := make([]byte,5) **2. Copy slices** - Becareful using copy(destination, source), as copy() copies the minimum number of len(dst) and len(src) elements **3. Sort slice** **4. Appending arrays to slices** **5. Maps** - Declaration ```bash iMap = make(map[string]int) delete(mapName, Key) for key, value := range iMap { fmt.Println(key, value) } The bad thing is that if you try to get the value of a map key that does not exist in the map, you will end up getting zero, which gives you no way of determining whether the result was zero because the key you requested was not there or because the element with the corresponding key actually had a zero value. This is why we have _, ok in maps.\n_, ok := iMap[\u0026#34;doesItExist\u0026#34;] if ok { fmt.Println(\u0026#34;Exists!\u0026#34;) } else { fmt.Println(\u0026#34;Does NOT exist\u0026#34;) } Please note that you cannot and should not make any assumptions about the order the map pairs are going to be displayed on your screen because that order is totally random.\nThe next Go code will not work because you have assigned the nil value to the map you are trying to use:\naMap := map[string]int{} // var aMap map[string]int aMap = nil fmt.Println(aMap) aMap[\u0026#34;test\u0026#34;] = 1 Saving the preceding code to failMap.go and trying to compile it will generate the next error message:\n$ go run failMap.go map[] panic: assignment to entry in nil map This means that trying to insert data to a nil map will fail. However, looking up, deleting, finding the length, and using range loops on nil maps will not crash your code.\n6. Constants\n7. Pointers\nWhen working with pointers, you need * to get the value of a pointer, which is called dereferencing the pointer, and \u0026amp; to get the memory address of a non-pointer variable\npackage main import ( \u0026#34;fmt\u0026#34; ) func getPointer(n *int) { *n = *n * *n fmt.Println(\u0026amp;n) } func returnPointer(n int) *int { v := n * n return \u0026amp;v } func main(){ n:=3 getPointer(\u0026amp;n) fmt.Println(n) k := returnPointer(12) fmt.Println(*k) fmt.Println(k) } Pointers allow you to share data, especially between Go functions. Pointers can be extremely useful when you want to differentiate between a zero value and a value that is not set\nTime\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main(){ fmt.Println(time.Now()) time.Sleep(time.Second*2) t:=time.Now() fmt.Println(t.Day(),t.Month(), t.Year()) t2:=time.Now() fmt.Println(t2.Sub(t)) fmt.Println(\u0026#34;time units\u0026#34;, time.Nanosecond , time.Microsecond , time.Millisecond , time.Minute , time.Hour) }  Parsing date +time from a string:     CODE Meaning     2006 Year   Jan Month   02 Day   15 Hour   04 Minute   05 Second    package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main(){ test:=\u0026#34;20201025 15:20:33\u0026#34; d,_:=time.Parse(\u0026#34;20060102 15:04:05\u0026#34;,test) fmt.Println(d.Hour(),d.Minute(),d.Second()) fmt.Println(d.Year(),d.Month(),d.Day()) }  Measure time execution  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main(){ start:=time.Now() time.Sleep(time.Second) fmt.Println(time.Since(start)) } Composite Types 1. Struct\npackage main import ( \u0026#34;fmt\u0026#34; ) type XY struct{ x int y int } func returnPointer(x,y int) *XY { x++ y++ return \u0026amp;XY{x,y} } func returnStruct(x,y int) XY { x-- y-- return XY{x,y} } func main(){ s1:=returnPointer(3,4) s2:=returnStruct(3,4) fmt.Println((*s1).x) // You need a pointer reference in this case fmt.Println(s2.x) } 2. Tuples\npackage main import ( \u0026#34;fmt\u0026#34; ) func retThree(x int) (int, int, int) { return 2 * x, x * x, -x } func main() { fmt.Println(retThree(10)) n1, n2, n3 := retThree(20) fmt.Println(n1, n2, n3) n1, n2, n3 = n3,n2,n1 //swap fmt.Println(n1, n2, n3) } **3. Json\u0026#34;\u0026#34; **4. XML\u0026#34;\u0026#34; - ### Data Structures ### Functions **1. Return values of function ```bash func namedMinMax(x, y int) (min, max int) { if x \u0026gt; y { min = y max = x } else { min = x max = y } return } Note that the return as this function has named return values in its signature, the min and max parameters are automatically returned in the order in which they were put into the function definition.\n2. FUcntions with pointer parameters\nfunc getPtr(v *float64) float64 { return *v * *v } func main() { x := 12.2 fmt.Println(getPtr(\u0026amp;x)) } 3. Functions that return pointers\nfunc returnPtr(x int) *int { y := x * x return \u0026amp;y } func main() { sq := returnPtr(10) fmt.Println(\u0026#34;sq value:\u0026#34;, *sq) fmt.Println(\u0026#34;sq memory address:\u0026#34;, sq) } 4. Functions that return other functions\nfunc funReturnFun() func() int { i := 0 return func() int { i++ return i * i } } func main() { i := funReturnFun() j := funReturnFun() mt.Println(\u0026#34;1:\u0026#34;, i()) fmt.Println(\u0026#34;2:\u0026#34;, i()) fmt.Println(\u0026#34;j1:\u0026#34;, j()) fmt.Println(\u0026#34;j2:\u0026#34;, j()) fmt.Println(\u0026#34;3:\u0026#34;, i()) } Executing returnFunction.go will produce the following output:\n$ go run returnFunction.go 1: 1 2: 4 j1: 1 j2: 4 3: 9 As you can see from the output of returnFunction.go , the value of i in funReturnFun() keeps increasing and does not become 0 after each call either to i() or j() .\n5. Functions that accept other functions as paramters\nfunc function1(i int) int { return i + i } func function2(i int) int { return i * i } func funFun(f func(int) int, v int) int { return f(v) } func main() { fmt.Println(\u0026#34;function1:\u0026#34;, funFun(function1, 123)) fmt.Println(\u0026#34;function2:\u0026#34;, funFun(function2, 123)) fmt.Println(\u0026#34;Inline:\u0026#34;, funFun( func(i int) int { return i * i*i }, 123)) } Executing funFun.go will produce the next output:\n$ go run funFun.go function1: 246 function2: 15129 Inline: 1860867 6. Varadic functions\nfunc varFunc(input ...string) { fmt.Println(input) } func oneByOne(message string, s ...int) int { # accepts a single string and a variable number of integer arguments. fmt.Println(message) sum := 0 for i, a := range s { fmt.Println(i, a) sum = sum + a } s[0] = -1000 return sum } func main() { arguments := os.Args varFunc(arguments...) sum := oneByOne(\u0026#34;Adding numbers...\u0026#34;, 1, 2, 3, 4, 5, -1, 10) fmt.Println(\u0026#34;Sum:\u0026#34;, sum) s := []int{1, 2, 3} sum = oneByOne(\u0026#34;Adding numbers...\u0026#34;, s...) fmt.Println(s) } The input function argument is a slice and will be handled as a slice inside the varFunc() function. The \u0026hellip; operator used as \u0026hellip;Type is called the pack operator, whereas the unpack operator ends with \u0026hellip; and begins with a slice. A variadic function cannot use the pack operator more than once.\nBuilding and executing variadic.go will generate the following output:\n$ ./variadic 1 2 [./variadic 1 2] Adding numbers... 0 1 1 2 2 3 3 4 4 5 5 -1 6 10 Sum: 24 Adding numbers... 0 1 1 2 2 3 [-1000 2 3] 7. Packages\n  How to deploy own package.\n Verify where go stores package source code.  $ echo $GOPATH /home/percy/go  Create a directory inside ~/go/src. This folder will be installed by \u0026ldquo;go install\u0026rdquo;  $ mkdir -p /home/percy/go/src/mypackagefolder $ touch /home/percy/go/src/mypackagefolder/packageCode.go  Copy the package source code inside packageCode.go. For example  package packageCodeImplementation // This is called by external functions import ( \u0026#34;fmt\u0026#34; ) func A() { fmt.Println(\u0026#34;This is function A!\u0026#34;) } const MY=123  Install mypackageFolder, not the packageCode.go.  $ go install mypackageFolder Note that \u0026ldquo;go install mypackageFolder\u0026rdquo; will create a \u0026ldquo;mypackageFolder.a\u0026rdquo; file at \u0026ldquo;/home/percy/go/pkg/linux_amd64/\u0026rdquo;\n Import mypackageFolder from another go code, for example.  package main import ( // Imports the FOLDER where the package is located. \u0026#34;mypackageFolder\u0026#34; // this calls mypackageFolder, not the packageCode and not packageCodeImplementaion. \u0026#34;fmt\u0026#34; ) func main() { fmt.Println(\u0026#34;Using packageCode!\u0026#34;) // this calls the pacakgeCodeImplemetation, not the mypackageFolder nor packageCode packageCodeImplementation.A() fmt.Println(packageCodeImplemenation.MY) }   NOTE: The following applies to FUNCTIONS, VARIABLES, TYPES, ETC.\n   Name comment     fmt.Println Functions that start with UPPER case are PUBLIC   fmt.myPrint It starts with LOWER case, it is PRIVATE    8. Init()\nEvery Go package can optionally have a private function named init() that is automatically executed at the beginning of the execution time. The init() function is a private function by design, which means that it cannot be called from outside the package in which it is contained. Additionally, as the user of a package has no control over the init() function, you should think carefully before using an init() function in public packages or changing any global state in init() .\nInit() is executed only once at the time where the import calls the package. For example:\npackage a import ( \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;init() a\u0026#34;) } func FromA() { fmt.Println(\u0026#34;fromA()\u0026#34;) } package b import ( \u0026#34;a\u0026#34; \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;init() b\u0026#34;) } func FromB() { fmt.Println(\u0026#34;fromB()\u0026#34;) a.FromA() } package main import ( \u0026#34;a\u0026#34; \u0026#34;b\u0026#34; \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;init() manyInit\u0026#34;) } func main() { a.FromA() b.FromB() } Will results as:\n$ go run manyInit.go init() a init() b init() manyInit fromA() fromB() fromA() 9. Modules\nGo modules allow you to write things outside of GOPATH. Modules are used to specify dependencies and their locations.\nFor example\npercy@prec:m$ mkdir test percy@prec:m$ cd test percy@prec:m$ touch test.go percy@prec:m$ cat test.go package main import ( v1 \u0026#34;github.com/percyperezdante/gomod\u0026#34; ) func main() { v1.Version() } requires github.com/percyperezdante/gomod which is in github.com and contain the following:\npackage gomod import ( \u0026#34;fmt\u0026#34; ) func Version() { fmt.Println(\u0026#34;Version 1.0.0\u0026#34;) } If you run at this stage you will get a similar error as the following:\ntest.go:3:5: cannot find package \u0026#34;github.com/percyperezdante/gomod\u0026#34; in any of: /usr/local/go/src/github.com/percyperezdante/gomod (from $GOROOT) /home/percy/go/src/github.com/percyperezdante/gomod (from $GOPATH) To execute test.go you could run the following:\n$ cd test $ export GO111MODULE=on $ go mod init anyname $ vim go.mod $ cat go.mod module anyname go 1.14 require github.com/percyperezdante/gomod v1.0.0 And then\n$ go run test.go NOTE  To remove this module:  $ go env GOPATH /usr/local/go $ cd $GOPATH/pkg/mod/github.com/percyperezdante $ ls -ltr dr-x------ 2 percy percy 4096 May 21 08:47 gomod@v1.0.0 dr-x------ 2 percy percy 4096 May 21 08:58 gomod@v1.1.0 $ rm -rf gomod@v1.0.0 $ cd $GOPATH/pkg/mod/cache/download/github.com/percyperezdante $ rm -rf gomod  To keep all dependencies in the current directory  $ cd test $ go mod init anyname $ go mod vendor Go downloads all dependencies in inside the test/vendor folder\n10. Type assertions\nA type assertion is the x.(T) notation, where x is an interface type and T is a type. Additionally, the actual value stored in x is of type T and T must satisfy the interface type of x.\nfunc main() { var myInt interface{} = 123 // This declares and defines an interface k, ok := myInt.(int) if ok { fmt.Println(\u0026#34;Success:\u0026#34;, k) } v, ok := myInt.(float64) if ok { fmt.Println(v) } else { fmt.Println(\u0026#34;Failed without panicking!\u0026#34;) } i := myInt.(int) // This is int and = 123 fmt.Println(\u0026#34;No checking:\u0026#34;, i) j := myInt.(bool) // This will fail as it myInt is int not boolean fmt.Println(j) } "
},
{
	"uri": "http://example.org/devops/gcp/fundamentals/",
	"title": "Fundamentals",
	"tags": [],
	"description": "",
	"content": "Introduction GCP offers four main services:\n- Compute - Storage - Big data - Machine learning  The fundamentals course covers mainly the first two plus networking.\nCloud computing is an on-demand infrastrucuture available under the following characteristics:\n- Compute resouces on-demand self-service: no human intervention - Access from anywhere in the internet - Resource pooling: GCP provides share-base resources - Rapid elasiticity: Get more resources quickly as needed - Measured service: pay for what you consume or use  There are serveral GCP computing services, such as:\n- IaaS: where you pay for what you allocated - PaaS: where you pay for what you use  GCP allocation:\n- Zone: Deploymenat area, not geographically related. A zone is a single failure domain witdh in a region. - Region: Group of zone, independen geographical areas.  The basics To work in GCP you organise your work load in projects. These projects organize GCP resorues with common basic objectives. Access to GCP is through IAM, GCP ID and Access Management, and it defifnes who can do what.\nUser interfaces to access and interact with GCP includes:\n- Web interface ( GCP console and generally used at the begining ) - SDK ( gcloud, gsutil for cloud storage, bq for Big query ) - Command line ( cloud shell ) - RESTful API ( uses JSON and OAuth 2.0 authentication and authorization ) - Mobile App  All resources in GCP are organised into projects.Optionally, this projects can be organised into folder and subfolders. All these projects, folders and subfolders that belongs to an organisation can be brought under an organisation node. Each project is separated compartiment and usually have the following id:\n- Project ID: Global inmutable unique name chosen by you. - Project name: Can be mutable and chosen by you. - Project number: Global inmutable number given by GCP.  On the other hand, a policy is set on a resource, where each policy contains a set of roles and role members. Resources inherit policies from parents, where a less restrictive policy overrides a more restrictive resource policy. For example, if a organisation node policy is set to read only, but a project policy is set to read and write, then read and write is possible for that project. Take in mind this when you design your policies.\nThe Identity and Access Management, IAM, helps to manage access rights to currents users of a project. There are three parts:\n- Who: Identifies the user or resource, such as google or service account, group, domain. - How/when: This uses AIM Role, a collection of permissions: primitive, predefined, and custom. - Primitive are broad an include: owner, editor , viewer and billing administrator. - Predefined: Pre designed rules that can be used. - Custom: Where you design and set your own roles. - What: GCP Resource.  Note to give access permissions to a VM you need to use a service account.\nGCP Launcher is a quick tool for deployment that contains a pre-packaged and ready-to-deploy solutions. Some of these solutions are offered by Google and others by third-party vendors. GCP upgrades on the VMs do not update installed packages, but GCP allows you to maintain them.\nVirtual Private Cloud (VCP) network VCP is a set of one or many virtual machines interconnected through a virtual network inside your project in GCP. It is similar to a traditional network where you can define your own firewall rules to restrict access to internal resources or create static routes to redirect traffic to specific destination. An important feature of VCP in GCP is its global scope, which allow your virtual machines to be reachable globaly. It is possible to allocate resources in different zones or expand resources, such as storage or network, to any virtual machine inside your VCP. One tip at this stage is to use preemptible VMs which allows you in some extend to reduce costs.\nA VM can be creatd by console UI or by command line, gcloud. If there is not a pre-define image of your interest, you can customise your own image. Additionally, in cases such as intensive data analysis, you can use a local SSD disk. However, you need to store the data of permanent value in different place as GCP will not leave data on your local SSD disk after all process has finished. For this last case, you can use GCP persistent disks.\nAt booting time, you can als provide to GCP start-up scripts or metadata to initialise your VM. It is possible to define the number of CPUs and memory size for each VM, for example, at the time of this writing, the maximum number of CPU to provision was 96, and the maximum memory size in data was 624 GB. To complete tasks such as intensive data analytics, GCP offers auto-scale, which automatically deploys new VMs base on the load of your task. Additionally, GCP offers cloud load balancing base on the incoming traffic. The options that GCP offer for cross regional load balancing are:\n- Global HTTP(s): Layer 7 load balancing based on load. It routes different URLs to different backends. - Global SSL proxy: Layer 4 load balancing for non-HTTPS SSL traffic. - Global TCP proxy: Layer 4 load balancing of non-SSL TCP traffic. - Regional: Load balancing of any traffic, TCP and UDP on any port. - Regional internal: Load balancing of traffic inside a VPC. All options have this option.  Inside a VCP, GCP already set up a firewall and routers, you do not need to provision these resources manually. However, you can configurate them as you need. This feature comes automatically because VCP networks belong to GCP projects. In case you need to allow communication between two VCP from different GCP project, you can use VCP peering. Additonally, you can difine access rules between VCPs using VCP sharing, which define who and what can be accessed from one VCP to another.\nOn the other hand, GCP offers also Cloud DNS, which can be manage programatically through its API or by command line. You can manage zones, which includes edit, add, remove DNS entries. Finally, Cloud CDN is able to cache your content close to your clients.\nNote that in the case you want to interconnect with external networks or your own local network, GCP offers the following options:\n- VPN: Through a cloud router using Boarder Gateway protocol. This means you access your GCP through internet. - Direct peering: This is private connection between your network and GCP. This is not cover GCP SLA. - Carried peering: Connection through the largest partner network of service providers. - Dedicated Interconnect: Connects nx10G transport circuit for private cloud traffic.  How to create a VM using UI console\n1. Login to GCP console --\u0026gt; Click product and services --\u0026gt; Compute Engine --\u0026gt; VM instances --\u0026gt; create. 2. Edit name, zone, VM specs as you need --\u0026gt; Click create. How to create a VM by command line\n1. Click on Activate Google Cloud shell ( icon on the top bar ) 2. $gcloud compute zones list # List all available zones 3. $gcloud config set compute/zone us-central1-c # Sets the zone  4. $gcloud compute instances create \u0026#34;MYVM\u0026#34; \\ --machine-type \u0026#34;n1-standard-1\u0026#34; \\ --image-project \u0026#34;debian-cloud\u0026#34; \\ --image \u0026#34;debian-9-stretch-v20170918\u0026#34; \\ --subnet \u0026#34;default\u0026#34; Basic inspection\n- You can ssh into your VM by click on the SSH in the console interface. - You can ssh from one VM to another by ssh \u0026lt;NAME_VM\u0026gt; directly. - You can install a web server, such as nginx-light, and use http to retrieve content. - Use sudo to have admin privileges without a password. Storage You can store data inside the VMs you ship in GCP. However, GCP can store structured, unstructured, transactional and relational data throught the following services:\n- Cloud storage - CLoud SQL - Cloud Spanner - Cloud Data Store - Google Big Table Cloud storage Cloud storage uses object storages to store your data. Object storage is not same as the traditional file storage or block storage. Instead, the whole object is stored by associating it to a key, a key that has a URL form.\nCloud storage is a set of buckets, that are inmutable, which means that you can not edit them, but you can create a new version of them. Cloud storage always encrypt your data in the server side.\nEach bucket has a unique id and location. You can move one bucket from one location to another in order to optimise latency. To control access to your buckets you can use Cloud IAM, which in general is sufficient. Each of the access control lists, ACLs, have two parts: one to specify the user or group of users and the other to specify the type of permissions associated to these users or groups.\nAdditionaly, you can turn on versioninng, which allows you track all modifcations of you object storage. However, if you turn off your versioning, you will always have one version of your object storage, which means that the old version will be replaced by the new one.\nGCP offers four Cloud storage classes:\n- Regional(99.95% availability): Let your store your data in a specific region. Cheaper but less redundant. Examples include: Europe west, asia east. - Multi-regional(99.90% availability): Stores your date in a at least two geographica regions separated by at lease 160 km. Examples include: EU, Asia. Ideal for data frequently accessed. - Nearline(99.00% availability): It is a low cost and ideal for unfrequently accessed data, such as once a month or lest on average. - Coldline(99.00% availability): It is a very low cost and ideal for data accessed less than once a year, such as archiving, online backups and disaster recovery. There are several methods to transfer your data into Cloud storage. These methods include gs-util, drag and drop, online transfer storage service and the offline transfer appicanes tools. Cloud Storage also works with other GCP services to transfer your data, these services include import export tables using BigQuery and cloud SQL, Object storage, logs and backups from App engines, scripts and images from Compute Engine.\nGoogle Big Table\nIt is a NOSQL Big Data database service. Bigtable can scale to billonw of rows and thousands of columns allowing you to store petabytes of data. It is ideal for storing large set of data with low latency, it support high throughput to both read and write, which make it a good choice for operational and analytical analysis.\nIt uses the same open source API as HBase. The advantages of using Bigtable over Hbase are:\n- Scalability: especially when query rates per time increases, GCP manages to scale up your cluster through a machine counter. - Administration: These tasks are transparent to the user, and GCP manages all operational work such as updates, and patches. - Encryption: Data is encrypted in both in-flight and at rest. Also IAM permissions can be applied to RBAC to Bigtable data. - GCP: Bigtable is same data base used by Google\u0026#39;s core services such as search, analytics and maps. Bigtable can be accessed by the following patterns:\n- App API: You can write and read from Bigtable using service layer like VMs, HBase REST server or Java server through HBase client. - Streaming: You can use popular tools such as Cloud Dataflow, Spark Streaming and Storm. - Batches: You can read and write using Hadoop Mapreduce, Dataflow, or Spark. CLoud SQL\nThis is a RDBMS service. Offers MySQL or PostgreSQL databases as service. The benefits of using Cloud SQL rather than set my own database in the cloud, is that Cloud SQL offers:\n- Automatic replication: read, failover and external replicas. This means that Cloud SQL can replicate data within multiple zones without failover. - Backups: Cloud SQL offers you backup your data on-demand or base on schedules. The backup could be vertical, by changing the machine type, or horizontal via read replicas. - Security: Cloud SQL includes network firewalls, customer data encryption when data is in internal Google\u0026#39;s networks. - Visibility: Cloud SQL instances are accessible by other GCP or external services. Cloud SQL can be used with App Engine application. Compute engine instances can be authorized to access Cloud SQL using external IP address and also can be configured with a preferred zone. Additionaly, Cloud SQL can be adminnistrated by external tools or can be set external replicas.  Cloud Spanner\nCloud Spanner offers an horizontal scalability for Cloud SQL. It offers transactional consistency at global scale, schemas , SQL and automatic synchronous replication for high availability. Cloud Spinner can be consider when you have an outgrown any relational database or you need glocal data transactional consistency such as the cases of financial application and inventory applications.\nCloud DataStore\nIt is another high scalable NOSQL database service. Its main case is to store structured data from App Engine applications. Cloud DataStore automaticaly handles replications and sharding.\nSummary\nThe following figures present a summary of technical differences between GCP services and their common use cases.\nNote: These figures are screenshots took during the course. Credits to GCP training material\nHow to get started with Cloud Storage and Cloud SQL  Create a VM with an Apache web server and upload an image to a new bucket.  - Create a VM as it was described on section \u0026#34;how to create a VM ...\u0026#34;. Before click create please add a start up script that install a web server in this VM by expanding the link \u0026#34;management, disks, networking, SSH keys\u0026#34; and look for the \u0026#34;Startup script\u0026#34;. You can copy the following: apt update apt isntall apache2 php php-mysql -y service apache2 restart - Create a bucket and upload any image to this bucket. You can click on cloud shell and type: $ gsutil mb -l EU gs://$DEVSHELL_PROJECT_ID # This creates a bucket with an UNIQUE ID,my google project id $ gsutil cp gs://cloud-training/gcpfxi/my-excellent-blog.png my-image-blog.png # Copy an image to my local VM $ ls # This should show that the images is in your VM now. $ gsutil cp my-image-blog.png gs://$DEVSHELL_PROJECT_ID/my-image-blog.png # This uploads my image to my bucket. $ gsutil acl ch -u allUsers:R gs://$DEVSHELL_PROJECT_ID/my-excellent-blog.png # modify access permissions $ gsutil ls gs://$DEVSHELL_PROJECT_ID # will show you the content of your bucket, not VM. Also you can see open your bucket using the GUI and see if it is there. To do so, click on Storage -\u0026gt; Browser -\u0026gt; Click on the name of the bucket. In here check the box for \u0026#34;Share plublicly\u0026#34;. Please, copy the link somewehre we will use it to point from the index.php page of this webserver. Create SQL instance  - Click on Products and Services -\u0026gt; Storage -\u0026gt; SQL - \u0026gt; Create Instance -\u0026gt; MySQL - Give a name of this instance and set a password. Remember to choose the same ZONE as the VM on step 1. Click create. - Click on the name and create an account: click on users -\u0026gt; create users -\u0026gt; type user name and password -\u0026gt; Click create. - Restrict the access to this SQL instance to VM on point 1. Click authorization -\u0026gt; add network -\u0026gt; Give a name -\u0026gt; Copy the publick IP address of the VM on point 1. Remember add /32 as mask for the public IP address, this is to protect from broad internet access. Configure your Apache main page  - Login to the VM created on point 1 using SSH. - Edit the /var/www/html/index.php and add php code to connect to the DB of point 2 and show the image uploaded in point 1. - Restart Apache and open in a browser \u0026#34;public ip address of VM\u0026#34;/index.php. You should be able to see the image we uploaded on point 1 and see a connection succed message to the SQL instance. As a quick example here some tips: html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Welcome to my excellent blog\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;img src=\u0026#39;https://storage.googleapis.com/qwiklabs-gcp-0005e186fa559a09/my-excellent-blog.png\u0026#39;\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to my excellent blog\u0026lt;/h1\u0026gt; \u0026lt;?php $dbserver = \u0026#34;CLOUDSQLIP\u0026#34;; $dbuser = \u0026#34;blogdbuser\u0026#34;; $dbpassword = \u0026#34;DBPASSWORD\u0026#34;; // In a production blog, we would not store the MySQL // password in the document root. Instead, we would store it in a // configuration file elsewhere on the web server VM instance. $conn = new mysqli($dbserver, $dbuser, $dbpassword); if (mysqli_connect_error()) { echo (\u0026#34;Database connection failed: \u0026#34; . mysqli_connect_error()); } else { echo (\u0026#34;Database connection succeeded.\u0026#34;); } ?\u0026gt; \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Containers in the Cloud You can build container images though tools such as docker or cloud build. However, more tasks are needed to have a reliable and scalable system, therefore more considerations are needed, such as: service discovery, application configuration, managing updates and monitoring.\n Introduction to GKE  Kubernetes is an open source orchestrator for containers, it helps to manage and scale your containers. Kubernetes lets you deploy containers inside nodes called cluster, where a cluster is a set of master nodes and containter nodes. In kubernetes, a node is a computing instance, in contrast, in GCP, a node is a VM running inside a compute engine.\nTo create a Kubernetes cluster in GKE, you can type the following command. Note that Kubernetes create \u0026ldquo;pods\u0026rdquo; to locate one or more than one containers of a Kubernetes cluster, which means a container runs inside a pod.\n``bash $ gcloud container clusters create k1 # This creates a Kubernetes cluster called k1\nHere some useful basic commands: ```bash $ kubectl run nginx --image=nginx:1.15.16 $ kubectl get pods $ kubectl expose deployments nginx --port=80 --type=LoadBalancer # creates public IP to be accessed by public Note that by exposing to the public, GKE creates a service that uses a public IP address. This service is the end point for any request from outside, and is this service that re-directs any outside request to the respective pod. The advantage to use service IP address instead of the pod\u0026rsquo;s IP address is that pods need manages in the case IP addresses change, whereasa service do the manage for you.\n$ kubectl scale services In case you need more resources, you can scale your infrastructure by:\n$ kubectl scale nginx --replica=3 $ kubectl autoscale nginx --min=10 --max=15 --cpu=80 $ kubectl get pods -l \u0026#34;app=nginx\u0026#34; -o yaml # declarative eay to use yaml file for configuration. $ kubectl get replicasets # replicas states $ kubectl get pods $ kubectl get deployments # To verify that replicas are running Hybrid and multi-cloud computing: Anthos  Anthos allows you to move some components of your on-premises application to the cloud, while keeping the rest on your own local infrastrcuture. Anthos allows both, cloud and on-prem, stay in sync, and offers a rich set of tools to manage services on-prem and on cloud, monitoring, migrations of apps from VMs int your clusters, and maintain policies across all clusters.\nApplications in the Cloud  App Engine: PaaS  This could be useful when you want to focus effort on implement your code rather than in the platform where you deploy your application. App engine is a better solution for a web app rather than a longer-running batch processing. Beside run your application, App engines offers other services to your application, such ash NoSQL databases, load balancing, loggin and authetication.\n The App Standard Engine  Run times services includes: java, python, php, and go. If you code in anothe language, Standard environment is not the rigth place, instead you can choose the flexible environment. In standard environment you use sandboxes, which no writes in local files, if you need data persistance, you can write down to a data service instead. Also all request time out at 60s, and has a limit of thrid-party software.\n The App Engine Flexible environment  You deploy your application inside containers, and GCP manage them for you.\n Could Endpoints and Apigee Edge  Cloud endpints helps you to create and mantain APIs, distribute API managemente through a console and expose your API using RESTfil interface. Apigee also helps you secure and monetize APIs. Apigee contains analytics, monetization and a developer portal. It is usually used by business developers when they want to expose legacy code though APIs to another business customers. Instead of replacing the monoitic application all in once, it peal layer by layer or component by component until complete all migration. It usually implementes microservices to expose APIs until the legacy code can be retired.\n Notes   App Engine manages the hardware and networking infrastructure required to run your code. It is possible for and App Engine application\u0026rsquo;s daily billing to drop to zero. Three advatnages for App Engine Standard: scaling is finer-grained, billing can drop to zero if your application is idle, and GCP provides and maintain runtime binaries. In case you want to do business analytics and billing on a customer-facing API, Apigee Edge is a good choice. In case you want to support developers who are building services in GCP through API logging and monitoring, Cloud Endpoints is good choice. Below some gcloud command to deploy an app in App Engine:  To run the app, not deploy it yet:\n$ gcloud auth list $ gcloud config list project $ gcloud components install app-engine-python $ gcloud app create --project=$DEVSHELL_PROJECT_ID $ git clone https://github.com/GoogleCloudPlatform/python-docs-samples $ cd python-docs-samples/appengine/standard_python37/hello_world $ sudo apt-get install virtualenv $ source venv/bin/activate $ pip install -r requirements.txt $ python main.py To run and deploy the app:\n$ cd python-docs-samples/appengine/standard_python37/hello_world $ gcloud app deploy $ gcloud app browse Developing, Deploying and Monitoring in the Cloud  Development   GCP includes a cloud source repository, it provides a git for development. It also includes a source viewer. Payments is in intervals of 100 milliseconds. GCP allows you to create triggers to update new events in your application.   Infrastrcuture as a code   GCP provides you a deployment manager for re-deployment by usign a .yaml template file. This .yaml template file describes your environment which are read by deployment manager. Deployment manager create the resources declared in the .yaml file.   Monitoring   GCP uses stackdriver for monitoring, logging, debug, error reporting, and trace.   Notes   Example of a yaml file to deploy a box in GCP.  $ export MY_ZONE=us-central1-a $ gsutil cp gs://cloud-training/gcpfcoreinfra/mydeploy.yaml test.yaml $ sed -i -e \u0026#39;s/PROJECT_ID/\u0026#39;$DEVSHELL_PROJECT_ID/ test.yaml $ sed -i -e \u0026#39;s/ZONE/\u0026#39;$MY_ZONE/ test.yaml $ cat test.yaml resources: - name: my-vm type: compute.v1.instance properties: zone: us-central1-a machineType: zones/ZONE/machineTypes/n1-standard-1 metadata: items: - key: startup-script value: \u0026#34;apt-get update\u0026#34; disks: - deviceName: boot type: PERSISTENT boot: true autoDelete: true initializeParams: sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/debian-9-stretch-v20180806 networkInterfaces: - network: https://www.googleapis.com/compute/v1/projects/$DEVSHELL_PROJECT_ID/global/networks/defaul accessConfigs: - name: External NAT type: ONE_TO_ONE_NAT $ gcloud deployment-manager deployments create HalloWOrld --config test.yaml # To update VM in caste test.yaml files is modified: $ gcloud deployment-manager deployments update HalloWOrld --config test.yaml  To create CPU Load  $ dd if=/dev/urandom | gzip -9 \u0026gt;\u0026gt; /dev/null \u0026amp; Big Data and Machine Learning in the Cloud   MLlib is to run classification algorithms. - TensorFlow - Used for classification, regresion, recommendation, anomaly detection, image and bideo analysis and text analytics.\n  Cloud Dataflow: - Manage data pipelines - \u0026ldquo;write code once and get batch and streaming\u0026rdquo; - ETL (extract, transform, load) pipelines to move, filter, enrich, shate data - Data analysis usgin streaming\n  BIg query: - Fully managed data warehouse - Provides near real-time interactive analysis of peta-bytes of datasets using SQL 2011 syntax - Computes with a terabit network\n  Data Pub/Sub - Supports many-to-many async messaging - Apps components make push/pull subscriptions to topics\n  Cloud datalab - Interactive data exploration for large-scale data exploration, transformation, analysis, and visualization - Built on Jupyter(Ipython) - Integration with BigQuery, Compute Engine and Cloud Storage using Python, SQL, and JS\n  Cloud vision - Analyse content of images with REST API - Logo detection - Label detection - Extract text\n  Cloud NLP - Return text from audio in real time - High accuracity, even with noise - It can do syntax analysis - Breaking down sentences supplied by our users into tokens - Identify the nouns, verbs, adjectives, and other parts of speech and figure out the relationships among the words\n  Cloud translation API - Translate arbitrary string in to another language\n  Cloud video intelligence API - Annotate content of videos - Detect sceme changes - Flag inappropiate events\n  Summary "
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week4/",
	"title": "SRE MMR Week4",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week3/",
	"title": "SRE MMR Week3",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week2/",
	"title": "SRE MMR Week2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/sre/online/sre-mmr-week1/",
	"title": "SRE MMR Week1",
	"tags": [],
	"description": "",
	"content": "Introduction to SRE How does SRE differ from Devops? Devops and SRE point to similar goals, which is break down organizational barriers to deliver software features faster.\nTraditionally, developers are responsable for features and operations for stability. Developers want to move faster to release new features and operations want to move slow to keep all service stable. As a commong results, tension between teams appears during realease times. Devops and SRE practices aim to break downs this tensions.\nIn general, Devops approach include the following areas:\n- Reduce organizational silos: --\u0026gt; Break down tension promotes collaboration between teams. - Accept failure as normal - Implement gradual change: --\u0026gt; Small and incremental changes are easy to review and maintain. - Leverage tooling and automation - Measure as much as you can On the other side, SRE appraoch to same areas are as following:\n- Reduce organizational silos: --\u0026gt; Operation responsability to release in production is shared with developers. - Accept failure as normal: --\u0026gt; Blameless postmordem. Failure is expected and it is hold by an error budget. - Implement gradual change: --\u0026gt; Small and incremental release, such as Canary releases. - Leverage tooling and automation: --\u0026gt; Measure toil and automate to minimise manual intervention. - Measure as much as you can: --\u0026gt; Measure toil, reliability and service. Therefore, one initial conclusion is that:\n SRE is a concrete class that implements Devops\n What is CRE? It stands for Customer Reliability Engineering and it is focus on breaking down organizational barries between service prodivers and customers. In this context, failure is pre-accepted as a conditions to enhance future cases, error budget.This pre-condition helps to minimise panic when a down time happens at the service level. Another important criteria is to implement measurements that can offer to everyone, in both sides, visibility of on how the service performs.\nHow can they help you be more reliable? In various cases, customers use provider\u0026rsquo;s services, through APIs. in different ways that are not expected by the service provider, and providers do not like to breaks the customer expectation. This sceneario is easly visible the number of customers scale up.\nCRE helps to providers to openinly communicate with customers. Clear communication of how your service was designed to behave, indirectly means to expose provider\u0026rsquo;s SRE practices, such is their SLOs, to the customers. By teaching or helping the customers to build their own SRE environment, we are teaching them how to interact correctly with our system or platform. Consequently, the real scope and limitations are presented clearly to the customer, which formilise what the customer can expect as a final result.\nIn conclusion, by sharing SRE practices with customers, not only new features can be released faster, it also enhance customer satisfactione.\nWhy Are SLOs Important for Your Organization? Building new features quickly leads to a negative correlation between development velocity and system reliability. This means that development velocity implies the posibility to break another features that can also affect our customers. Therefore, measuring SLOs gives real indications of possibel effects on the expectation of customers when a new feature is released.You can plan proactively by estimating risks to your reliability from the roll-out of new features in terms of time to detection, time to resolution, and impact percentage. Also, if you have enough error budget you can reduce the effort to cover any potential risk.\nTargeting Reliability Introduction Promises, Promises. SLOs vs SLAs. Happiness Test. How Do We Measure Reliability? Edge Cases How Reliable Should a Service Be? Setting Targets for Reliability. Iterate! Operating for Reliability Introduction When Do We Need to Make a Service More Reliable? Error Budgets. Trading off Reliability Against Features How Do We Make a Service More Reliable? "
},
{
	"uri": "http://example.org/sre/online/",
	"title": "Online resources",
	"tags": [],
	"description": "",
	"content": "The following pages contain notes that I took from courses online. These notes are in here as way to summaries my understanding and record them as personal notes.\n* Site Reliability Engineering: Measuring and Managing Reliability by Google Cloud SRE-MMR This course is divided in four weeks.\n Week 1:  Introduction to SRE Targeting Reliability Operating for Reliability   Week 2 Week 3 Week 4  * SRE CON 2019 Dublin Notes about some of the talks at SRE Dublin 2019\n"
},
{
	"uri": "http://example.org/sre/references/",
	"title": "References",
	"tags": [],
	"description": "",
	"content": "This page present a list of references related to SRE.\nBooks   Beyer, B., Jones, C., Petoff, J. and Murphy, N.R., 2016. Site Reliability Engineering: How Google Runs Production Systems. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;.\n A online available version can be read from here.    Beyer, B., Murphy, N.R., Rensin, D.K., Kawahara, K. and Thorne, S., 2018. The site reliability workbook: Practical ways to implement SRE. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;.\n Online version of this book can be found in here.    Jennifer P., JC van W.,Preston Y., Jessie Y., Jesus C., and Myk T.,2020. Training Site Reliability Engineers. \u0026ldquo;O\u0026rsquo;Reilly Media, Inc.\u0026quot;.\n Online version available at here.     Papers   Sloss, B.T., Nukala, S. and Rau, V., 2019. Metrics that matter. Communications of the ACM, 62(4), pp.88-88.\n Online read in here.     Online resources   Unknown author, unkown year, SLO Workshop Google Cloud, https://www.usenix.org/sites/default/files/conference/protected-files/srecon18emea_slides_fong-jones.pdf.\n  Google Cloud, unkown year, Site Reliability Engineering: Measuring and Managing Reliability, Coursera online courses. https://www.coursera.org/learn/site-reliability-engineering-slos/home/info\n  "
},
{
	"uri": "http://example.org/tools/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "This page presents suggestions on how to install tools of interest. It is ordered alphabetically.\nA G   GO\n $ git clone https://github.com/golang/go.git $ cd go/src $ ./all.bash # \u0026lt;-- This should generate ../bin/go file. $ cd .. $ export GOPATH=`pwd` # \u0026lt;-- Add this to your profile such as ~/.bashrc $ cd bin $ export GOBIN=`pwd` # \u0026lt;-- Add this to your profile such as ~/.bashrc $ go version    P   Packer\n $ git clone https://github.com/hashicorp/packer.git $ cd packer $ go build -o bin/packer . # \u0026lt;-- This will create ./bin/packer executable file $ ./bin/packer version $ sudo ln -s `pwd`/bin/packer /usr/local/bin/packer # \u0026lt;-- Optional $ packer --help    V   Vi File Manager\n $ mkdir temp $ cd temp $ git clone https://github.com/vifm/vifm.git $ cd vifm $ dpkg -l | grep libncursesw5-dev # \u0026lt;-- be sure you have this package $ ./configure $ sudo make install $ vifm . # \u0026lt;-- Enjoy vi File Manager    Vimium\n This extension is available in Google Chrome and Firefox. It provides keyboard shorcuts for navigation.    Z "
},
{
	"uri": "http://example.org/research/nmanet/",
	"title": "NManet",
	"tags": [],
	"description": "",
	"content": "Very brief overview NDN for MANETs approach, represented as nMANET, aims to offer an alternative perspective on how the characteristics of NDN can be utilised to solve the limitations of MANETs. nMANET has its roots in Named Data Network NDN, an instance of Content Centric Networks CCN. In contrast with traditional TCP/IP networks, CCN enables content addressing instead of host based communication, and secures the content instead of securing the communication channel between hosts. Therefore the content can be obtained from the intermediate caches or final information producers.\nThough NDN has proven to be an effective design in wired networks, it does not perfectly fit in Mobile Adhoc Networks. This is due to the high mobility of mobile devices and their resource constrains such as remaining energy in batteries. nMANET intends to fill this gap by developing a prototype called JNFD and Mini-JNFD.\nMore details about the project will be released in the last trimester of 2020\n"
},
{
	"uri": "http://example.org/devops/images/",
	"title": "Building Images",
	"tags": [],
	"description": "",
	"content": "This page presents a set of templates to build images through tools such as packer. The content of these templates is the result of a combination of available resources online.\nPacker This sections present cases about how to build linux-base images. More details in the Ajayu github repository.\n Oracle Linux 6.10 using a DVD .iso file. Jenkins and Capistrano V2 base on OL6. It is also available in Vagrant Cloud  "
},
{
	"uri": "http://example.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]